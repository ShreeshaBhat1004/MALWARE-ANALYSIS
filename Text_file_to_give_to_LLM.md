# Infrastructure as a service:
The collection of precise real-world behavior of malware has always been challenging. Malware depends on real-world conditions, Internet connectivity, and short-lived remote servers to reveal its behavior, while executing them unleashed in connected environments can have catastrophic consequences. Consequently, researchers typically rely on virtualized isolated environments, which are easily evaded by modern malware. We present JUGAAD, a first-of-its-kind malware behavior-as-a-service, which provides a real-world heterogeneous testbed with Internet connectivity to execute malware. It enables users to submit malware hashes or programs and retrieve their precise and comprehensive real-world run-time characteristics, across the computing stack, including network, operating system, and hardware. The unbiased, comprehensive view of malware activity fast-tracks research while offloading the time, effort, infrastructure, and risks involved in collecting such data.

# Labeled data as service:
Research in cyber-security is often hampered by a lack of access to ground-truth data of malware behavior in the wild. Primarily, malware is as dangerous and stealthy as a ticking time bomb, adding significant challenges to collecting their real-world behavior without being affected by its consequences. We present RaDaR, an open real-world dataset for malware behavioral analysis, with mechanisms to keep pace with the evolving malware landscape. With a comprehensive view of malware activity across the system and diverse perspectives for analyzing them, RaDaR has multiple use cases for AI-based security research, including developing novel multi-featured countermeasures and unbiased comparison of detection approaches. By abstracting the challenges in malware data collection, RaDaR opens up the cyber-security field by enabling not only security researchers, but other communities, especially data science researchers, to explore and analyze it quickly.

# Multi-featured solution:
Malware programs are diverse, with varying objectives, functionalities, and threat levels ranging from mere pop-ups to financial losses. Consequently, their run-time footprints across the system differ, impacting the optimal data source (Network, Operating system (OS), Hardware) and features that are instrumental to malware detection. Further, the variations in threat levels of malware classes affect the user requirements for detection. Thus, the optimal tuple of <data-source, features, user-requirements> is different for each malware class, impacting the state-of-the-art detection solutions that are agnostic to these subtle differences. This paper presents SUNDEW, a framework to detect malware classes using their optimal tuple of <data-source, features, user-requirements>.

# RaDaR: 
RaDaR: A Real-World Dataset for AI Powered
Run-time Detection of Cyber-Attacks
Sareena Karapoola
sareena@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Nikhilesh Singh
nik@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Chester Rebeiro
chester@iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Kamakoti V.
kama@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
ABSTRACT
Artificial Intelligence techniques on malware run-time behavior
have emerged as a promising tool in the arms race against sophisticated
and stealthy cyber-attacks. While data of malware run-time
features are critical for research and benchmark comparisons, unfortunately,
there is a dearth of real-world datasets due to multiple
challenges to their collection. The evasive nature of malware, its
dependence on connected real-world conditions to execute, and its
potential repercussions pose significant challenges for executing
malware in laboratory settings. Consequently, prior open datasets
rely on isolated virtual sandboxes to run malware, resulting in data
that is not representative of malware behavior in the wild.
This paper presents RaDaR, an open real-world dataset for runtime
behavioral analysis of Windows malware. RaDaR is collected
by executing malware on a real-world testbed with Internet connectivity
and in a timely manner, thus providing a close-to-real-world
representation of malware behavior. To enable an unbiased comparison
of different solutions and foster multiple verticals in malware
research, RaDaR provides a multi-perspective data collection
and labeling of malware activity. The multi-perspective collection
provides a comprehensive view of malware activity across the network,
operating system (OS), and hardware. On the other hand, the
multi-perspective labeling provides four independent perspectives
to analyze the same malware, including its methodology, objective,
capabilities, and the information it exfiltrates. To date, RaDaR includes
7 million network packets, 11.3 million OS system call traces,
and 3.3 million hardware events of 10, 434 malware samples having
different methodologies (3 classes) and objectives (9 classes), spread
across 30 well-known malware families.
CCS CONCEPTS
• Security and privacy→Malware and its mitigation; • Computing
methodologies→Machine learning.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CIKM ’22, October 17–21, 2022, Atlanta, GA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9236-5/22/10. . . $15.00
https://doi.org/10.1145/3511808.3557121
KEYWORDS
Artificial Intelligence for Cyber-Security, Datasets, Malware Analysis,
Run-time behavior

1 INTRODUCTION
Cyber-attacks worldwide have increased at an alarming scale, affecting
at least 60% of enterprises worldwide in 2021 [7]. The consequences
of these attacks vary from data loss to reputation damage,
business disruption, financial loss, extortion, and even sabotage
of critical infrastructures. The instrumental tool that enables adversaries
to execute such a wide range of offensive maneuvers is
malware. Despite the decades-long research in malware detection,
the increasing number of attacks and their sophistication indicates
that the problem is far from solved.
Over the last two decades, there have been several attempts
to use Artificial Intelligence (AI) for malware detection that rely
on datasets of malware samples [3, 15, 17, 18, 20, 24, 35, 42, 44,
47, 48, 51]. However, most published works typically use private
datasets. Each dataset captures different malware features such as
static strings in the malware binaries or dynamic run-time behavior,
including network communications, system calls invoked at the
operating system (OS), or hardware events. Furthermore, the size
of the datasets across different papers vary from as low as 500
samples [2, 9, 23] to 1.1 million malware samples [1, 41, 44, 47].
Given the diversity in the datasets, it is not easy to make a fair
comparison across the different detection approaches.
A few organizations, such as Endgame [1] and Microsoft [41]
have attempted to address the issue mentioned above by creating
large-scale open malware datasets containing static features from
malware binaries. These datasets facilitate static analysis that infers
maliciousness using signatures extracted from the malware binaries,
for example, strings in the binaries. These datasets have inherently
become standard benchmarks to compare different detection techniques.
However, detection techniques based on static analysis are
easily evaded by packing and obfuscating the binaries as is becoming
popular in polymorphic and metamorphic malware [36, 37].

Dynamic analysis that executes malware and analyzes their runtime
behavior has recently gained traction over static analysis due
to its ability to counter packing and obfuscation [37]. Such dynamic
analyses, powered with AI, are increasingly adopted for their ability
to detect zero-day1 malware. Primarily, AI enables the modeling
of benign behavior and the identification of anomalies to facilitate
the detection of such malware. Unfortunately, to date, there are
no real-world datasets to compare different dynamic analysis techniques.
This is primarily due to multiple orthogonal challenges in
executing malware to create such a dataset.
C-1. Malware execution typically requires real-world environments,
failing which it can choose not to execute, remaining
stealthy. Specifically, modern malware looks for triggers typically
present in virtualized environments to detect and evade
analysis.
C-2. Malware execution heavily depends on its communications
to remote servers, known as command-and-control (C&C)
servers, that guide and instruct the malware on its subsequent
actions. This requires an active Internet connection
when the malware is executed.
C-3. In many cases, the C&C servers associated with a malware
sample are short-lived. They are pulled down within a few
months after the malware is first reported [29], rendering
later executions of the malware futile.
C-4. While it is important to execute malware in connected environments,
it can be catastrophic if the malicious impact is
not contained.
Hence, executing malware in real-world and connected environments
in a timely manner (before its short-lived C&C servers are
unavailable), is essential to capture a precise representation of malware
behavior, while ensuring the containment. Multiple prior
works have attempted to build datasets of run-time behavior of malware
[3, 15, 17, 18, 20, 24, 35, 42, 44, 47, 48, 51]. However, they are
either not precise due to the use of virtualized and non-connected
environments [17, 18, 20, 24, 35, 44, 47, 51], or are not open to the
research community [3, 15, 18, 20, 42, 48, 51].
This paper presents RaDaR, an open and growing real-world
dataset for run-time behavioral analysis of Windows malware.
RaDaR dataset is precise as it is collected by executing malware
samples in a timely manner (C-3), on a real-world testbed (C-1)
with Internet connectivity (C-2), while containing their malicious
impact (C-4). To ensure a real-world environment, the testbed employs
a network of physical machines connected to the Internet to
execute malware. For timely execution, RaDaR uses an automated
framework that periodically downloads the latest samples from
online repositories [49] and executes them on the testbed to collect
the run-time trails. Thus, the framework ensures that RaDaR is
regularly updated with newer malware samples. Finally, the framework
uses a dedicated Internet connection and a two-level firewall,
which allows the malware to operate while containing its spread.
RaDaR can foster different verticals in malware research with
a multi-perspective data collection and labeling of malware behavior.
A multi-perspective collection, with simultaneous capture
of network, OS, and hardware behavior, enables a fair comparison
of different solutions based on these trails. 
<A Figure>
The figure offers several key insights into the analysis of malware using multiple independent perspectives. Here are the main takeaways:

1. **Multi-dimensional Malware Analysis**: 
   The figure demonstrates that malware can be analyzed from various perspectives, each offering a distinct classification scheme (e.g., family, objective, methodology). This layered approach helps researchers and analysts view malware in a more holistic way, revealing different traits of the same sample depending on the context.

2. **Overlap Between Classifications**:
   The figure shows how the same malware sample (represented by points on the grid) can belong to different categories across perspectives. For example, malware samples labeled "A" and "B" belong to the same family but have different attack objectives and methodologies. This indicates that classifying malware by just one perspective may not be sufficient to capture its full behavior or intent.

3. **Complexity of Malware Behavior**:
   The figure highlights the inherent complexity in malware behavior. A single sample can have multiple functionalities and characteristics that cross different perspectives (e.g., a piece of malware could be part of a family, perform a specific attack objective, use a distinct methodology, and have both backdoor and keylogger capabilities). This underlines the need for diverse analytical lenses to fully understand the behavior of malware.

4. **Different Levels of Abstraction**:
   Each layer represents a different level of abstraction. The "Family" perspective groups malware into 20 broad classes, while perspectives like "Objective" and "Methodology" offer more specific or targeted classifications. This multi-level abstraction highlights the varying granularity that different perspectives provide.

5. **Significance of Cross-Perspective Analysis**:
   The figure emphasizes the importance of considering multiple factors when studying malware. Single-perspective analysis, such as focusing solely on the malware's family or attack objective, may miss key details about its behavior or capabilities. For instance, the same family can consist of samples with diverse objectives and methods, implying that deeper insights are gained by cross-analyzing these factors.

6. **Potential for Better Detection and Defense**:
   By using this multi-dimensional analysis, malware detection systems can be improved. Viewing malware from multiple perspectives allows for a more nuanced understanding, leading to better detection of polymorphic or multi-functional malware, which might otherwise evade detection if analyzed from just one perspective.

In summary, the figure underscores the complexity of malware analysis and the necessity of using a multi-perspective approach to fully capture the diverse behaviors, objectives, and methodologies of malware samples.
<A Figure/>
Figure 1: Independent perspectives of analyzing malware in RaDaR
and their class boundaries. Each surface is a grid of 10,000 points,
where each point represents a malware sample, and its color indicates
its class based on the perspective. For instance, the common corpus
of 10,000 samples belongs to 20 malware families. However, they can
be grouped into 9 classes based on their attack objective or 3 classes
based on their methodology. These classes overlap. For instance, two
malware samples, A and B of the same family, have different attack
objectives and methodologies.
infeasible today as the set of malware samples that these solutions
use in their respective datasets are not consistent. Further, such
a collection has the potential to enable multi-featured analyses,
as malware classes differ in their functionality, leaving varying
run-time trails at network, OS, and hardware.
Another critical aspect not addressed in prior works is the multiperspective
labeling of malware. The same malware can be labeled
differently based on attributes such as its attack objective, methodology
used to infect the victim, capabilities, the information they
exfiltrate, or their family (i.e., code lineage). For instance, spyware
and ransomware have different objectives but may share the same
methodology for infection. Further, some malware may have few
capabilities in addition to their main objective, such as stealthily
logging user keystrokes. Hence, it is beneficial to analyze these malware
attributes independently to draw clear class boundaries. Unlike
prior works [15, 17, 20, 24, 35, 44, 48, 51], which propose a single
perspective to label malware (e.g., family), we propose four independent
perspectives, namely objective, methodology, additional
capabilities, and the information exfiltrated by the malware. Figure 1
illustrates the class distribution and boundaries of 10,000 malware
samples based on these perspectives. While the samples fall into
20 classes based on their family, they can be grouped into 9 classes
based on their objective2. As the class boundaries for the same set
of samples vary widely based on the perspective, independent analysis
of these perspectives is beneficial for effective detection. Such
multi-perspective labeling enables designing specialized solutions
2The classes based on objective include benign applications, ransomware, spyware,
backdoor, banker, cryptominer, deceptor, downloaders, and PUAs.
RaDaR: A Real-World Dataset for AI Powered Run-time Detection of Cyber-Attacks
and novel countermeasures, thus facilitating multiple verticals in
malware research.
Following are the major contributions of this paper:
(1) RaDaR presents an open3 dataset of real-world behavior of
malware samples from 2016 till date collected in a timely
manner, with mechanisms for regular updates, providing
multiple perspectives of malware behavior.
(2) RaDaR provides simultaneous capture of run-time malware
behavior observable at network, OS, and hardware enabling
multi-dimensional analysis and fair comparison of different
solutions.
(3) We propose four independent perspectives to label malware,
including its methodology, objectives, capabilities, and the
type of information it exfiltrates.
(4) To date, RaDaR contains 2.7 terabytes of data with 7 million
network packets, 11.3 million OS system call traces, and 3.3
million hardware events of 10, 434 malware samples having
different methodologies (3 classes), objectives (9 classes), and
spread across 30 well-known malware families.
Following is the organization of the rest of the paper. Section 2
provides the necessary background for the paper. Section 3 presents
the data collection framework. Section 4 discusses the multiple
perspectives of malware behavior that RaDaR provides. Section 5
presents the RaDaR dataset and its class distributions. Section 6
presents our evaluations on the dataset. Section 7 presents the
related work. Finally, Section 8 concludes the paper.
2 BACKGROUND
Malware is a program with malicious intent, which can vary widely
in objective from popping up annoying advertisements (adware),
downloading malicious applications (downloader), exfiltrating sensitive
data (spyware), stealing financial credentials (banker), mining
crypto-currencies (cryptominer), opening a stealthy access pathway
for the attacker (backdoor), to sabotaging the entire system
(ransomware). They can adopt different methodologies to enter the
victim, such as being bundled with other legitimate software, spam
emails that trick the users into downloading malicious files from
infected removable drives, or by exploiting vulnerabilities.
For identification and analysis, security researchers use different
taxonomies to label each malware sample. Malware is traditionally
known based on its type as listed in Table 1 [38]. These names are
based on their unique propagation methodologies (for e.g., trojan,
virus) or their attack objective (for e.g., ransomware). Alternatively,
they are known by their family, which is a collection of malware
produced from the same code base and authors, and may have similar
filename references, language, or C&C infrastructures. Typically,
Anti-Virus (AV) companies also include in the label, a string that
indicates the platform (Windows, Linux, etc.), type (Table 1), and
family[16]. For instance, Win64.Trojan.NukeSped.A_ sample is a
64-bit Windows executable trojan belonging to NukeSped family.
Malware Analysis. Malware analysis is typically done in two
ways. Static analysis examines malware binaries statically without
executing them to extract signatures and imply its maliciousness.

Table 1: Malware taxonomy based on its type or common-name [15,
17, 20, 37, 38, 48]
| Class       | Description                                                                                              |
|-------------|----------------------------------------------------------------------------------------------------------|
| Trojan      | A type of malware that downloads onto a computer disguised as a legitimate program.                       |
| Virus       | A program that can copy itself and infect a computer without the knowledge of the user via infected removable drives. |
| Worm        | A type of malware that typically exploits vulnerabilities to spread by making copies of itself from computer to computer. |
| Bot         | A self-propagating malware capable of infecting a large number of hosts and taking complete control over a computer. |
| Spyware     | A type of malware that infiltrates the victim and keeps gleaning sensitive information for an extended period. |
| Adware      | A type of malware that pops-up annoying advertisements and inappropriate content.                         |
| Downloader  | A type of malware that downloads other malware on the victim.                                             |
| Ransomware  | A type of malware that can sabotage user files and extort a ransom from the user for restoration.         |
| Cryptominer | A type of malware that exploits the computing resources of the victim to mine cryptocurrencies.           |
| Backdoor    | A type of malware that bypasses access control and grants an alternate covert pathway to resources at the victim. |


However, such static signatures can be easily thwarted by techniques that change the malware binary without affecting its
functionality or run-time behavior. For instance, packing used in
the popular polymorphic malware encrypts the contents of the
binary, whereas obfuscation modifies the binary to create different
copies of the same malware [36]. In contrast, dynamic analysis
executes the malware and analyzes the run-time trails observable
on the system stack. Consequently, it can counter the packing and
obfuscation techniques that typically foil static analysis. Further, its
potential to facilitate non-signature-based approaches that compare
the run-time behavior of malware and benign applications makes
it capable of detecting even zero-day malware.
Malware run-time behavioral trails are typically collected at network
[3], OS [18], or hardware [42]. The network data capture all
malware communications, including that to its C&C, whereas OS
data captures its attempt to remain stealthy, achieve persistence,
and execute its objective. More recently, researchers have explored
the potential of micro-architectural events (e.g., number of cache
misses) to detect malware [42]. These hardware events are measurable
using hardware performance counters (HPC) [19] available in
most modern microprocessors. Researchers rely on these behavioral
trails to analyze and detect malware.
Evasion. While dynamic analysis is more powerful than static
analysis against packing and obfuscation, modern malware have
evolved to identify and evade even dynamic analysis environments.
Specifically, they look for artifacts (such as the presence of a virtual
machine) to identify analysis environments and choose to remain
dormant. To this end, the precise collection of malware behavior
requires executing malware in real-world environments.
CIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, and Kamakoti V.
Figure 2: Automated real-world framework [22] for data collection
in RaDaR.
This image represents a workflow for analyzing malware using a system with multiple stages, potentially part of the RaDAR (or a similar) framework for malware analysis. Here’s a detailed breakdown of each component and insights into how they fit together:

### Components and Workflow:

1. **Update Engine** (Top Left):
   - The Update Engine checks the Internet for **new malware samples**.
   - Once new samples are found, they are added to the **Malware Corpus**, which is a repository of malware that will undergo further testing and analysis.

2. **Malware Corpus** (Center):
   - The **Malware Corpus** stores all collected malware samples. It acts as a central database for new and existing malware used in the test and evaluation process.
   - This step highlights the importance of constantly updating the malware dataset to ensure analysis is based on the latest real-world threats.

3. **Test Engine** (Center Top):
   - The **Test Engine** takes raw data from the malware corpus and initiates tests. The tests are likely designed to observe the behavior of the malware in different conditions and collect information such as its objective, methods, capabilities, etc.
   - The **Multi-Perspective Data** (indicated in blue) is generated during this process. It implies that the test engine gathers information across various perspectives (e.g., objectives, methodologies, capabilities), providing a broader understanding of each malware sample.

4. **Label Engine** (Center Bottom):
   - The **Label Engine** processes the raw data from the Test Engine and assigns **Multi-Perspective Labels** (indicated in green). These labels classify the malware samples into different categories based on attributes such as attack objectives, methodologies, family, or capabilities (as discussed in previous figures).
   - The labeled data is then added to the **RaDaR Dataset**, creating a comprehensive dataset enriched with multi-perspective classifications.

5. **RaDaR Dataset** (Bottom Left):
   - The **RaDaR Dataset** is the output of the Label Engine, containing malware samples with detailed, multi-perspective labels.
   - This dataset is likely used for further analysis, research, and training of malware detection systems. It emphasizes the importance of comprehensive labeling, ensuring that the dataset captures malware behavior from multiple viewpoints.

6. **Real-World Testbed** (Top Right):
   - The **Test Engine** can also interface with a **Real-World Testbed** (connected to the Internet). This means the system can test malware in live, real-world environments (e.g., on actual or simulated networks and systems), providing data on how the malware behaves in real, operational settings.
   - This real-world testing is crucial to understanding how malware interacts with different systems and how effective it is under real conditions.
   - It offers insights into how malware could spread, infect, or exploit vulnerabilities in a real-world scenario.

### Insights:

1. **Multi-Perspective Malware Analysis**:
   - The image emphasizes a multi-perspective approach to malware analysis. By collecting both multi-perspective data and labels, the system can classify malware in various ways, such as based on family, attack objectives, capabilities, or techniques. This comprehensive classification allows for a deeper understanding of the malware’s full behavior and impact.
  
2. **Dynamic and Real-World Context**:
   - The workflow integrates real-world testing, making it highly relevant to actual malware threats. By linking to real-world testbeds, the system doesn't just analyze malware in isolation but sees how it behaves in live environments, offering practical insights into its effectiveness and spread.
  
3. **Continuous Updating**:
   - The continuous feedback loop where the **Update Engine** retrieves new malware samples from the Internet ensures that the **Malware Corpus** is always updated. This dynamic aspect of the workflow ensures that the system keeps pace with evolving threats.
  
4. **Integration of Raw Data and Labels**:
   - The system ensures that raw data from testing is converted into meaningful, multi-dimensional labels, enriching the dataset and providing a resource that can be used for future detection models, research, and defense systems.

5. **Importance of Comprehensive Datasets**:
   - The image highlights the creation of a robust, labeled malware dataset (RaDaR Dataset). Having a well-labeled dataset is critical for training machine learning models to detect and classify malware accurately.
  
6. **Modular Design**:
   - The system is modular, with distinct engines for different tasks (e.g., testing, labeling, updating), which could suggest flexibility. Each component can potentially be updated or enhanced independently, allowing for scalability and adaptability as malware threats evolve.

### Conclusion:

This diagram illustrates a sophisticated malware analysis and detection framework, focusing on collecting, testing, and classifying malware across multiple perspectives. By leveraging both real-world test environments and dynamically updated datasets, the system provides comprehensive insights into malware behavior and characteristics, helping improve detection and mitigation strategies. The use of multi-perspective data and labels ensures that malware analysis is both thorough and multi-dimensional, capturing the complexities of modern malware.

3 DATA COLLECTION FRAMEWORK
The trails captured by executing a malware sample are precise if
they closely represent its real-world behavior. This section presents
the framework used for collecting the RaDaR dataset and describes
how it addresses the challenges C-1 to C-4 to ensure the precision
of the collected trails.
Figure 2 describes the data collection framework [22], which has
three engines, namely the update, test, and label engines. The update
engine downloads the malware samples for analysis, whereas
the test engine executes them on a real-world testbed to collect
their run-time trails. Finally, the label-engine labels the collected
trails based on different perspectives. Next, we discuss how the
framework addresses the challenges in precisely collecting malware
behavior.
Real-World Testbed (C-1). To prevent malware from evading
analysis, the environment should be close to that of the real-world,
which includes non-virtualized physical systems, preferably a heterogeneous
network of devices, and Internet connectivity. The test
engine employs a real-world testbed designed in our laboratory
with a network of 512 different physical machines to execute malware.
These machines include Windows and Linux desktops and
single-board computers (Raspberry Pi and Intel Galileo boards).
This heterogeneity of devices ensures that the testbed is likely to
have sufficient real-world conditions that malware looks for similar
to that in the wild.
Further, the execution of each sample can affect the system state,
such as files, registry, and the micro-architecture. Since these modifications
made by prior samples can affect subsequent analysis,
we start every execution in a clean initial state of the system. To
achieve this, the testbed has a quick state-reset mechanism that
resets the machines to initial states before executing every sample.
Internet Connectivity (C-2) and Containment (C-4). To provide
connectivity while containing the malware impact, we use
a dedicated Internet connection (ERNET [10]) for the framework
that is isolated from our university network. Further, the testbed
connects to the Internet via a two-level firewall that is lenient on incoming
communications while extensively scrutinizing all outgoing
communications for malicious behaviors such as Denial-of-Service
attempts, network scans, and spam emails. On detecting such malicious
outgoing communications, the firewall blocks them. Thus, the
testbed allows malware communications with their remote C&C
servers while preventing the malicious impact from permeating
outside the testbed.

Figure 3: T-SNE visualization [52] that indicates the distinguishability
of backdoor, spyware, and ransomware run-time trails from that
of benign applications at network, OS, and hardware. We capture
these trails by executing 1000 samples of each class on the real-world
testbed. The trails are pre-processed to extract 40 features from the
network (e.g., number of flows), 9 features from OS (e.g., write to file),
and 56 features from the hardware (e.g., L1 cache misses) trails. The
axes values are 2D- projections of these multi-dimensional features.
### Diagram Insights:

This figure appears to visualize the distribution of malware samples (likely backdoor, spyware, and ransomware) across different environments or dimensions: **Network**, **OS**, and **HPC** (which could stand for High-Performance Computing). Each subplot shows how the malware samples cluster or spread within each dimension. Here are detailed insights for each section:

1. **Rows Represent Different Environments**:
   - **Network**: The top row shows how different malware types (backdoor, spyware, ransomware) are distributed in a network environment.
   - **OS**: The middle row represents the distribution across an operating system environment.
   - **HPC**: The bottom row illustrates how malware is distributed in a high-performance computing (HPC) environment.

2. **Columns Represent Different Malware Types**:
   - **Backdoor**: The first column visualizes the distribution of backdoor malware samples.
   - **Spyware**: The second column visualizes spyware distribution.
   - **Ransomware**: The third column visualizes ransomware distribution.

3. **Color Coding**:
   - The color coding (red and blue) likely represents different subcategories or classification characteristics (such as distinct classes or families within a particular type of malware).
   - The colors can also indicate the results of different clustering algorithms, showing how samples with certain traits tend to group together.

4. **Key Observations by Malware and Environment**:
   - **Network (Top Row)**:
     - The distribution of samples (for backdoor, spyware, and ransomware) appears fairly scattered in the network environment, with no obvious structure or clustering for any malware type.
     - This could suggest that in network analysis, the behavioral patterns of these malware types are varied and do not fit into strict categories.
   
   - **OS (Middle Row)**:
     - The distribution for backdoor malware in the OS environment shows a sparse scattering of samples, suggesting either a lack of distinct characteristics or that the OS layer is not as relevant for backdoor analysis.
     - Spyware and ransomware show more structured, circular clustering. This could suggest that in the OS environment, these types of malware follow clearer, identifiable patterns, likely due to specific techniques they use (e.g., targeting system processes or files).
   
   - **HPC (Bottom Row)**:
     - For the HPC environment, backdoor malware shows a tight, circular clustering, suggesting that HPC characteristics are strongly correlated with certain backdoor behaviors.
     - Spyware has a more complex structure, indicating that its behavior in HPC environments might be more varied or dependent on additional factors.
     - Ransomware exhibits a dense, tightly clustered distribution, possibly due to the specific ways ransomware exploits HPC systems for high impact (e.g., encrypting large data sets or locking out critical infrastructure).

### Text Replication:

Here’s how the diagram might be described in text, column by column, for easy interpretation:

---

**Backdoor Malware**:
- **Network**: The samples are scattered widely with no apparent clustering, indicating varied behavior in network environments.
- **OS**: The backdoor samples show minimal distribution and are sparsely placed, suggesting a lack of distinctive behavior or a weaker correlation with the OS environment.
- **HPC**: In HPC environments, the backdoor samples cluster tightly in a circular pattern, suggesting stronger behavioral similarities or predictable attack patterns.

---

**Spyware Malware**:
- **Network**: The samples are spread out and show some clustering, but overall behavior is still varied in the network environment.
- **OS**: There is a clear circular clustering of spyware samples, indicating that specific spyware behaviors emerge when interacting with the OS layer.
- **HPC**: The spyware samples form a more complex, partially circular structure in the HPC environment, suggesting some consistency but also diversity in how spyware interacts with HPC systems.

---

**Ransomware Malware**:
- **Network**: The ransomware samples are widely distributed with no obvious clustering, showing varied behavior in the network environment.
- **OS**: Similar to spyware, the ransomware samples form a clear circular pattern, implying consistent and predictable behaviors when attacking the OS environment.
- **HPC**: Ransomware samples exhibit a very dense clustering, indicating that ransomware behavior in HPC systems is uniform and strongly correlated, likely due to its specific attack methods on high-value computing resources.

---

### Conclusion:

This visualization demonstrates how different types of malware (backdoor, spyware, ransomware) interact with different environments (network, OS, HPC). The distribution patterns suggest that malware exhibits varied behavior depending on the environment, with some environments showing clearer, more consistent attack patterns (e.g., ransomware in HPC) while others display greater diversity in behavior (e.g., backdoor malware in network environments). Understanding these patterns can aid in designing better detection and defense mechanisms tailored to each environment.


Automated Timely Execution of Malware (C-3). The framework
in Figure 2 is automated to collect behavioral data of malware
in a timely manner when their short-lived C&C servers are likely
Benign Malware to be active. The update engine periodically crawls public malware
repositories for newly reported samples and downloads them to the
malware corpus. The addition of new samples to the corpus triggers
the test engine, which executes the latest samples from the corpus
on the real-world testbed. The testbed collects the behavioral trails,
which are later labeled and added to the RaDaR dataset. Thus the
framework ensures a regular feed of new malware samples reported
in the wild, which are analyzed immediately and updated to the
RaDaR dataset.
4 MULTI-PERSPECTIVE ANALYSIS
RaDaR presents multiple perspectives of malware execution, including
data collection observed at network, OS, and hardware, and
different ways of labeling them. While multi-perspective collection
provides a comprehensive view of malware activity in the system,
multi-perspective labeling provides different perspectives to analyze
the same malware. This section discusses the need for different
perspectives to collect and label malware behavior.
4.1 Multi-Perspective Collection
Malware classes differ in objectives and functionalities and can leave
varying trails in the network, OS, and hardware. Figure 3 presents
a t-SNE visualization [52] of behavioral features that is indicative
of the distinguishability of backdoor, spyware, and ransomware
trails from that of benign applications. As seen in the figure, some
malware classes are more easily identifiable using one trail than
the others.
RaDaR: A Real-World Dataset for AI Powered Run-time Detection of Cyber-Attacks CIKM ’22, October 17–21, 2022, Atlanta, GA, USA
Figure 4: Overlapping malware attributes: (a) Methodology and
Objective: The horizontal blue shaded bars (trojan, worm, and virus)
differ in the methodology, while the vertical bars (e.g., spyware,
backdoor, and downloader) differ in the attack objective; (b) Family
and Objective: The green circles indicate families of different sizes.
The rectangular boxes indicate families having the same objective.

### **Deep Insights into the Image**

This image contains two distinct sub-diagrams: (a) Methodology and Objective, and (b) Family and Objective. Both provide deep insights into how malware is categorized and how different characteristics like **methodology**, **family**, and **objective** interplay in malware analysis. Let’s dive into each sub-diagram:

---

### **(a) Methodology and Objective**

In this diagram, different malware types are represented by overlapping rectangular blocks, with each block corresponding to a malware class. The x-axis likely represents **methodology** (the techniques or processes the malware uses), and the y-axis shows **objective** (what the malware aims to achieve). Key observations:

1. **Overlap Between Methodologies and Objectives**:
   - Certain malware types, such as **Spyware**, **Backdoor**, and **Downloader**, have significant overlap in both methodology and objectives, indicated by their shared space. This suggests that these types of malware may share similar ways of operating and achieving their goals (e.g., gaining unauthorized access, stealing information).
   - **Backdoor** and **Downloader** malware, in particular, overlap strongly, implying that they may rely on similar methodologies (like gaining access covertly and downloading additional payloads).
   - On the other hand, classes like **Virus** and **Botnet** are more isolated, indicating distinct methodologies or objectives, which means they may be more specialized in their behaviors.

2. **Boundaries of Malware Categories**:
   - The boundaries separating the malware types hint that some classes are relatively distinct. For example, **Trojan** and **Ransomware** sit on the lower end of the methodology/objective axis, meaning they could have more well-defined techniques that differentiate them from others.

3. **Adware and Ransomware Relationships**:
   - **Adware** and **Ransomware** occupy separate but connected spaces. This relationship suggests that while their objectives differ (e.g., Adware seeks profit through advertisements, Ransomware demands ransom for decryption), they could share similarities in how they are delivered or installed (methodology).

4. **Visual Clustering**:
   - The way some classes, like **Backdoor** and **Downloader**, are closer together could indicate that defense mechanisms or detection systems for one might be effective for the other, as they share common traits. The same could apply to **Botnet** and **Virus**.

---

### **(b) Family and Objective**

This sub-diagram uses circle clusters to represent malware families and their objectives. Each quadrant corresponds to a different type of malware (Spyware, Backdoor, Ransomware, Downloader), and the size and color of the circles carry key insights:

1. **Circle Size Represents Family Proportion**:
   - The size of the circles likely indicates the proportion of samples belonging to each family. Larger circles represent more prevalent families, meaning those families contribute the most to that particular malware type.
   - For example, **Spyware** and **Ransomware** show large circles, indicating certain families dominate within these malware types. This suggests that these types have "leading families" that are responsible for the bulk of activity in this space.

2. **Color Represents Objectives**:
   - The color coding differentiates between **families with similar objectives** (yellow, pink) and **families with different objectives** (green). 
   - In some quadrants, like **Ransomware**, green circles dominate, suggesting that multiple families of ransomware share similar objectives (e.g., extortion), while in **Downloader**, the objectives might vary more (represented by a mix of colors).

3. **Concentration of Objectives**:
   - In the **Spyware** and **Backdoor** quadrants, you notice a more evenly distributed mix of small and large circles, representing that these malware types are more diverse in their objectives and family composition. This diversity could make it harder to classify or detect these types, as different families may employ different strategies and methods.
   - In contrast, **Ransomware** shows tight clustering with large green circles, suggesting a more unified objective (likely financial gain) and more concentration around a few prominent families.

4. **Different vs. Similar Objectives**:
   - The color differentiation highlights that while many families share similar objectives, there are some notable differences. For example, in the **Backdoor** quadrant, some families (in pink and yellow) pursue distinct objectives, which could imply that while the delivery mechanisms are similar, the end goals differ (e.g., some might aim to steal data, others to maintain persistent access).

5. **Implications for Malware Detection**:
   - The clustering patterns in each quadrant suggest that malware detection methods could be tailored based on the specific objectives and families of each type. For example, detecting **Ransomware** might be easier given the more concentrated objectives, while detecting **Downloader** malware could require more nuanced approaches due to the variety of objectives and methods represented.

---

### **General Insights from Both Diagrams**:

1. **Interplay of Methodology, Objective, and Family**:
   - The two diagrams together emphasize that malware classification is not linear or one-dimensional. Malware families can share objectives but employ different methodologies, or conversely, the same methodology could be used to achieve different objectives across families.
   - This complexity implies that effective malware detection systems need to account for both objective-based (e.g., what the malware is trying to achieve) and method-based (e.g., how it operates) analysis to be comprehensive.

2. **Family-Objective Relationship**:
   - The second diagram shows that while some families are objective-agnostic (represented by green circles with different objectives), others are strongly linked to a particular goal. This distinction can help in focusing security efforts. For example, focusing on large green circles in the **Ransomware** quadrant might yield more success in detecting financially motivated attacks.

3. **Overlap in Functionality**:
   - The overlap in the first diagram hints that malware can often serve multiple purposes, or evolve over time from one type to another (e.g., a **Downloader** could evolve into a **Ransomware** threat by first downloading the ransomware payload). Understanding these overlaps can help security researchers anticipate and detect blended threats.

4. **Modularity in Malware Analysis**:
   - Both diagrams showcase the modularity in how malware can be studied: by **methodology**, **objective**, and **family**. This kind of breakdown allows for multi-dimensional analysis, where researchers can see not only how malware behaves (methodology) but also what it aims to achieve (objective) and how it relates to others in its class (family).

---

### Conclusion:

These diagrams provide a rich visualization of how different types of malware relate to their methodology, objectives, and families. They highlight the diversity in behavior and goals among different malware types and suggest that a holistic approach—looking at both the methods and objectives—is necessary for effective malware detection and classification. Additionally, the clear clustering in some areas (e.g., **Ransomware** families) suggests there are dominant trends within certain malware classes, while others (e.g., **Spyware**) are more diverse and harder to classify definitively. These insights can be invaluable for building more robust malware detection systems and security defenses.


 For instance, backdoor functionality involves consistent communications to its remote adversary to create a covert access
pathway, leaving strong indicators in the network. Hence, backdoor
behavior is most distinguishable from benign in the network
compared to the OS and hardware. On the other hand, spyware
functionality, which involves reading files and gleaning sensitive information,
is most distinguishable in the OS and network compared
to the hardware. Likewise, ransomware is most distinguishable in
the hardware as it triggers distinct micro-architectural events when
it encrypts a large number of victim’s files.
Prior works have extensively explored run-time trails for malware
detection, typically using one of the network, OS, or hardware
trails [3, 5, 15, 17, 18, 20, 24, 35, 44, 48, 51]. Each of these works
presents highly acceptable results using the respective trails. However,
a comparison of this large body of research to evaluate the
capabilities of different trails is infeasible today, as every work
uses execution trails of different samples collected in different timeframes
and environments. A fair comparison of these solutions
requires a comprehensive view of malware run-time activity in the
system. While few datasets present a combination of network and
OS trails [17, 20, 35, 44], we argue that the hardware perspective is
also needed to build a comprehensive view of malware behavior,
especially for classes like ransomware. Further, the differences in
capabilities of run-time trails provide opportunities to explore more
sophisticated ensemble-based approaches.
To facilitate multiple run-time perspectives, the testbed in Figure
2 simultaneously captures the network, OS, and hardware trails
during malware execution. We use tshark [34] and Windows Process
Monitor [33] to capture network and OS trails, respectively. On
the other hand, we develop a customized Windows driver based on
existing works to measure Hardware Performance Counters [13].
The network logs are collected at the gateway connecting the
testbed to the Internet since all the network traffic is routed through
it. The traffic can be attributed to different machines in the testbed
based on the IP address. On the other hand, OS and hardware behavior
are collected locally at the machine executing the malware,
and are attributed to the malware based on its process identifier.
4.2 Multi-Perspective Labeling
Contemporary malware research typically labels malware based
on type, family, or AV-labels (Refer Section 2) [15, 18, 20, 24, 35, 44,
45, 47, 48]. However, malware samples are diverse with multiple
attributes, making it favorable to label malware with different perspectives.
Further, some of these attributes may overlap, warranting
an independent evaluation of each perspective.
Malware diversity and attributes. A malware sample can be
characterized by a tuple of its attributes as ⟨methodology, objective,
capabilities, family, information it exfiltrates⟩, each of which can
vary widely as discussed next.
(1) Methodology. Malware can adopt different methodologies
to infect the victim and propagate to other systems. Accordingly,
they can be a virus, trojan, or worm. A virus is
a malicious piece of code that attaches to a host program
to get executed. It is transmitted from one computer to another
through the host program. On the other hand, trojans
and worms are standalone programs. While trojans require
user interactions for activation and propagation, worms can
self-activate and self-replicate via the network.
(2) Objective. Malware can have different attack objectives based
on which it can be adware, downloader, spyware, banker,
cryptominer, backdoor, botnet, or ransomware (Refer Section
2). Accordingly, they pose varying levels of risk to users.
For instance, ransomware that sabotages the system is a
high-risk malware, whereas adware that merely pops up
user-annoying advertisements is a low-risk malware.
(3) Capabilities. Apart from the main objective, we observe that
some malware may also have other capabilities. Some malware
have key-logging capability to log user inputs, while
others may have a hidden backdoor that opens an alternate
access pathway for the attacker, in addition to their primary
objective.
(4) Information Exfiltrated. We observe that every malware either
steals or destroys some information of the target. The
exfiltrated information typically includes one or more of
the following | (i) System details (e.g., version of OS and
system settings to identify analysis environments for evasion);
(ii) User credentials; (iii) Keystrokes; (iv) Application
passwords; (v) Details of email accounts; (vi) Clipboard and
screenshots; (vii) Digital certificates; (viii) File-system contents;
(ix) Process and hardware details; (x) Network-related
details, including active ports and other systems in the network;
(xi) Online activities of the user; and (xii) location
and language.
Overlapping Attributes. While malware samples have different
tuples characterizing them, they share some commonalities due
to overlapping attributes. For instance, spyware that exfiltrates
data could be implemented using any methodology (trojan, worm,
or virus). Figure 4a illustrates this overlap with a distribution of
malware samples based on their methodology (blue shaded horizontal
bars) and objective (vertical bars). The horizontal and vertical
bars are individually disjoint, but together, they overlap and can
significantly affect the accuracy of classification.
Similarly, Figure 4b illustrates the overlap between attributes of
family (green circles) and objective (rectangular boxes). As evident,
many families can share the same attack objective. For instance,
Corebot [39], Delf [12], and Formbook [31] are all backdoor families.
Further, a family may have malware samples of different objectives
(i.e., circles overlapping two or more boxes). For instance,
CIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, and Kamakoti V.
Table 2: Features observed at network, operating system and hardware.
| Category | Features |
|----------|----------|
| **Network** | Connection-based (Destination IP and Port, Protocol; Total number of flows, packets per flow; Number of inbound and outbound packets; Average and standard deviation duration; Ratio of sizes of packets from originator and responder; Ratio of established states.) |
|          | TLS-based (Ratio of TLS and non-TLS connection records; Ratio of TLS and SSL version in connection record; SNI and destination IP comparison; Ratio of self-signed certificates; Ratio of SSL records having SNI; Ratio of self-signed certificates; Average of certificate paths.) |
|          | Certificate-based (Average length of certificate keys, average and standard deviation of certificate validity; Validity of certificate period; Number of certificates; Number of domains in certificate; Ratio of certificate records and SSL records; the presence of ServerName Indication in subject alternative name.) |
|          | HTTP-based (URL path length; Number of URL query parameters; Filename length; Inter-arrival time; Number of URL flows; Number of downloaded and uploaded bytes; Number of files; Ratio of digits, Alphabets, Special characters; Upper/lower case, and Vowels in filename, URL, and Hostname respectively.) |
| **OS (e.g., L_ OS)** | Registry (Read, Query or Write to Windows Registry), File (Read a file, Create/Write to a file, Lock/Unlock a file); Security (Retrieve/change security descriptors of files, Add new CLSID2, Modify existing CLSID); Process-related (Process Start, Process Exit, Load an image, Thread create, Thread exit); Path-indicator (Encoding of the path of the resource accessed, Prior knowledge of association of the filename, or directory path with malware); Network (TCP Send/Receive, UDP Send/Receive, Length of data); Parameters and return values (Length of data read/write; Desired Access rights for the requested resource, Shared Read, Write or Delete, Encoding of the options including Open Reparse Point, Synchronous I/O Alert etc, return values) |
| **Hardware** | Hardware Performance Counters (HPCs) (54 HPCs such as: Core clock cycles; Instructions retired; Instruction length decoder stalls; micro-operations from loop stream detector, decoders, and micro-operation cache; resource stalls; Branches taken; Mispredicted branches; Register moves eliminated; Register moves elimination unsuccessful; L1-data cache misses; L1-data cache replacements Instruction-TLB misses, L2 requests, DTLB store/load misses leading to a walk; Hardware interrupts received; Instruction cache misses); Memory load LLC hits/misses.) |

Bladabindi is a family of samples that can be a backdoor or spyware
[32]. Figure 1 shows the significant variations between classes
based on these attributes of a corpus of 10, 000 malware samples.
Thus, it is beneficial to analyze different malware attributes independently
to draw clear class boundaries for effective classification.
Labeling in RaDaR. To facilitate multi-perspective analysis, RaDaR
labels malware with four different independent attributes in addition
to its family, namely, methodology, objective, two capabilities
including keylogger and backdoor, and the information it exfiltrates.
Each of these attributes presents different perspectives of malware
and can aid in designing specialized solutions such as mechanisms
to prevent malware infection (based on methodology) or attack
mitigation (based on objective). Specifically, an objective-based perspective
can enable multi-dimensional models to improve detection
accuracy (refer Section 4.1) and customized responses based on user
tolerance to false positives. For example, users would prefer the
termination of high-risk malware (such as ransomware) as soon
as possible to minimize the attack impact. On the other hand, they
would not want the termination of low-risk classes like adware
unless the detection is highly precise in order to minimize the false
positives. Likewise, a capability-based perspective can help design
specialized keylogger or backdoor detectors, whereas an information
leak-based perspective can help implement appropriate data
protection mechanisms.
In contrast, the single perspective of type, family or AV-labels in
prior works [15, 18, 20, 24, 35, 44, 45, 47, 48], limits the scope of analyses
possible on such datasets. Further, both type and family-based
perspectives can result in fuzzy class boundaries, thus affecting
the classification accuracy. Specifically, type-based perspective (Refer
Table 1) mixes the attributes of methodology and objective
(Figure 4a). On the other hand, different families can have similar
functionalities and behavioral trails, while others may have distinctly
behaving malware samples in the same family (Figure 4b).
Such a characteristic of family-based perspective can be attributed
to its definition, which is more indicative of code lineage and static
features than run-time behavior. In contrast, AV-based labels often
include specific meta-data in addition to type and family names,
making them too specific for any generalization [43].
5 RADAR DATASET
The RaDaR dataset to date contains the behavior of 10,434 malware
samples from 2016 obtained from Anti-Virus companies [21] and
public malware repositories [50] using the automated framework
discussed in Section 3. In this section, we first describe the snapshots
and features in the RaDaR dataset. We next present the class
distributions of different perspectives in RaDaR .
Raw Behavioral Snapshots. As described in Section 2, the logs
capture the time-series behavior of malware execution observable
at network, OS, and hardware. Network logs contain the network
packets from the machine executing the malware. In contrast, the
OS logs capture all the system call traces of the malware, including
its file, registry, process, and other operations. On the other hand,
the hardware logs contain the values of hardware performance
counters at a periodic interval of 100 ms. To date, RaDaR has 2.7
tera-bytes of data, including 7 million network packets, 11.3 million
OS system call traces, and 3.3 million hardware events.
RaDaR: A Real-World Dataset for AI Powered Run-time Detection of Cyber-Attacks CIKM ’22, October 17–21, 2022, Atlanta, GA, USA
Features. RaDaR extracts a comprehensive set of features about
malware execution as listed in Table 2. For the network data, we use
the Zeek [54] tool to pre-process the packet-level logs into network
flow summaries. A network flow comprises of all communications
that share the same source and destination IP addresses and ports.
In total RaDaR has 60K network flow summaries and 58 features.
We use custom scripts to parse the OS and hardware logs. In this
way, we extract 11 features for OS and 54 features for hardware.
The data is converted to a matrix, where the rows are the behavioral
snapshots and columns are the feature values. Thus, RaDaR has
3 matrices of order 60K × 58 for network, 11.3M × 11 for OS, and
3.3M× 54 for hardware, and each row in these matrices are labeled
as per the perspectives.
Class Distributions. Table 3 shows the distribution of malware
based on different perspectives in the RaDaR dataset. Most malware
samples in the dataset belong to the trojan class (71%), which is similar
to the distribution of malware in the real-world [46]. Similarly,
RaDaR contains representative classes having different objectives,
including banker, downloader, Potentially Unwanted Applications
(PUA), deceptor, spyware, backdoor, ransomware, and cryptominers.
While 20.3% of malware samples in RaDaR can log keystrokes
of the users in addition to their primary objective, 19.1% of them
have backdoor capabilities. A graphical representation of this distribution
and the significant overlaps between malware classes across
perspectives is shown in Figure 1.
Table 3 also provides the number of behavioral snapshots present
for each class per perspective across network, OS, and hardware in
RaDaR. As evident, the distribution of network, OS, and hardware
snapshots may not match that of malware in the dataset, as each
malware differs in its activity across the three system components.
Table 4 lists the distributions of malware families in the dataset.
RaDaR contains 30 families that can be grouped into 9 classes based
on the objective of the malware. Table 5 shows the distribution
of samples based on the information the malware collects and
steals from the target. Most malware (> 51%) collect the system
information to help it identify virtualized analysis environments.
While 19% of malware log keystrokes, 6% and 3% of samples capture
the screenshots and clipboard.
6 RESULTS
In this section, we present the results of our evaluation of RaDaR.
These results on well-known models provide a baseline for future
experiments. For our experiments, we apply Principal Component
Analysis (PCA) to reduce the feature space to 10 network features,
10 OS features, and 20 features from the hardware. For each perspective,
we split the dataset in 70:15:15 ratio corresponding to
train, validate and test sets, with an even distribution of classes.
Methodology. To evaluate the detection of methodology, we train
standard multi-class machine learning (ML) models including Decision
Tree [25], k-Nearest Neighbours [27], Logistic regression [26],
Random-Forest [28], XGBoost [53], and LightGBM [30]. For XGBoost
and LightGBM, we consider both the one-versus-one (OvO)
and one-versus-rest (OvR) configurations for multi-class classification
[30, 53]. Table 6 presents the best F1-Score observed for detecting
methodology using network, OS, or hardware trails. Methodology
of a malware is best detected using OS and hardware features
as compared to network. Intuitively, methodology deals with how
malware infects a system and activates itself, and hence, OS and
hardware trails have stronger indicators than the network trails.
While Random-Forest offers the best F1-Score for detection at network
and OS, LightGBM gives the best F1-Score at hardware.
Objective.We also observe that the detection F1-Score of objective
using multi-class classifiers [25–28, 30, 53] was very low, which
presents a wide scope for model improvements. In this regard, we
next evaluate how different each objective class is from benign applications.
To this end, we train specialized XGBoost binary models

| **Perspective** | **Class**     | **%age** | **Number of Snapshots**               |
|-----------------|---------------|----------|---------------------------------------|
|                 |               |          | **Network** | **OS** | **Hardware** |
| **Methodology** | Trojan         | 71.55%   | 36K         | 37K   | 4.8M        |
|                 | Worm           | 11.8%    | 14K         | 2.6M  | 413K        |
|                 | Benign         | 9.95%    | 8594        | 1.3M  | 578K        |
|                 | Cryptominer    | 4.95%    | 393         | 155K  |             |
|                 | Banker         | 13.5%    | 1755        | 777K  | 517K        |
|                 | Spyware        | 13.5%    | 1878        | 1.9M  | 512K        |
|                 | Backdoor       | 7.35%    | 894         | 1.1M  | 578K        |
| **Objective**   | PUA            | 10.75%   | 1594        | 4.8M  |             |
|                 | Downloader     | 15.45%   | 1991        | 7.1M  | 6.8M        |
|                 | Deceptor       | 9.45%    | 893         | 2.3M  | 578K        |
|                 | Benign         | 9.95%    | 8694        | 1.3M  | 578K        |
| **Capabilities**| Keylogger      | 30.5%    | 8618        | 1.8M  | 108K        |
|                 | Non-Keylogger  | 69.75%   | 2395        | 8.4M  | 2.4M        |
|                 | Benign         | 9.95%    | 8694        | 1.3M  | 578K        |
|                 | Backdoor       | 71%      | 334         | 4.1M  | 3.2M        |
|                 | Non-Backdoor   | 9.75%    | 33.4K       | 1.1M  | 3.3M        |
|                 | Benign         | 9.95%    | 8694        | 1.3M  | 578K        |

_Table 4: Families in RaDaR_

| **Objective Class** | **Count** | **Family**                                                                 |
|---------------------|-----------|-----------------------------------------------------------------------------|
| Downloader          | 1991      | Agent(1989), Chindo(69), Small(16), XeyaRAT(6), erosaa(9)                   |
| Banker              | 1442      | Emotet(144)                                                                 |
| PUA                 | 1106      | enox(1106)                                                                  |
| Ransomware          | 770       | Gandcrab(550), cueloo(305), Ryuk(40), Rapid(29), Ouroboros(25), Sigma(40)   |
| Spyware             | 1639      | Bladabindi(946), Agent(350), Volso(255), Butbray(77), Expolit(14)           |
| Backdoor            | 1388      | Corebot(330), Formbook(250), Agent(441), Delf(377)                          |
| Deceptor            | 585       | Deceptor(585)                                                               |
| Cryptominer         | 500       | Canniminer(500)                                                             |
| Dropper             | 260       | Agent(220), NukeSpeed(33), Delf(10)                                         |

_Table 5: Distribution of information exfiltrated in RaDaR_

| **Information** | **%**   | **Information** | **%**   | **Information** | **%**    |
|-----------------|---------|-----------------|---------|-----------------|----------|
| System          | 51.31   | Accounts        | 1.35    | Location        | 3.71     |
| User            | 20.00   | Keystrokes      | 19.89   | Language        | 3.79     |
| Network         | 8.70    | Screenshots     | 8.19    | Data            | 3.43     |
| Hardware        | 4.91    | Passwords       | 1.34    | Documents       | 3.49     |
| Process         | 2.40    | Clipboard       | 3.19    | Unknown         | 41.45    |

CIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, and Kamakoti V.

Table 4: Results of the evaluation of RaDaR
| Perspective    | Class         | Best F1-Score                                | Best Model                                |
|----------------|---------------|----------------------------------------------|-------------------------------------------|
|                |               | **Network** | **OS**   | **Hardware** | **Network** | **OS**   | **Hardware** |
| **Methodology**| Benign         | 0.996       | 1.0     | 0.999         | RF          | LGBM     | LGBM         |
|                | Trojan         | 0.856       | 0.999   | 0.968         | RF          | RF       | LGBM         |
|                | Worm           | 0.866       | 0.999   | 0.988         | RF          | RF       | LGBM         |
| **Objective**  | Cryptominer    | 0.83        | 0.87    | 0.94          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Banker         | 0.9         | 0.82    | 0.75          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Spyware        | 0.89        | 0.92    | 0.7           | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Backdoor       | 0.92        | 0.8     | 0.75          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Ransomware     | 0.86        | 0.8     | 0.68          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | PUA            | 0.88        | 0.65    | 0.56          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Downloader     | 0.99        | 0.84    | 0.6           | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Deceptor       | 0.89        | 0.87    | 0.78          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Benign         | 0.99        | 0.998   | 0.998         | XGB-Binary  | LGBM-Binary | XGB-Binary |
| **Capabilities**| Non-Keylogger | 0.892       | 0.994   | 0.999         | XGB-OVR     | XGB-OVR   | XGB-OVR     |
|                | Keylogger      | 0.875       | 0.89    | 0.999         | XGB-OVR     | XGB-OVR   | XGB-OVR     |
|                | Benign         | 0.998       | 0.999   | 0.998         | XGB-OVR     | XGB-OVR   | XGB-OVR     |
|                | Non-Backdoor   | 0.930       | 0.998   | 0.998         | LGBM-OVR    | RF        | XGB-OVR     |
|                | Backdoor       | 0.935       | 0.8     | 0.9           | LGBM-OVR    | RF        | XGB-OVR     |
| **Family (e.g., Backdoor)** | Corebot [39] | 0.430   | 0.050   | 0.870         | XGB-OVR     | RF        | XGB-OVR     |
|                | Delf [12]      | 0.390       | 0.153   | 0.982         | XGB-OVR     | RF        | XGB-OVR     |
|                | Agent [11]     | 0.682       | 0.151   | 0.458         | XGB-OVR     | RF        | XGB-OVR     |
|                | Formbook [31]  | 0.496       | 0.316   | 0.401         | XGB-OVR     | RF        | XGB-OVR     |

RF: Random Forest XGB: XGBoost LGBM: LightGBM XGB-Binary/LGBM-Binary: Binary Models comparing benign and an objective class
OvR: One-versus-Rest OvO: One-versus-One
on each objective class and benign applications. Table 6 presents
the results. Each malware class differs in the trails that best differentiate
it from benign applications, as highlighted in the table.
While the network trails are effective for most classes, OS trails are
the best to detect spyware. Similarly, hardware trails are the most
effective for detecting cryptominers and ransomware. We leave the
exploration of complex models and ensemble-based approaches
that can exploit these differences in run-time trails for future work.
Capabilities. We next evaluate detection of keylogging capability
using standard multi-class models [25–28, 30, 53] on network,
OS and hardware trails. The perspective has three classes namely,
benign, non-keylogger, and keylogger. Interestingly, we find the
keylogging capability is best detected using hardware and OS features,
as shown in Table 6. Its detection F1-Score is the lowest in the
network, as keylogging primarily involves intercepting the system
calls to log the user keystrokes and does not involve any network
activity.
Similar evaluation of backdoor capability using multi-class models
[25–28, 30, 53] indicate that it is best detected with OS features,
as compared to network and hardware (Table 6). This capability
perspective has three classes, namely, benign, non-backdoor, and
backdoor. The results are in contrast to the objective-based evaluation,
wherein OS trails had lower detection F1-Score than network.
We believe the models trained with the capability perspective
are able to learn the traits of backdoor functionality better than
objective-based classes, which can have overlapping capabilities.
Family. Finally, we evaluate the relevance of family taxonomy for
malware detection based on run-time behavior. As there are a large
number of malware families (Table 4), we consider the example of
backdoor families. Table 6 presents the results of detection F1-Score
using standard multi-class classifiers on 4 backdoor families using
the network, OS, and hardware trails. The results are sub-optimal.
In essence, family is an indicator of code lineage and attribution and
hence mainly useful for static analysis. In contrast, run-time behavior
depends on malware functionalities, which is the same for all
families of a particular class of malware and can affect classification.
7 RELATEDWORK
Multiple prior works have proposed datasets of run-time behavior
of Windows malware [3, 5, 15, 17, 18, 20, 24, 35, 42, 44, 47, 48,
51]. Table 7 compares them based on the environment they use to
execute the malware and the perspectives of data collection and
labeling that they present.
Analysis Environments. Most works rely on virtual machines
that are easily evaded by modern malware, and hence, are not
representative of real-world behavior [17, 18, 20, 24, 35, 44, 47, 51].
On the other hand, the datasets generated in a timely manner under
real-world conditions are not open [3, 15, 42, 48], or are least two
decades old (2001) [5]. Such outdated datasets may not be relevant
in the current malware landscape, as modern malware have evolved
considerably. In contrast, the real-world testbed framework (refer
Section 3) ensures a precise representation of malware behavior
in the wild while providing mechanisms to continually augment
RaDaR with the latest malware samples (refer Section 5).
RaDaR: A Real-World Dataset for AI Powered Run-time Detection of Cyber-Attacks CIKM ’22, October 17–21, 2022, Atlanta, GA, USA
Table 7: Comparison of prior works based on analysis environments and perspectives of data collection and labeling.
| Dataset     | Real World | Year of Samples   | Year of Capture | Multi-Perspective Collection                                      | Multi-Perspective Labeling                                                                                                                                                        |
|-------------|------------|-------------------|-----------------|------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|             |            |                   |                 | Network | OS  | Hardware | Independent Analysis | Binary | AV Label | Method | Objective | Family | Keylogger? | Backdoor? | Target of Information | Open-World Testbed |
| **CAIDA [5]** | ✓          | 2001              | 2001            | ✓       | ✗   | ✗       | ✓                  | ✓      | ✗      | ✗      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **ISOT [35]** | ✗          | * (2004-05, 2010, 2017) | * (2004-05, 2010, 2017) | ✗       | ✓   | ✗       | ✓                  | ✓      | ✗      | ✓      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **CTU [24]**  | ✓          | 2011              | 2011            | ✓       | ✗   | ✗       | ✗                  | ✓      | ✗      | ✗      | ✓        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **[48]**      | ✗          | 2011              | 2011            | ✓       | ✗   | ✗       | ✗                  | ✓      | ✓      | ✓      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **ADFA [17]** | ✓          | 2012-2013         | 2013            | ✓       | ✓   | ✗       | ✗                  | ✓      | ✗      | ✗      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **UCI [47]**  | ✓          | 2010-2014         | 2010-2014       | ✗       | ✗   | ✗       | ✗                  | ✓      | ✗      | ✗      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **[18]**      | ✗          | 2015              | 2015            | ✓       | ✗   | ✗       | ✓                  | ✓      | ✗      | ✓      | ✓        | ✓      | ✗         | ✗         | ✗                    | ✗                |
| **MalRec [44]**| ✓          | 2014-2016         | 2014-2016       | ✓       | ✓   | ✓       | ✓                  | ✓      | ✓      | ✓      | ✓        | ✓      | ✓         | ✗         | ✓                    | ✗                |
| **[42]**      | ✓          | 2018              | 2018            | ✓       | ✓   | ✓       | ✗                  | ✓      | ✓      | ✓      | ✓        | ✓      | ✗         | ✗         | ✗                    | ✗                |
| **RaDaR [51]**| ✓          | 2016-2022         | 2019-2022*      | ✓       | ✓   | ✓       | ✓                  | ✓      | ✓      | ✓      | ✓        | ✓      | ✓         | ✓         | ✓                    | ✓                |


# ✗ in Independent Attributes indicates the type taxonomy that mixes different attributes. Ψ Presents only one or two classes.
- Not Present * No information available or Not Open
$ Growing dataset till date.
Data Collection. Most datasets lack a simultaneous capture of
different trails of malware behavior, including hardware. They either
present a single trail [3, 5, 15, 17, 18, 20, 24, 35, 42, 44, 48, 51],
or a combination of network and OS trails [17, 20, 35, 44]. In contrast,
the comprehensive view of malware activity across the system
stack facilitates a fair comparison of different solutions and
a multi-dimensional analysis of malware behavior as discussed in
Sections 4.1 and 6.
Perspectives in Labeling. Prior datasets use the perspective of
binary, type, family or AV-based strings to label malware [3, 5, 15,
17, 18, 20, 24, 35, 42, 44, 45, 47, 48, 51]. Binary datasets that classify
samples into benign and malware are too restricted for any
multi-dimensional analysis such as assessing risk, capabilities or
forensics [3, 5, 15, 18, 20, 24, 35, 42, 44, 45, 48, 51]. On the other hand,
the datasets based on type [15, 20, 47, 48] and family [18, 24, 35, 44]
present only a single multi-class perspective, and can have fuzzy
class boundaries due to overlapping attributes. While few open
datasets present independent perspectives of objective or methodology
of malware, they are limited to one or two classes (Ransomware
and Botnet [35], RedWorm [5]), thus limiting the scope of analyses
using such datasets. Finally, the AV-based perspective [43, 45] is too
specific for any generalization, thus affecting the classification. In
contrast, we present a dataset with four independent perspectives
in addition to family: methodology, objective, additional capabilities
including keylogging and backdoor, and the information it exfiltrates
(Section 4.2). To the best of our knowledge, RaDaR is the first
open dataset to capture precise malware behavior using real-world
systems with diverse perspectives of its run-time activities.
8 CONCLUSION
This paper presents RaDaR, an open real-world dataset for malware
behavioral analysis, with mechanisms to keep pace with the evolving
malware landscape. RaDaR has multiple use cases for AI-based
security research, including an unbiased comparison of detection
approaches and the development of novel countermeasures incorporating
multiple perspectives of malware execution. While the
challenges in executing malware have resulted in datasets being
largely private or restricted to the security researchers, we firmly
believe that the open RaDaR dataset enables other communities,
especially the data science researchers, to explore and analyze it.
ACKNOWLEDGMENTS
The authors acknowledge K7 Security Pvt. Ltd. and Education and
Research Network for India (ERNET) for the access to malware
samples and the support in the experimental setup. This research
was funded by the Information Security Education and Awareness
(ISEA) project from the Ministry of Electronics and Information
Technology, Government of India, and the FIST program from the
Department of Science and Technology, Government of India. The
authors also thank the reviewers and the technical committee for
reviewing the manuscript and providing constructive comments.

_____________________________________________________________
1
SUNDEW: An Ensemble of Predictors for
Case-Sensitive Detection of Malware
Sareena Karapoola, Nikhliesh Singh, Chester Rebeiro, and Kamakoti V
Abstract—Malware programs are diverse, with varying objectives, functionalities, and threat levels ranging from mere pop-ups to
financial losses. Consequently, their run-time footprints across the system differ, impacting the optimal data source (Network, Operating
system (OS), Hardware) and features that are instrumental to malware detection. Further, the variations in threat levels of malware
classes affect the user requirements for detection. Thus, the optimal tuple of hdata-source, features, user-requirementsi is different
for each malware class, impacting the state-of-the-art detection solutions that are agnostic to these subtle differences.
This paper presents SUNDEW, a framework to detect malware classes using their optimal tuple of hdata-source, features,
user-requirementsi. SUNDEW uses an ensemble of specialized predictors, each trained with a particular data source (network,
OS, and hardware) and tuned for features and requirements of a specific class. While the specialized ensemble with a holistic view
across the system improves detection, aggregating the independent conflicting inferences from the different predictors is challenging.
SUNDEW resolves such conflicts with a hierarchical aggregation considering the threat-level, noise in the data sources, and prior domain
knowledge. We evaluate SUNDEW on a real-world dataset of over 10,000 malware samples from 8 classes. It achieves an F1-Score of
one for most classes, with an average of 0.93 and a limited performance overhead of 1:5%.
Index Terms—Dynamic Malware Analysis, Machine Learning for Security, Cross-dimensional Malware Analysis, Case-sensitive
Detection, Multi-input Ensemble
F
1 INTRODUCTION
MALWARE attacks against enterprises have proliferated
at an alarming scale. Industry analysis reports almost
17 million malware programs targeting businesses in 2021,
with estimated financial losses in billions [1]. The ramifications
of these attacks range from user-annoying popups to
ex-filtration of sensitive data, financial loss, extortion, and
even sabotaging critical infrastructures. Accordingly, malware
programs can be grouped into classes based on their
objectives and functionalities – Potentially Unwanted Applications
(PUA) pop up unwelcome advertisements; Bankers
stealthily steal financial credentials; Backdoors open hidden
access paths for a remote adversary; Spyware stealthily
exfiltrates sensitive data of its victim; Downloaders install a
malicious payload; Cryptominers mine cryptocurrencies for
the adversary while Ransomware encrypts the data victim
for extortion.
The diversity in malware classes can impact the datasource,
features and the user-requirements that are instrumental
in analyzing and detecting malware, as illustrated in Figure
1. First, the optimal run-time data-source that can detect a
malware class differs based on the functionality. Backdoors
maintain consistent communication with a remote adversary,
leaving strong indicators on the network, whereas spyware
are likely to leave indicators on the operating system
(OS) when they scan a large number of files. On the other
hand, ransomware are prone to trigger distinct hardware
events due to the encryption they perform. Second, malware
 S. Karapoola, N. Singh, C. Rebeiro and Kamakoti V. are with the
Department of CSE, Indian Institute of Technology Madras, Chennai,
India.
E-mail: fsareena, nik, chester, kamag@cse.iitm.ac.in.
Fig. 1: The optimal tuple hdata-source; features; user-
requirementsi varies for each malware class. The optimal
run-time data-source and features that can distinguish a
malware class from benign applications differ based on the malware
functionality. Further, low-risk malware (e.g., spyware)
typically have stricter classification thresholds than high-risk
malware (e.g., ransomware).
classes differ in the features that best identify them. For
example, in the OS, a high number of encryptions and writeto
files are indicators of ransomware, whereas a high rate
of file-system and registry reads are indicators of spyware.
Third, the user’s requirements vary for different malware
classes. For high-risk malware like ransomware, users are
more likely to tolerate false positives than low-risk malarXiv:
2211.06153v2 [cs.CR] 14 Nov 2022
2
P P P
Average or Majority
Final Prediction
Network OS
Data
sources
Source-level Aggregation
Final Prediction
Network OS Hardware
P P P P P P
Data input to
Predictors the predictors
(b) Single-input ensembles (c) Multi-input ensemble (Proposed in this paper)
Benign Malware
class A
Malware
class B P Preditctors
Different classes in the data
Model Aggregation Model Aggregation Model Aggregation
P
Final
Prediction
Network/
OS/
hardware
(a) Single
classifier
Fig. 2: Malware detection mechanisms based on (a) single classifier;
(b) single-input ensembles; and, (c) multi-input ensemble
(proposed in this paper), for an example case of three classes
(benign, malware classes A and B).
ware like spyware and PUAs. Thus, a model for high-risk
malware would ideally need lower classification thresholds
than low-risk malware. Hence, detecting a malware class
can benefit in accuracy and false-positive guarantees with
models trained with the optimal data-source (network, OS,
or hardware) and fine-tuned for class-specific features and
classification thresholds. In fact, we observe that the optimal
tuple of hdata-source; features; user-requirementsi is
unique for each malware class. Additionally, the optimal
tuple is also sensitive to the system load conditions, which
can infiltrate noise into the run-time data.
The intuitive approach to leverage the optimal tuple for
any malware class is to have different specialized predictors,
wherein each predictor is fine-tuned for a specific datasource,
class-specific features, and requirements. Na¨ıve solutions
include hyper-specialized predictors for classes such
as ransomware [32], which fail on other classes of malware.
On the other hand, most works in literature employ single
generic classifiers, that are too generalized to support
such a specialized handling of different classes (Refer to
Figure 2a) [2]–[12]. Alternative works explore an ensemblebased
classifiers for detection [13]–[23]. Such solutions train
a collection of predictors either in parallel or sequentially on
the same input data formed by combining data from one [13]–
[20] or multiple sources [21]–[23]. Figure 2b illustrates one
such single-input ensemble that trains predictors in parallel.
Primarily, these approaches aim to minimize the detection
error by averaging the predictions from the individual predictors
(when trained in parallel) or learning adaptively
(when trained in sequence). Either way, same-input ensembles
cannot achieve the optimal tuple for detection as they do not
support fine-tuning individual predictors to pursue classspecific
features and requirements.
Figure 2c illustrates our proposed ensemble, which we
call a multi-input ensemble of specialized predictors, wherein
each predictor is trained using a different input data-source
to detect a specific malware class from benign applications.
However, given an unknown program sample, such finegrained
specialization alone is not sufficient to leverage
the optimal predictor (i.e., achieve the optimal tuple) in
the ensemble. The primary challenge lies in arriving at a
consensus from the independent, conflicting predictions from
different specialized predictors for any input test program.
As each predictor is tuned differently, their inferences are
likely to conflict. Na¨ıve approaches such as majority or
average of individual predictions [13], [17], [18], [20] may
not be optimal. This is because each predictor has different
definitions of boundaries between malware and benign
behavior and deals with different noise levels in the input
from different data sources.
This paper presents SUNDEW, a detection framework
that employs a multi-input ensemble of predictors with an
insightful aggregation mechanism to leverage the optimal
tuple of hdata-source; features; user-requirementsi for
any input test program. SUNDEW has three components,
each utilizing different data sources across the system stack
to provide a holistic view of malware activity. Internally,
each component has multiple specialized predictors, each
tuned to differentiate a specific malware class from benign
applications using data collected from the particular data
source (network, OS, or hardware) (Refer to Figure 2c). To
resolve the conflicts between the predictors, SUNDEW uses
a two-level hierarchical structure of aggregator functions
that first collates the inferences from different specialized
predictors inside each component and later aggregates the
inferences from the three data sources (Refer to Figure 2c).
These functions employ a combination of predictor statistics,
prior knowledge of the capabilities of each predictor,
risk factor, and the current system load to aggregate inferences
to an optimal prediction. Thus, unlike prior works, the
holistic view of malware activity and specialization enable
SUNDEW to achieve a case-sensitive analysis and detection,
thus improving the accuracy and resilience while ensuring
class-specific false-positive guarantees. Following are the
contributions of the paper:
1) A reliable malware analysis framework, SUNDEW,
with a holistic view of malware activity across the
system, an ensemble of specialized predictors, and
aggregator functions to help derive the best-case
prediction for any malware class (Section 5).
2) An evaluation of various design choices for the aggregator
functions that resolve conflicts between the
independent specialized predictors. Given an unknown
sample, the two-level aggregation in SUNDEW
relays the inference of the specialized predictor
to the output without any loss, while boosting
the performance of the optimal predictor by at least
1.42% (Section 6).
3) An evaluation of SUNDEW on a rich dataset that
presents precise and comprehensive real-world malware
behavior of more than 10; 000 malware samples,
including cryptominers, bankers, spyware,
backdoors, ransomware, downloaders, deceptors,
and potentially unwanted applications (PUAs).
SUNDEW can achieve an F1-Score of 1 for most
malware classes, an average of 0:93 for any malware
class, and 0:82 even under highly noisy conditions,
with an average overhead as low as 1:5% at the endhost
machines (Section 7).
4) To the best of our knowledge, SUNDEW is the
first to provide a multi-input ensemble for a casesensitive
detection of malware classes. It evaluates
the run-time inputs from three system components,
risk factors, and the dynamic system noise, to detect
malware reliably. SUNDEW is 10% more accurate,
with 89% lower false positives, than prior state-of3
the-art predictors based on network [2], OS [19],
hardware [20] and ensembles [23] that do not consider
a holistic view of malware activity and multiinput
ensemble of specialized predictors [2]–[28].
Following is the organization of the rest of the paper.
Section 2 provides the necessary background for the paper.
Section 3 highlights the motivation for the need for an ensemble
of predictors. Section 4 presents the related work.We
discuss the high-level overview of SUNDEW in Section 5.
Section 6 discusses the use of different insights to effectively
aggregate predictions in SUNDEW. Section 7 presents the
implementation of SUNDEW and results of our evaluation.
Section 8 discusses the limitations of SUNDEW and future
work. Finally, Section 9 concludes the paper.
2 BACKGROUND
Malware are programs with malicious intents. With differing
attack objectives, they pose varying levels of risk to
system users. Ransomware that can sabotage an entire system
is a high-risk malware, whereas adware or potentially
unwanted applications (PUA) that are mere user-annoying
in nature are low-risk malware [29]. Table 1 presents a few
notable malware classes with their objectives and corresponding
risk levels.
Run-time data sources for malware detection. Malware
behavioral analysis and detection is a widely studied and
mature field [2]–[28], [30]. The detection mechanisms use
trails of malware activity, observable at different system
components which include – (1) Network (e.g. malware
communications to its command-and-control server); (2)
Operating system (e.g. system calls); and, (3) Hardware (e.g.
micro-architectural events).
These behavioral trails provide different abstractions
of malware behavior. The network trails provide insights
into malware communications to external entities, including
its command-and-control servers. The features of interest
include the unencrypted meta-data about connections, domains
contacted, TLS handshakes, and X.509 certificates
from HTTPS/HTTP flows. In contrast, the OS component
captures the malware interactions with the system software
when it attempts to remain stealthy, achieve persistence,
and execute its objective. These interactions include the file
system, registry, process, and network-related system call
traces of the malware. Though the system call traces include
network communications (TCP Send/Receive), they are at
a higher abstraction as compared to that captured at the
network component. Additionally, malware activities also
trigger specific micro-architectural events that can be observed
using special registers called Hardware Performance
Counters (HPCs) [31]. A few notable HPC events used
for malware detection include cache hits/misses at various
levels, branch instruction response, and CPU activity. These
behavioral trails are used to train detection models that predominately
rely on machine learning (ML) to differentiate
malicious and benign behavior.
Relevance of HPC in malware detection. Though the use
of HPCs for malware detection is much debated, they are
found to be beneficial when used in the right way, using interrupt
and context switch management [32]. Additionally,
TABLE 1: Objectives and risk levels of various malware classes
Malware
Class Objectives Risk
level [29]
User
requirements
Cryptominer
Exploit computing resources of
the victim to mine
cryptocurrencies
High High TPR
Banker Steal financial credentials High High TPR
Spyware
Infiltrate and keep gleaning
sensitive information for an
extended period
Medium Low FPR
Backdoor
Grant alternate covert pathways
to system resources, bypassing
access control
Very
High High TPR
Ransomware
Sabotage user files and extort a
ransom from the user for
restoration
Very
High High TPR
PUA
Pop-up annoying
advertisements and
inappropriate content
Low Low FPR
Downloader
Covertly download other
malware from a remote server
to execute and infect
Low Low FPR
Deceptor
Bypass the system with close to
benign behavior and
adware-like payload
Low Low FPR
with minimal instrumentation required in the HPC trails
before they can be fed to the models and their limited performance
overheads, HPC-based detection enables a tradeoff
between accuracy and overhead in malware detection.
User Requirements. The detection models are fine-tuned
for an optimal trade-off between two orthogonal user requirements
– (1) a high true-positive rate (TPR), with some
tolerance to mispredictions, to detect as many malware as
possible; or a (2) a low false-positive rate (FPR), with no tolerance
to mispredictions, to prevent any impact on benign
applications. These requirements are, in turn, dependent on
the risk level of the malware. Users would prioritize a high
TPR for high-risk malware while preferring a low FPR for
adware/PUA that are very similar to benign applications
in behavior. Table 1 provides preferred user requirements
based on the risk level of different malware classes.
3 MOTIVATION
In this section, we analyze the differences in malware runtime
activity and present our observations that motivate
the need for a case-sensitive analysis of malware. To this
end, we study the difference in the behavior of malware
classes from benign applications observable across the three
data-sources. For each class, we assess the difference using
specialized binary ML models, each trained with behavioral
data of 1000 programs, including the malware class and
benign applications. Based on our observations, we make
the following claims on the benefits of exploring multiple
data-sources and class-specific features and addressing userspecific
requirements.
C-1: Detection can benefit in performance from a crossdimensional
view of malware activity. Malware classes
differ in their actions. In turn, the actions determine the
quantum of malware activity across the system: network,
OS, and hardware. Figure 3 indicates the distinguishability
of activities of different malware classes from benign
applications across the three data-sources. The darker the
4
Fig. 3: Differences between malware and benign behavior for different
classes across the three data data-sources. The darker the color, the more
distinguishable are the activities from benign applications.
color, the more distinguishable are the activities from benign
applications. The network shows a high distinction
in activities for banker, backdoor, and deceptor, whereas
OS shows distinguishable activity for ransomware and spyware.
Similarly, hardware shows distinct activity in the case
of ransomware. Hence, some classes are better aligned to
be detected using a specific data-source than others. We
explain this with an example of backdoor, spyware, and
ransomware (Refer to Figure 1). A backdoor creates a reverse
shell, escalates privileges, and provides code injection
capabilities to a remote adversary. However, the constant
factor in its attempt to execute any command from the
adversary is its sustained communication with the remote
server. We observe that the average duration of network
flows for a backdoor is notably different from other classes.
On the other hand, spyware aims to gather information
about the victim. Hence, it scans the filesystem, leaving
distinct trails at the OS, while its network activities are
not significantly distinguishable from benign applications.
Similarly, ransomware scans the files at the victim to encrypt
and make them inaccessible. In the process, a high rate of
reading and writing files is observable at the OS. However,
the high encryption rate leaves a significant fingerprint of
micro-architectural events visible at the hardware. Hence, it
is beneficial to explore multiple data-sources to get a complete
picture of malware activity, and build appropriate defenses.
C-2: Detection can benefit in resilience by employing
all data-sources. The data collected at the network and
hardware is affected by other processes executing in the
system. With an increase in system load (i.e., the number of
processes), the network communications of other processes
get induced into the network data. Similarly, other processes
sharing micro-architectural resources in the system can affect
the hardware performance counters. In contrast, the
OS data is collected only for the specific PID and hence is
agnostic to the system load. Thus, it is beneficial to employ
multiple data-sources for resilient malware detection.
C-3: Detection can benefit from class-specific features.
Within each data-source, malware classes differ in the features
that best express their maliciousness. As an example,
in hardware, the performance counter event, which counts
the number of switches from the Decode Stream Buffer
(DSB) to the Micro-instruction Translation Engine (MITE),
DSB2MIT SW CNT is empirically one of the most important
features in classifying ransomware from benign applications.
However, for spyware, the corresponding feature is
the event M LD Ret L1Hit, which counts retired loads that
encounter a hit in the L1 cache in a specific cache coherency
state. Thus, it is beneficial to have predictors specialized for class-
[12]
[2] [3] [4]
[5] [6] [7]
[8] [9] [10]
[11] [24] [25]
[26] [27] [28]
[30] [33]
[13] [14] [15]
[16] [17] [18]
[19] [20]
[21] [22] [23] No existing work
No existing work
No existing work No existing work SUNDEW
(this paper)
N+OS N+OS+H
One
(N /OS / H
Single
Classifier
Same-input
Ensemble
Multi-input
Ensemble
TYPES OF DETECTION MODELS
DATA SOURCES
Network (N), Operating System (OS),
Hardware (H)
Specialization
False-Positives
Resilience
Accuracy
(II)
(I) (IV) (VII)
(V) (VIII)
(III) (VI) (IX)
Fig. 4: Prior detection mechanisms differ in specialized handling of
malware classes based on the run-time data source and the machine
learning models they employ.
specific features rather than a generic approach.
C-4: Detection can benefit if fine-tuned to appropriate user
requirements. System users respond differently to different
malware classes. For instance, users would want to kill
ransomware as soon as possible to restrict further damage.
Hence, models for high-risk malware (e.g., ransomware,
backdoor) target a high true-positive rate to detect every
malware sample, while tolerating some false positives. In
contrast, users are comparatively lenient to low-risk malware
(e.g., PUA and deceptor) that are mere user annoying
in nature. Users would prefer to kill such malware only if
the prediction is precise to reduce any impact on benign
applications. Thus, false positives are a concern for such
malware. The predictor classification threshold controls the
trade-off between the true-positive rate (TPR) and the falsepositive
rate (FPR). High-risk malware would require lower
thresholds to promote high TPR, whereas low-risk malware
would require stricter thresholds to reduce FPR. Hence, it
is beneficial to have predictors specialized for class-specific user
requirements.
In essence, the optimal tuple hdata-
source; features; user-requirementi for each malware
class is different, making it essential to have a holistic view
of the data-sources and specialized models to improve
detection efficiency.
4 RELATED WORK
Malware analysis and detection using run-time behavior
have been extensively explored in literature [2]–[28], [30],
[33]. Figure 4 compares the highly cited prior works in the
last decade based on the run-time data sources and the
machine learning models they employ to cater to different
malware classes.
Run-time data sources. Most prior works employ run-time
data from a single system component to detect malware
(refer to cells I and IV in Figure 4). These include trails
from the network [2], [4], [6], [7], [13]–[16], [24], [25], operating
system (OS) [5], [11], [19], [28], or hardware [3], [8]–
[10], [17], [18], [20], [26], [27], [30], [33]. Alternatively, few
works employ a combination of features extracted from the
network and OS (refer to cells II and V in Figure 4 [12],
5
[21]–[23]). While the multiple inputs from network and OS
can increase the detection accuracy, these solutions can miss
indicators of classes like ransomware that have high microarchitectural
activity (refer to C-1 in Section 3). Unlike prior
solutions, SUNDEW employs a comprehensive view of malware
activity across the computing stack (cell IX), including
hardware, thus improving the accuracy and resilience of
detection (refer to C-1 and C-2 in Section 3).
Generic vs. Specialized models. State-of-the-art solutions
in dynamic analysis use different detection models, as
shown in Figure 2 and in the columns of Figure 4. While
these models can provide a binary or multi-class prediction,
internally, they either use a single classifier (cells I and II) or
a same-input ensemble of predictors (cells IV and V). A single
classifier can use data from a single (cell I [2]–[11], [24]–[28],
[30], [33]) or multiple sources (cell II [12]). Though relatively
light-weight, most single predictors are too generalized to
support any specialization [2]–[12], [24]–[28], [33]. On the
other hand, hyper-specialized predictors [30] that are finetuned
for specific classes, such as ransomware, can fail on
other classes of malware.
Alternatively, a same-input ensemble (cell IV) consists
of multiple predictors, wherein all predictors train on the
same data. These predictors are trained either in sequence
or parallel to improve the predictive performance [13]–
[20]. While the former approach trains the predictors sequentially
in an adaptive manner [14]–[16], [19], the latter
employs all predictors in parallel and outputs the average
of their predictions [13], [17], [18], [20]. Inherently, such
ensembles do not support training predictors with different
data pertaining to a specific class and benign behavior. The
addition of alternate data sources [21]–[23] (cell V) does
not help either, as the features from different input sources
are transformed to a single representation before feeding to
the model. Unlike prior works, SUNDEW proposes a multiinput
ensemble of predictors, wherein each classifier trains
on a different run-time data source and malware class and is
specialized to maximize the class-specific user requirement
(cell IX).
When predictors in a same-input ensemble (that are
trained in parallel) test any given unknown sample, their
independent predictions are aggregated (e.g., by averaging)
to minimize the cumulative errors of the individual predictors
and form a final prediction [13], [17], [18], [20],
[22]. In contrast, aggregation in the multi-input ensemble
of SUNDEW, aims to relay the inference of the specialized
predictor corresponding to the given sample to the final
output. Challenges in such an aggregation are two-fold.
First, the individual predictors in a multi-input ensemble
deal with different noise levels in the input data from
different data sources. Second, the predictors have different
definitions of positive and negative class. We discuss how
SUNDEW addresses these aggregation challenges to relay
the optimal prediction of the specialized predictor to the
final output in Section 6. Other industrial solutions such
as [34] present class-specific behavioral analysis similar to
the goals of SUNDEW. However, based on our limited
understanding, these closed-box solutions do not support
aggregation to present a final prediction of the input sample.
Thus, the comprehensive view of malware run-time activity,
the multi-input ensemble and the aggregation mechanism
ensure that SUNDEW is (1) more accurate in detecting
malware (unlike [2]–[28], [33] that do not exploit C-1 and
C-3); (2) more resilient to infiltrating noise (unlike [2]–[4],
[6]–[10], [13]–[18], [20], [24]–[27] that do not exploit C-2);
(3) can support class-specific false positives (unlike [2]–[28]
that do not exploit C-4); and, (4) caters to a diverse set of
malware classes (unlike [30]).
Comparison with SIEM. The comprehensive analysis in
SUNDEW has similarities with industrial efforts such as System
Information and Event Management (SIEM) [35] that
consider a holistic correlation of events across an enterprise
to detect threat scenarios. However, there are differences
between the two. First, SIEM collects data from diverse
sources such as antivirus software, security appliances, firewalls,
and other organizational applications, including human
interactions (such as repeated access attempt failures).
It correlates these events against pre-defined rules to detect
threats and create alerts. In contrast, SUNDEW is an alternative
input source to SIEM that can replace the antivirus
software to provide accurate and resilient detection of malware
activities. Second, the machine learning ensemble in
SUNDEW is more sophisticated than the correlation rules
predominantly used in SIEM and thus capable of detecting
zero-day threat scenarios.
Comparison with multi-input solutions in static analysis.
Few prior efforts based on static analysis have explored
malware detection using multi-input ensembles [36], [37].
These solutions employ a heterogeneous ensemble of predictors,
where each classifier trains on different static features
extracted from the malware binary. However, unlike
dynamic analysis, static techniques can be easily evaded by
polymorphic and metamorphic malware that are popular
today. To the best of our knowledge, SUNDEW is the first
multi-input ensemble for dynamic analysis, considering a
comprehensive view of run-time activity to provide a casesensitive
detection of malware.
5 THE SUNDEW FRAMEWORK
In this section, we first present a high-level overview
of SUNDEW followed by a formal description of the multiinput
ensemble.
5.1 High-Level Overview
SUNDEW is a multi-input ensemble of predictors that leverages
the optimal tuple of hdata-source, features, user-
requirementsi for accurate and resilient detection of any
malware class. Figure 5 presents a high-level overview of
the working of SUNDEW. The ensemble has a component
for each data-source, namely, network, OS, and hardware
(shown by the dashed boxes in Figure 5). During program
execution, a collection engine collects the program’s
network, OS, and hardware behavioral data and invokes
the respective components. Internally, each component has
multiple predictors, each of which is specialized for different
malware classes with class-specific features and user
requirements (refer to C-3 and C-4 in Section 3). Each
predictor is a binary classifier trained for a specific class,
6
Fig. 5: Multi-input ensemble of predictors in SUNDEW. The
ensemble consists of three components corresponding to each data
source (network, OS, hardware). Internally, each component has a
specialized predictor for each class. MA() aggregates the inferences of
the specialized predictors inside each component, whereas the CA()
aggregates the inference of the three components to make the final
output of SUNDEW .
where it can infer if the program is of the malware class
of its specialization or benign. For example, a backdoorspecialized
predictor predicts if a program is a backdoor,
likewise, a PUA-specialized program predicts if a program
is a PUA.
The predictors are likely to output conflicting inferences.
Primarily, each specialized predictor has a different
definition of the boundary between benign and malware.
Figure 6 shows the distribution of probability estimates of
different specialized predictors in the network component
when they are tested with benign applications (green boxes),
and the class of their specialization (dark-red boxes). As
evident these distributions overlap. Thus, due to similarities
between deceptor and some benign applications, the
backdoor-predictor might infer a deceptor program as benign
while, the deceptor-predictor infers it as a deceptor.
Further, while a predictor can predict its class of specialization
with high confidence (dark-red boxes), it predicts
other classes as malware with varying likelihood (light-red
boxes). As SUNDEW starts with no notion of the program’s
class; the challenge lies in choosing the right prediction
from the set of independent predictions. SUNDEW addresses
this challenge by aggregating predictions using a
configurable model-aggregator within each component. The
model-aggregator MA() multiplexes the output of the bestcase
specialized predictor to the output. For this, it leverages
predictor statistics (e.g. probability estimates) and prior
knowledge to assess the confidence of inferences inside each
component. Prior knowledge can include the capabilities of
components to reveal certain classes as observed during the
training phase. For instance, prior knowledge that hardware
trails have strong indicators of ransomware can help assess
the confidence of a ransomware-specialized predictor in
hardware. Based on the statistics and prior knowledge, MA()
computes the confidence score of each predictor and relays
the most confident inference as the output of the component.
The outputs from the three MA()s are likely to differ on
the class of the program due to two reasons. First, each
data-source varies on its capability to distinguish a specific
malware class from benign (Refer to Figure 3 and Claim
C-1 in Section 3). Second, the data-sources have varying
levels of noise from other processes running in the sys-
Fig. 6: Distribution of probability estimates of different specialized
predictors in the network component, when tested with data of benign
class (green), the corresponding specialization (red), or from any other
class (light red). Each predictor has different definitions of class boundaries.
tem (Refer to Claim C-2 in Section 3). For this, SUNDEW
uses a component-aggregator function CA() to aggregate
the outputs from the model-aggregators and yield the most
confident inference as the final classification output. Similar
to MA(), it exploits the components’ statistics (the output
confidence score from MA()) and its prior knowledge. Its
prior knowledge can include a broader understanding of the
distinguishing capabilities of different data sources. Further,
the CA() also checks the system load and considers the
output of a resilient data source, with the least infiltrating
noise for aggregation to the output. At both model and component
aggregators, multiple predictors/components likely
can end up with similar confidence scores. In such scenarios,
both MA() and CA() leverage the known risk-level [29] of the
classes as a tie-breaker. They choose the riskiest class as their
aggregated inference to resolve the tie. For instance, if two
conflicting inferences, backdoor and deceptor have similar
confidence scores, the aggregators choose the higher-risk
class (backdoor) of the two.
Thus, given any input test program, relaying the predictive
benefits of the corresponding specialized predictor
(that employs the optimal data-source, features, and userrequirements)
to the output of SUNDEW is important to
improve accuracy and resilience. The choice and weights
of statistics and prior knowledge control this relay and the
effectiveness of the aggregation. We explore this aspect and
evaluate different designs for MA() and CA() in Section 6.
5.2 Formal Description of SUNDEW
Let B = hD;M;P; Ai represent the SUNDEW ensemble
(Refer to Figure 5). D = fN; O; Hg is the set of components
(i.e. data sources), namely network (N), OS (O) and hardware
(H). M= fm1; m2; : : : mng is the set of n malware classes. P
is the set of specialized predictors, while A is the set of
aggregator functions. Algorithm 1 describes SUNDEW. It
takes as input a program z from the set of programs Z to
test. For each component k, it first gets the behavioral data
for the program (Line 5).
Behavioral data. Given a program z, its behavioral data in
component k 2 D corresponds to a time series of snapshots
collected during the execution of the program. These snapshots
are captured at different granularities across components,
as shown in Figure 7. At the network, we log the
7
Fig. 7: Behavioral snapshots collected at different granularities
at network, OS, and hardware components.
data for every flow1 while we log every system call at the
OS. The hardware component logs the HPCs at a fixed time
interval of 100ms. The collected data is pre-processed and
converted to a matrix dz;k, with features as columns and
rows representing each snapshot (Line 5). These rows are
labeled with the class of the program, mj 2M; 8j 2 [1; n].
Prediction. After getting the behavioral data, Algorithm 1
invokes all the predictors in P, each of which is specialized
to detect one of the classes inM(Line 6). P is given by,
P = fPk;j(); 8k 2 D; 8j 2 [1; n]g ; (1)
where predictor Pk;j is specialized in component k to classify
a program as malware mj or benign. These predictors are
trained to predict row-wise inferences along with the probability
estimate of a row in dz;k being malicious. Accordingly,
the probability-estimates for dz;k is the cumulative average
of probability estimates of its rows. As the predictors
are trained row-wise, the prediction of a test program might
contain some rows inferred as malware (malicious) while
others as benign. Interestingly, the percentage of malicious
rows per program, i.e. malicious-row-percentage varies
for different classes (Refer to Figure 8). Accordingly, the
specialized predictors Pk;j() are fine-tuned to these classspecific
thresholds (in Figure 8) to conclude the class of the
test program. For instance, the network-based backdoorspecialized
predictor infers a program as malware if at
least 40% of the rows are identified as malicious. Similarly,
spyware-specialized predictor infers malware if at least 30%
of the rows in the program are malicious.
Hence, for data dz;k of a program z in component k, Pk;j
outputs a tuple of its prediction rk;j and statistics sk;j as
follows:
hrk;j; sk;ji = Pk;j(dz;k); 8k 2 D; 8j 2 [1; n] ; (2)
where,
rk;j =
(
1 , for a malware of class mj
0 , for a benign program
; (3)
and, sk;j is a tuple of h probability-estimates,
malicious-row-percentagei. The statistics sk;j is an indicator
of confidence of the prediction of rk;j. Thus, the
output from the predictors in component k are the set
of predictions (Rk = frk;j; 8j 2 [1; n]g) and statistics
(Sk = fsk;j; 8j 2 [1; n]g) (Line 7 in Algorithm 1). Each
element in these sets corresponds to a malware class.
1. All communications having the same source and destination IP
address, and source and destination port belong to a flow. Thus the
network packets are grouped into traffic flow summaries
Fig. 8: Percentage of malware rows per program varies for
different classes, across the three components (data sources).
Algorithm 1: SUNDEW
Input: z: Program to test
Result: h^rB; ^cBi: Final label and confidence.
1 begin
2 D   fN; O; Hg components
3 PriorKnowledge   Prior knowledge on predictors
in P
/* Aggregating predictions at model-level
*/
4 for k 2 D do
5 dz;k   Behavioral data of z in component k
6 Pk   Specialized predictors in k
7 hRk; Ski   Pk(dz;k) . Get predictions
8 Expertk   Expert set of predictors in k .
(derived from PriorKnowledge)
9 h^rk; ^cki   MA (Rk; Sk; Expertk)
10 . Highly confident prediction in Rk
/* Aggregating predictions at components
Level */
11 ^R
  f^rk, 8 k 2 Dg . Prediction of each component
12 ^C
  f^ck, 8 k 2 Dg . Confidence of each component
13 ^E
 
fPrior-known strength of component k to predict ^rkg
. Derived from PriorKnowledge
14 L   Number of processes in the host machine .
System load
15 h^rB; ^cBi   CA (^R
;^C
;^E
; L)
16 . Highly confident prediction among components
17 return h^rB; ^cBi
Aggregation. The independent predictions in Rk are likely
to conflict, as shown in Table 2A, which shows an example
output of the specialized predictors in the network component
when tested with a backdoor sample. While four predictors
(e.g., the ones specialized for banker and backdoor)
detect the sample as malware, others predict it as benign.
The conflicts arise as each predictor has different estimates
for maliciousness (Sk), including probability-estimates (Figure
6) or the number of malicious rows in a program
(Figure 8). Similarly, the components may differ in their
prediction (Refer to Table 2B), primarily due to the varying
noise levels that affect the probability-estimates. While the
network component flags the sample as malware, OS and
hardware component predicts it as benign. For an unbiased
aggregation, while leveraging the benefit of the optimal
predictor in each component, SUNDEW adopts a two-level
aggregation as shown in Figure 5. Thus, A = fMA(); CA()g
is a set of aggregator functions that compare the confidence
of each predictor to resolve conflicts inside each component
and among the components. In either case, the statistic Sk
is not sufficient, as aggregating based on the maximum or
average of Sk may not relay the optimal prediction (gray
cells) to the output (red cells) in most cases (Refer to Table 2).
8
TABLE 2: (A) Conflicts among specialized predictors when
tested with a backdoor sample in the network component.
While four specialized predictors (e.g., banker, backdoor) detect
the sample, four others predict it as benign. Naive aggregation
schemes based on Sk (probability) may not be optimal to relay
the output of the backdoor-specialized predictor (gray cell)
to the output (red cells). Similarly, (B) illustrates the conflict
among different components about the sample. While the
network component detects it as malware, OS and hardware
components flag it as benign.
(A) Output from predictors inside a component Aggregation
Specialized
predictor
Cryptominer
Banker
Spyware
Backdoor
Ransomware
PUA
Downloader
Deceptor
Majority?
Maximum
probability
Average
probability
Prob-1 0.47 0.38 0.33 0.73 0.33 0.34 0.19 0.49 - 0.73 0.41
Prob-0 0.53 0.62 0.67 0.27 0.67 0.66 0.81 0.51 - 0.81 0.59
Prediction 1 1 0 1 0 0 0 1 ? 0 0
(B) Output from each component Aggregation
Network OS Hardware Majority? Maximum Average
Prob-1 0.73 0.58 0.20 - 0.73 0.50
Prob-0 0.27 0.42 0.80 - 0.80 0.49
Prediction 1 0 0 0 0 0
* Assuming each component outputs its best prediction.
Prediction of 1 indicates malware class, and 0 indicates benign class. Accordingly,
Prob-1 indicates the probability of the sample being malware. Prob-0 indicates
the probability of the sample being benign.
Prior knowledge. To validate the confidence put forward by
Sk, SUNDEW also leverages the knowledge built using past
experience or domain insights. Thus, PriorKnowledge(Pk;j)
is a comparative measure of prior-known efficiency of predictor
Pk;j to detect malware mj in component k, 8k 2 D,
8j 2 [1; n]. An example of such a measure is the F1-
Score of Pk;j() observed in the train-validate phase or past
deployments of SUNDEW. Based on this measure, some
predictors are experts (more confident than others) in a
component. For instance, backdoors’ operations are known
to be network-intensive from domain insights. Thus, if the
backdoor-predictor in the network component predicts a
program as a backdoor, its prediction is likely to be the
most accurate. Accordingly, the expert set Expertk of a
component k (Line 8 in Algorithm 1) is the set of predictors
having high prior-known strengths in k, given by,
Expertk = fmj; j PriorKnowledge(Pk;j) > ; mj 2 Mg ;
(4)
for a domain-specific configurable threshold .
Model-level Aggregation. To deduce the most confident
prediction from the output of all predictors inside a component
k (i.e., Rk = frk;j; 8j 2 [1; n]g, per Equations 2
and 3), SUNDEW invokes the function MA() with inputs:
the predictions (Rk), statistics (Sk = fsk;j; 8j 2 [1; n]g), and
expert set (Expertk) of predictors in k (Line 9). Internally,
MA() uses Sk and Expertk to evaluate the confidence of each
prediction in Rk, and return ^rk and ^ck, the most confident
prediction of the component and its confidence measure
(Line 9).
Component-level aggregation. At the components level,
SUNDEW has three independent predictions ^R
from each
MA() in Line 9, wherein ^R
= f^rk, 8 k 2 Dg (Line 11). Similarly,
SUNDEW has their corresponding confidence values
^C
= f^ck, 8 k 2 Dg (Line 12). As the predictions in ^R
are
likely to differ, SUNDEW again leverages the prior known
strengths of the specialized predictor for the predicted malware
class ^rk 2 ^R
in component k, given by,
^E
= f PriorKnowledge(Pk;^rk ) 8k 2 Dg : (5)
These scores are indicative of the confidence of a component
k in predicting ^rk. As noise induced by system load could
affect detection, SUNDEW also considers the system load,
L. For example, L can be the number of processes executing
at the host machine (Line 14). Finally, SUNDEW invokes
the component-aggregator CA() to aggregate the three predictions.
The function CA() takes as input the predictions
(^R
), confidences (^C), and prior-known strengths (^E
), and the
system load (L). Similar to MA(), it evaluates the confidence
of each component, and outputs the highly confident prediction
(^rB) and its confidence (^cB), as the final output of
SUNDEW (Line 15). In the next section, we discuss how
SUNDEW builds insights from predictor statistics and priorknown
strengths for case-sensitive detection of malware.
6 INSIGHTFUL AGGREGATION OF PREDICTIONS
The functionalities of the model and component aggregators
are different. A model aggregator (MA()) resolves conflicts
among predictors that use the same data source to test
a sample, but have different definitions of the boundary
between positive and negative classes. In contrast, a component
aggregator (CA()) resolves conflicts between predictors
that use different data sources (having varying levels
of noise) for the same sample. While the predictions can
be aggregated in different ways, the optimal aggregation
mechanism for different malware classes and components
varies due to the differences in the boundary definitions
and the noise levels. For instance, naive comparisons such
as majority [18], [22] or averaging of statistics [13], [20]) may
not lead to the optimal aggregation for all malware classes.
Thus, SUNDEW leverages a configurable MA() and CA()
that explore different mechanisms on test-time predictor
statistics, prior-known strengths, and system load to relay
the optimal prediction as the final output, as discussed next.
6.1 Model-Aggregator
Algorithm 2 describes the model-aggregator MA() function.
For a given component k, it takes as input the predictions
(Rk), corresponding statistics (Sk), and expert set of predictors
Expertk. As each predictor has different definitions
of malware-benign boundary (Figure 6), MA() aggregates
predictions in a two-step process, by achieving consensus
first on the maliciousness of the program and then on the
specific class of the malware.
Binary consensus on maliciousness. Firstly, MA() assesses
the predictions and statistics to vote if the
test program is malware or benign using a function
ConsensusIfMalware()(Line 2 of Algorithm 2). Multiple alternatives
are possible for realizing ConsensusIfMalware(),
including consensus based on logical-OR, majority-vote,
confidence, or learning. Naive mechanisms infer malware
if at-least one of the predictions (logical-OR), or most predictions
are malware (majority-vote). However, such approaches
can significantly increase the false positives.
9
Algorithm 2: Model-Aggregator (for component k)
Input: Rk = frk;j, Predictions from Pk;j8j 2 [1; n]g,
Sk = fsk;j, Statistics from Pk;j8j 2 [1; n]g,
Expertk: Expert set of component k.
Result: h^r; ^ci: Final vote and confidence.
1 begin
// Check if predictions in R indicate
malware
2 (label, probmalware)  
ConsensusIfMalware(Rk; Sk)
3 if label is malware then
// Identify the set of most confident
predictor(s) from R | rk;j is 1.
4 C Set   GetConfidentSet (Rk; Sk; Expertk)
5 ^r   Malware class of the most risky in C Set
6 ^c   Probability estimate of ^r
7 else
8 h^r; ^ci   hbenign , 1 􀀀 probmalware)i
9 return h^r; ^ci
A most-confident ConsensusIfMalware() infers malware
if the aggregated confidence of malware predictions is
higher than benign predictions. It aggregates confidences
using the mean-probability-difference, which is the mean difference
between the probability-estimates of malware (Prob-
1 in Table 2) and benign class (Prob-0) of all predictors.
It infers malware when mean-probability-difference > 0,
and benign otherwise. Another potential metric to aggregate
confidences is mean-maliciousness-difference, which is
the mean difference between the percentage of malicious
and benign rows inferred for a program by all predictors.
However, we find that mean-maliciousness-difference is
sub-optimal for ConsensusIfMalware() as the percentage
of malicious rows varies for each class (See Figure 8).
Alternatively, a learning-based ConsensusIfMalware()
uses trained models to infer malware. Two configurations
are possible for such models. A booster learns to minimize
the loss function of the specialized predictors. On the other
hand, a multiplexer learns to multiplex the output of the
specialized predictor to the final output of SUNDEW. Both
these configurations train their models with the predictor
outputs (Refer to Equation 2) observed for all programs z 2
Ztrain, which is the set of programs in the training phase.
Thus, these models train on Xk = [fPk;j(dz;k); 8j 2M; 8z 2
Ztrain] to predict target labels Yk = [yz;kjyz;k 2 M; 8z 2
Ztrain]. The target label yz;k for a program z is different for
booster and multiplexer. As the booster minimizes the loss
function of the component, its yz;k is the actual-class c 2M
of the program z. On the other hand, the multiplexer aims
to relay the best-case prediction of z to the output. Thus, its
yz;k is the predicted class of z when tested on the predictor
specialized for class c in component k.
Table 3 enlists the aggregation-loss of MA(), which is the
difference between the F1-Score of ConsensusIfMalware()
as compared to the baseline, i.e., the F1-Score achieved by
a specialized predictor that is optimum for the program.
An aggregation-loss of zero indicates that MA() is able to
relay the inference of the optimum specialized predictor
to the output of SUNDEW. On the other hand, negative
loss indicates that MA() can boost the detection performance
TABLE 3: Aggregation-loss with different alternatives for MA()
and CA(). The comparison baseline is the F1-Score of the specialized
predictor that is optimum for the program. An aggregation-loss of zero
indicates that the MA() and CA() can relay the prediction of the optimum
specialized predictor to the final output for any program. A negative
loss indicates that the aggregation is able to improve the detection
performance beyond that of the specialized predictor.
Network OS Hardware
Baseline 0% 0% 0%
Logical-OR 41:92% 80:05% 80:05%
Majority-vote 33:66% 6:81% 68:82%
Most-Confident 59:04% 7:12% 7:12%
Multiplexer 5:18% 4:64% 4:64%
Binary
Consensus
IfMalware()
Booster 4:76% 􀀀0:28% 􀀀1:35%
Network OS Hardware
Baseline 0% 0% 0%
Confidence 4:75% 21:51% 0:56%
Prio-knowledge 34:16% 􀀀1:74% 0:64%
MA()
Multi-Class
GetConfidentSet()
Confidence-Window 4:79% 21:51% 􀀀2:18%
Binary Multi-Class
Most-Confident 􀀀1:42% 12%
CA()
Prior-Knowledge 􀀀1:42% 7:86%
Fig. 9: Probability distribution of different specialized predictors in the
OS and hardware component, when tested with data from the benign
class (green), their corresponding specialization (red), or any other class
(light red). The distributions overlap significantly in OS as compared
to that in the network (Figure 6) and hardware, making confidence
(probability-estimate), a poor metric to aggregate class in MA().
beyond that of the specialized predictor by minimizing its
loss function. Naive voting mechanisms such as logical-OR
or majority-vote have high aggregation-losses. While the
logical-OR function leads to high false positives, majorityvote
fails when less than half of the predictors can detect
the malware sample. Similarly, the performance of the mostconfident
ConsensusIfMalware() can be sub-optimal as the
range of probability estimates that differentiates malware
and benign are different for each specialized predictor (refer
to Figure 6). In contrast, the learning-based mechanisms can
learn these class-specific probability estimates effectively to
reduce aggregation losses. Specifically, the booster improves
the performance of both OS and hardware components by
at least 1% while reducing the aggregation-loss to as low as
4:76% in the network component.
Multi-class consensus. After consensus on the maliciousness
of the program, the probability estimates (Sk) of individual
predictors could help identify the most confident
predictor, and hence the class. However, these estimates get
unreliable when a specialized predictor attempts to predict
on data of any other class (light-red boxes in Figure 6).
The figure plots these estimates of different specialized
predictors in the network component. Figure 9 plots the
corresponding distributions in the OS and hardware component
when tested with data of benign class (green), the
corresponding specialization (red), or from any other class
10
Fig. 10: Detection F1-Score observed for different classes for
different alternatives for MA() at network, OS, and hardware.
The baseline for comparison is the F1-Score achieved with the
specialized predictor optimum for each class.
(light red). The overlapping distributions especially in the
OS and hardware component makes identifying the malware
objective class non-trivial. Alternatively, some predictors
have a confidence-window, wherein the aforementioned
distributions (distance between their inter-quartile range)
are far apart (e.g., spyware in network component in Figure
6). If the test-time statistics of a predictor fall in such
confidence windows, the predictor can be considered highly
confident. As evident, confidence windows are beneficial in
the network (Figure 6) or hardware component (Figure 9),
whereas in OS, they can get unreliable. An alternative is to
employ prior-knowledge and prioritize predictions of the
expert-set alone (refer Equation 4).
Accordingly, if the test program is a malware, MA()
evaluates the confidences of all predictions rk;j 2 Rk which
predicted malware (i.e. rk;j = 1 in Equations 2 and 3). For
this, it employs a configurable function GetConfidentSet()
that evaluates the statistics (Sk) and expert set (Expertk)
to return a confident set of predictor(s), C Set (Line 4 of
Algorithm 2). To compute such a set, GetConfidentSet()
can use one of the following metrics: (1) confidence, that
returns the predictor with high probability-estimates; (2)
prior-knowledge, that prioritizes expert predictors in Expertk
to choose predictors with high probability-estimates; or (3)
confidence-window, that returns the predictors whose statistics
fall within their respective confidence window. These
options can return a set of confident predictors. To resolve
the contention in C Set in such cases, MA() prioritizes the
classes in accordance to the risk categories [29] and outputs
the most risky class in C Set as the prediction of the
component (Line 6).
Figure 10 evaluates the F1-Score of MA() when
tested with any program, for different alternatives of
GetConfidentSet(), against the baseline F1-Score achieved
with the corresponding specialized-predictor that is optimum
for the program. The detection is inferred as correct if
the risk level of the predicted class is the same or higher than
that of the program. MA() can restrict the aggregation-losses
to as low as 4% at the component outputs to aggregate the
class for any program (Refer to Multi-class row in Table 3).
While the confidence metric is the most effective for the network
component, prior-knowledge and confidence-window
metrics are effective for the OS and hardware, respectively.
Algorithm 3: Component-Aggregator
Input: ^R
= f^rN; ^rO; ^rHg:Predictions from MA()for all k 2
D, ^C
=
f^cN; ^cO; ^cHg:Confidences from MA() for all k 2 D,
^E:
Prior-knowledge, L
:
System Load (
Number of
processes in the system).
Result: h^rB; ^cBi: Final label and confidence.
1 begin
2 if L <  then
// At low system loads, all components
are reliable.
3 C Set   Compute confident set(^R;^C;^E)
4 ^rB   Class of the most risky among C Set
5 ^cB  
Confidence of the most risky among C Set
6 else
// At higher system loads, OS is the
most reliable.
7 h^r; ^ci   h^rO; ^cOi
8 return h^rB; ^cBi
6.2 Component Aggregator
The CA() is responsible for choosing the most confident
prediction in ^R
= f^rk, 8 k 2 Dg, where ^rk is the output
prediction aggregated by the MA() in each component k
(Line 9 of Algorithm 2). CA() can weigh components based
on the empirical confidence of their prediction observed at
test-time (^C
= f^ck, 8 k 2 Dg (Line 9 of Algorithm 2)), or
their prior-known strengths (^E, Equation 5) in predicting
^R.
Alternatively, it can weigh components based on their
resilience to noise and system load. At higher loads, the
OS component is the most stable and noise-free, as OS logs
are collected specifically to the process PID. In contrast, the
network and hardware can get noisier with an increase in
the number of processes.
Accordingly, CA() (presented in Algorithm 3) takes as input
the predictions from each component (^R), corresponding
confidences (^C
), prior-knowledge (^E), and the system load
(L) which is the number of processes in the system. At lower
system loads (Line 2), it computes a confident set C Set
using multiple options such as : (1) most-confident selects
the prediction which has high ^c; (2) prior-known selects the
prediction which has high prior-knowledge scores; or, (3)
majority selects the prediction that is common between at
least two components (Line 3). Similar to MA(), CA() resolves
contentions in C Set by choosing the most risky class in
C Set as the final aggregated prediction (Lines 4 and 5). On
the other hand, at higher system loads, the OS component
is the most stable, and hence CA() outputs predictions of the
OS component directly (Line 7).
Figure 11 evaluates the F1-Score obtained for different
alternatives of CA() against the performance of the component
that is optimum for each class. Given any program,
we consider the prediction to be correct if the risk of the
predicted class is the same or higher than that of the
program class. Exploiting prior-knowledge, CA() is able to
detect any malware boosting the performance beyond that
of its best-case specialized predictor by at least 1:42%, and
detect the objective class of the program with a loss as low
as 7:86% (Refer to the row CA() in Table 3).
11
Fig. 11: Detection F1-Scores for different classes using different
confident computing functions in CA(). Employing priorknowledge
gives the best F1-Score for most malware classes.
7 IMPLEMENTATION AND EVALUATION
In this section, we discuss the real-world behavioral data
used to build the ensemble, followed by the implementation
and evaluation of SUNDEW.
7.1 Real-World Behavioral Dataset
For unbiased cross-dimensional analyses, SUNDEW requires
access to a simultaneous capture of network, OS,
and hardware run-time trails of a large corpus of malware
samples of different classes. SUNDEW relies on the RaDaR
dataset [38] that provides such a comprehensive view of the
real-world activity across the system stack of diverse Windows
malware families labeled with their attack objective.
RaDaR is collected by executing live malware samples (2017
ongoing) on a real-world testbed [39] with Internet connectivity,
in a timely manner, when their remote command-andcontrol
servers are highly likely to be active. Each sample is
executed for 2 minutes in an automated manner, which is
known to be sufficient to elicit malicious activities of most
malware samples [40]. For a fair comparison, the benign
samples are executed in an automated manner similar to
malware, as user interactions are easily distinguishable,
unlike the stealthy malware activities.
The dataset [38] provides a comprehensive set of popular
features extracted based on prior works [2], [15], [16], [20],
[28], [41], [42], from 7 million network packets, 11:3 million
OS system call traces, and 3:3 million hardware events
collected for 10; 434 samples. These features include 58
features at network, 11 at OS, and 54 micro-architectural
events at the hardware [38]. Each row in the data represents
a snapshot of network flow2, system call in OS, and
periodic HPC measurement in 100ms intervals in hardware.
Table 4 summarizes the number of snapshots corresponding
to each malware class from the three data-sources. With
data of 10,434 samples evenly spread across 30 well-known
malware families belonging to 8 different classes (attack
objectives) and benign applications, RaDaR [38] provides a
diverse representation of malware classes for evaluations.
Train-validate-test partitions. Finally, we split the dataset in
a 70:15:15 ratio into the train, validate and test sets. Specifically,
we ensure that the train set does not contain samples,
whose data is collected at a later point of time than a sample
in validate/test sets to prevent experimental biases [43]. To
ensure unbiased learning, the train set contains an even
distribution of benign and malware classes.
2. All communications having the same source and destination IP
address, and source and destination port belong to a flow. Thus the
network packets are grouped into traffic flow summaries
TABLE 4: Summary of behavioral snapshots of different malware
classes from the three data-sources in RaDaR [38]. Snapshots
indicate the number of flows in the network, system call
traces in OS, and periodic HPC logs in the hardware.
Cryptominer
Banker
Spyware
Backdoor
Ransomware
PUA
Downloader
Deceptor
Benign
Network 992 4878 11588 7845 2239 7152 9277 4617 8964
OS 293K 772K 1.9M 1.5M 807K 2M 1.7M 440K 1.9M
Hardware158K 51K 59K 371K 182K 914K 502K 478K 578k
Fig. 12: The F1-Score observed with SUNDEW Binary and SUNDEW
Multi, in comparison with the specialized predictors fine-tuned
for each class in each component. SUNDEW Binary infers if a program
is malware/benign, whereas SUNDEW Multi infers the class of the
program. SUNDEW Binary achieves an F1-Score of 1 for most classes
as compared to their best-case specialized predictors in network, OS, or
hardware.
7.2 SUNDEW Implementation
We implement the specialized predictors in Python v3.6.2
using XGBoost3 v1.4.2 library. We train each specialized
predictor with the train-validate set containing data of the
specific malware class and benign programs. Next, we test
every specialized predictor with the train-validate sets of
all other malware classes to generate conflicting predictions.
The resultant predictions and statistics form the trainvalidate
sets for the aggregators. We implement the aggregators
using Python LightGBM library v3.3.14. Finally, we
evaluate the performance of SUNDEW using the test set of
malware samples.
7.3 Evaluation
We compare the performance and resilience of SUNDEW
against the best-case specialized predictors of all malware
classes, as well as the state-of-the-art malware classifiers.
Finally, we evaluate the overheads incurred by SUNDEW.
Specialized predictors. Figure 12 compares the performance
of SUNDEW to detect a malware class against
the corresponding predictor specialized for that class (and
hence the optimum) in different components (data sources).
We consider two configurations: SUNDEW Binary measures
the F1-Score of detecting if a test sample is malware/
benign, whereas SUNDEW Multi measures the F1-
Score of inferring the class of the sample. As evident,
SUNDEW Binary, with its holistic view of malware activity
from the three data sources, an ensemble of specialized
predictors, and aggregation, can achieve performance
similar to the corresponding specialized predictor for any
malware class. The aggregation in SUNDEW Binary boosts
3. https://xgboost.readthedocs.io/en/stable/python/
4. https://lightgbm.readthedocs.io/en/latest/
12
(a) F1-Scores of detection models based on network and
OS under various load conditions for their best detectable
classes. The OS-based models are resilient to infiltrating
noise from increasing system load.
(b) F1-Scores of detection models based on network, OS, or
hardware and that of SUNDEW under varying load conditions
for Cryptominer class. SUNDEW is able to leverage the best of
the three components for detection accuracy while benefiting
from the resilience of the OS component.
Fig. 13: Impact of system load on detection efficacy.
the average detection performance beyond that of the bestcase
specialized predictors in any of the three components,
by 1:14%. SUNDEW Binary has an F1-Score of 1 for most
malware classes and an average score of 0:93 for any malware
class. While gaining performance in high-risk malware,
the performance of low-risk malware slightly drops
due to aggregation. On the other hand, aggregating the
correct objective class of the sample in SUNDEW Multi
incurs an aggregation loss of 7:86%. This is because the
evaluation considers a detection successful only if the ensemble
predicts the actual class or a riskier class for the
test sample. Hence, though PUA is successfully detected,
its measure in SUNDEW Multi drops as the riskier class
Deceptor is chosen when resolving conflicting predictions
during aggregation.
Resilience to noise. We next evaluate SUNDEW under
varying noise infiltration induced by system load. We use
the number of processes in the system to measure noise. To
generate data for the experiment, we run benign applications
from CNET [44] in multiples of 10 in the background
while running the malware programs and collect the corresponding
data at network, OS, and hardware. While these
benign applications represent use-case scenarios, an extensive
characterization covering a wide range of system loads
is planned for future work. Figure 13a plots the F1-Score
of specialized predictors in network and OS under varying
system load conditions. As evident, the performance of the
network component decrease, while the OS component is
agnostic to system load.
We next compare the resilience of SUNDEW using the
case of cryptominer, which is best detected in hardware.
Figure 13b plots the F1-Score of the cryptominer-specialized
predictors based on network, OS, and hardware and SUNDEW
on cryptominer data collected under varying system
load conditions. As evident, the performance of hardwarebased
predictor though higher than OS and network at
TABLE 5: Comparison of SUNDEW with prior state-of-the-art solutions
including single classifiers [2], [20], [42] and single-input ensembles
[23] based on (A) F1-Score, and (B) False-positive rate, of detection
observed on the RaDaR dataset.
Detection Performance
(F1-Score) (B)False-Positive Rate
Detection
model
Single
Classifier
SIE
SUNDEW
Single
classifier
SIE
SUNDEW
Component
)
Class +
N [2]
O [42]
H [20]
N + O [23]
N+O+H
N [2]
O [42]
H [20]
N+O [23]
N+O+H
Cryptominer 0.80 0.87 0.93 0.87 0.82 0.2 0.01 0.14 0.11 0.013
Banker 0.85 0.83 0.76 0.83 1 0.15 0.49 0.23 0.16
Spyware 0.89 0.87 0.81 0.86 1 0.12 0.35 0.13 0.15 0
Backdoor 0.82 0.83 0.79 0.70 1 0.11 0.32 0.13 0.28 0
Ransomware 0.78 0.64 0.75 0.73 1 0.25 0.04 0.27 0.35 0
PUA 0.83 0.72 0.74 0.81 0.99 0.12 0.35 0.28 0.2 0.003
Downloader 0.96 0.88 0.84 0.91 0.89 0.04 0.21 0.11 0.07 0.055
Deceptor 0.87 0.88 0.78 0.86 0.78 0.13 0.51 0.24 0.15 0.051
Mean 0.85 0.82 0.8 0.82 0.935 0.15 0.31 0.19 0.19 0.015
*SIE- Same Input Ensembles, N - Network, O - OS, H - Hardware
lower system loads, decreases significantly as load increases.
In contrast, the OS-based predictor, agnostic to system
load, outperforms both the network and hardware-based
predictors as soon as more than 10 additional user applications
start executing simultaneously. Hence, the OSbased
predictor is the most resilient to noise. In contrast,
SUNDEW leverages the best of three worlds to achieve
accurate and resilient malware detection (claims C-1 and C-
2 in Section 3). At lower system loads, SUNDEW prioritizes
network and hardware components for higher accuracy,
whereas, at higher system loads, it uses the reliable OS
component for prediction.
Comparison with prior art. Table 5 compares SUNDEW
against our implementation of prior state-of-the-art predictors
including single classifiers that rely on a single
data source (network [2], operating system [42], or
hardware [20]); and, same-input ensembles that do not
employ class-wise specialization [23]. We compare these
works based on the detection F1-Score and false-positive
rate observed on the RaDaR dataset (Table 4). The crossdimensional
view of malware activity, specialization, and
insightful aggregation of predictions in SUNDEW improve
the detection F1-Score by at least 10% as compared to
these prior works (Table 5A). Similarly, the class-specific
specialization in SUNDEW decreases the false-positive by
at-least 89% as compared to the prior works (Table 5B).
Finally, with the incorporation of different data sources, we
observe that SUNDEW is as resilient as the state-of-the-art
OS-based works, even under noisy conditions.
Overheads. We next evaluate the overheads of SUNDEW
considering an example deployment in an enterprise network.
To measure the overheads, we first present the design
and workflow of SUNDEW in the deployment in Figure 14.
SUNDEW runs as a service on a middle-box server in the
enterprise network, whereas the host machines run the
client agents that enable the hosts to access the service to
test any program. While the client agents collect the OS
and hardware trails of the program under test, the gateway
in the network collects the network behavior. Figure 14b
illustrates the workflow of SUNDEW when a host accesses
13
Gateway MiddleBox Server Client Agent
Report exe
hash
Report OS and
Collect NetworkHPC data
data
Inform
Decision
5
Extract Features
Run Models
1
2
3
4
MiddleBox
Collects
Network Data
Collects OS
& Hardware
Data
Runs
detection
models
Internet
(a)
Execution of
a new file
Start capture
of OS and
Hardware data
(b)
Mitigate
if
malware
Exe hash
Detection Hisory
Decision
Gateway
Hosts
Client Agents Client Agents Client Agents
Fig. 14: (a) An example deployment in an enterprise network. The
network data is collected at the gateway, whereas the OS and hardware
data are collected at the host machines. The middlebox server runs the
SUNDEW framework. (b) The workflow of SUNDEW. .
its service. At the host machine, whenever a new program
executes, the client agent reports the hash of the program to
the server (Step 1 in Figure 14b). To avoid repeated testing of
the same program, SUNDEW maintains a detection history
of program hashes at the server. Thus, if the program is
not analyzed previously, SUNDEW starts the capture of
respective logs at the host, as well as the gateway for 2
minutes (Step 2). After the duration, the server collects the
OS and hardware data from the host, and the network
data from the gateway (Step 3). The server then extracts
the features from each data source, invokes the respective
components of SUNDEW to predict the class of the program,
and informs the final prediction to the host (Step 4). Finally,
the client agent at the host takes the necessary action based
on the prediction.
We use GeekBench [45] tool and observe that end-hosts
incur an average overhead of 1:5% at the first execution of
a test program. Note that the end-hosts (client-agents) are
responsible only for the collection of OS and hardware data,
whereas the heavy-weight operations of feature extraction
and specialized predictors run on the middle-box server
(refer to Figure 14a). While the middle-box server would
require a dedicated provision of resources, the impact on
users is minimal (1:5%) and is restricted to the execution of
new applications alone.
8 DISCUSSION
In this section, we first discuss the applications of SUNDEW.
Next, we discuss its limitations and present plausible directions
for future work.
Applications. The multi-featured approach and aggregation
in SUNDEW can serve as an analysis framework for anti-virus
companies and defense solutions for securing enterprises. As
an analysis framework, the holistic view and specialization
enable precise characterization of samples, thus reducing
the manual efforts to label thousands of newer samples
reported daily. Alternatively, SUNDEW can serve as defense
solutions in enterprise networks to provide accurate and
resilient detection of malware attacks.
Incremental update of predictors. The SUNDEW ensemble
involves predictors specialized for a set of malware
classes. Further, aggregator functions are customized based
on statistics from these predictors. With malware behavior
evolving, the specialized predictors and aggregators would
require updates. While mechanisms for incremental updates
need to be explored in the future, we propose an autoconfiguration
engine that auto-configures the SUNDEW ensemble
for any update or any deployment setting. Such
an engine takes as inputs the labeled data from the three
data sources and the user requirements per malware class.
It outputs the ensemble, including its specialized predictors
and aggregator functions.
Scalability. With a rampant increase in newly reported malware
classes, the number of specialized predictors is bound
to increase 3x (one for each data source), increasing the
complexity of aggregator functions and overheads. Hence,
specialized predictors for each class can get infeasible. A
viable solution is to club models that share common features
and user requirements in the 3-tuple to reduce the number
of specialized predictors for each data source. We intend
to build an automated framework to configure SUNDEW
with an optimal number of specialized predictors in future
work. Alternatively, Locality Sensitive Hashing (LSH) can
assist in identifying the similarity of test programs to previously
tested program hashes. Accordingly, LSH can assist
in enabling only the relevant specialized predictor or data
components to decrease overheads.
Extensive Characterization of Noise.We analyze the impact
of noise on SUNDEW using well-known benign applications.
However, an extensive characterization of varying
system load conditions and impact on the three data sources
is planned for future work.
9 CONCLUSION
In this paper, we emphasize that malware classes are inherently
different, and catering to the differences can improve
the efficiency and resilience of detection. We propose
SUNDEW, a novel multi-input ensemble of predictors and
aggregator functions that leverages a multi-dimensional
view of malware execution, considering its activities at the
network, OS, and hardware and the system noise to provide
a case-sensitive prediction. Our evaluations of SUNDEW
on a real-world dataset indicate that the multi-dimensional
view and specialization enable SUNDEW to avert infiltrating
noise into the behavioral data while improving the
accuracy, resilience, and false-positive guarantees. To the
best of our knowledge, SUNDEW is the first to provide
a multi-dimensional case-sensitive characterization of malware.
The holistic approach and aggregation strategies open
new avenues for malware research and detection models.
REFERENCES
[1] “AVTest: Malware Stastics.” Accessed: 2021-09-26.
[2] K. Bartos, M. Sofka, and V. Franc, “Optimized invariant representation
of network traffic for detecting unseen malware variants,”
in 25th USENIX Security Symposium, pp. 807–822, 2016.
[3] X. Wang and R. Karri, “NumChecker: detecting kernel controlflow
modifying rootkits by using hardware performance counters,”
in The 50th Annual Design Automation Conference, DAC, 2013.
[4] F. A. Narudin, A. Feizollah, N. B. Anuar, and A. Gani, “Evaluation
of machine learning classifiers for mobile malware detection,” Soft
Computing, vol. 20, no. 1, pp. 343–357, 2016.
[5] M. S. Alam and S. T. Vuong, “Random forest classification for
detecting android malware,” in IEEE international conference on
green computing and communications and IEEE Internet of Things and
IEEE cyber, physical and social computing, pp. 663–669, 2013.
[6] B. Anderson and D. McGrew, “Machine learning for encrypted
malware traffic classification: accounting for noisy labels and nonstationarity,”
in 23rd ACM SIGKDD International Conference on
knowledge discovery and data mining, pp. 1723–1732, 2017.
14
[7] N. Nissim, R. Moskovitch, L. Rokach, and Y. Elovici, “Novel active
learning methods for enhanced pc malware detection in windows
os,” Expert Systems with Applications, vol. 41, pp. 5843–5857, 2014.
[8] J. Demme, M. Maycock, J. Schmitz, A. Tang, A.Waksman, S. Sethumadhavan,
and S. J. Stolfo, “On the feasibility of online malware
detection with performance counters,” in The 40th Annual International
Symposium on Computer Architecture, ISCA, 2013.
[9] H. Peng, J. Wei, and W. Guo, “Micro-architectural features for
malware detection,” in 11th Conference on Advanced Computer Architecture,
ACA (J. Wu and L. Li, eds.), Springer, 2016.
[10] A. Tang, S. Sethumadhavan, and S. J. Stolfo, “Unsupervised
Anomaly-Based Malware Detection Using Hardware Features,” in
17th International Symposium on Research in Attacks, Intrusions and
Defenses, RAID, pp. 109–129, 2014.
[11] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kr¨ ugel, and E. Kirda,
“Scalable, behavior-based malware clustering,” in Network and
Distributed System Security Symposium, NDSS, 2009.
[12] M. R. Watson, A. K. Marnerides, A. Mauthe, D. Hutchison, et al.,
“Malware detection in cloud computing infrastructures,” IEEE
Transactions on Dependable and Secure Computing, vol. 13, 2015.
[13] Y. Zhou, G. Cheng, S. Jiang, and M. Dai, “Building an efficient intrusion
detection system based on feature selection and ensemble
classifier,” Computer networks, vol. 174, p. 107247, 2020.
[14] M. K. Alzaylaee, S. Y. Yerima, and S. Sezer, “Dl-droid: Deep
learning based android malware detection using real devices,”
Computers & Security, vol. 89, p. 101663, 2020.
[15] G. Gu, R. Perdisci, J. Zhang, and W. Lee, “BotMiner: Clustering
Analysis of Network Traffic for Protocol- and Structure-
Independent Botnet Detection,” in 17th USENIX Security Symposium,
pp. 139–154, USENIX Association, 2008.
[16] R. Perdisci, W. Lee, and N. Feamster, “Behavioral Clustering of
HTTP-Based Malware and Signature Generation Using Malicious
Network Traces,” in Proceedings of the 7th USENIX Symposium on
Networked Systems Design and Implementation, NSDI, 2010.
[17] P. Feng, J. Ma, C. Sun, X. Xu, and Y. Ma, “A novel dynamic android
malware detection system with ensemble learning,” IEEE Access,
vol. 6, pp. 30996–31011, 2018.
[18] D. Li, Q. Li, Y. Ye, and S. Xu, “A framework for enhancing deep
neural networks against adversarial malware,” IEEE Transactions
on Network Science and Engineering, vol. 8, no. 1, pp. 736–750, 2021.
[19] S. Das, Y. Liu, W. Zhang, and M. Chandramohan, “Semanticsbased
online malware detection: Towards efficient real-time protection
against malware,” IEEE Trans. Inf. Forensics Secur., 2016.
[20] H. Sayadi, N. Patel, S. M. P. D., A. Sasan, S. Rafatirad,
and H. Homayoun, “Ensemble learning for effective run-time
hardware-based malware detection: a comprehensive analysis and
classification,” in Proceedings of the 55th Annual Design Automation
Conference, DAC, pp. 1:1–1:6, ACM, 2018.
[21] M. Rhode, P. Burnap, and K. Jones, “Early-stage malware prediction
using recurrent neural networks,” computers & security,
vol. 77, pp. 578–594, 2018.
[22] F. Salo, A. B. Nassif, and A. Essex, “Dimensionality reduction with
ig-pca and ensemble classifier for network intrusion detection,”
Computer Networks, vol. 148, pp. 164–175, 2019.
[23] T. Chakraborty, F. Pierazzi, and V. S. Subrahmanian, “EC2: ensemble
clustering and classification for predicting android malware
families,” IEEE Trans. Dependable Secur. Comput., vol. 17, 2020.
[24] S. Wang, Q. Yan, Z. Chen, B. Yang, C. Zhao, and M. Conti,
“Detecting android malware leveraging text semantics of network
flows,” IEEE Trans. Inf. Forensics Secur., vol. 13, no. 5, 2018.
[25] P. M. Comar, L. Liu, S. Saha, P.-N. Tan, and A. Nucci, “Combining
supervised and unsupervised learning for zero-day malware detection,”
in 2013 Proceedings IEEE INFOCOM, pp. 2022–2030, 2013.
[26] M. B. Bahador, M. Abadi, and A. Tajoddin, “HPCMalHunter: Behavioral
malware detection using hardware performance counters
and singular value decomposition,” 4th International Conference on
Computer and Knowledge Engineering (ICCKE), pp. 703–708, 2014.
[27] X. Wang, S. Chai, M. Isnardi, S. Lim, and R. Karri, “Hardware
Performance Counter-Based Malware Identification and Detection
with Adaptive Compressive Sensing,” ACM Trans. Archit. Code
Optim., vol. 13, no. 1, 2016.
[28] D. Canali, A. Lanzi, D. Balzarotti, C. Kruegel, M. Christodorescu,
and E. Kirda, “A quantitative study of accuracy in system callbased
malware detection,” in International Symposium on Software
Testing and Analysis, ISSTA, pp. 122–132, ACM, 2012.
[29] “Types of Malware: Kasperkey Solutions .” Accessed: 2021-09-26.
[30] M. Alam, S. Bhattacharya, S. Dutta, S. Sinha, D. Mukhopadhyay,
and A. Chattopadhyay, “Ratafia: ransomware analysis using time
and frequency informed autoencoders,” in IEEE International Symposium
on Hardware Oriented Security and Trust (HOST), 2019.
[31] Intel Corporation, Intel 64 and IA-32 Architectures Software Developer’s
Manual - Volume 3B, August 2007.
[32] S. Das, J. Werner, M. Antonakakis, M. Polychronakis, and F. Monrose,
“Sok: The challenges, pitfalls, and perils of using hardware
performance counters for security,” in 2019 IEEE Symposium on
Security and Privacy (SP), pp. 20–38, IEEE, 2019.
[33] K. N. Khasawneh, N. Abu-Ghazaleh, D. Ponomarev, and L. Yu,
“RHMD: Evasion-Resilient Hardware Malware Detectorss,” in
2017 50th Annual IEEE/ACM International Symposium on Microarchitecture
(MICRO), pp. 315–327, 2017.
[34] JoeSandbox, “Automated malware analysis.” Accessed: 2021-08.
[35] D. R. Miller, Security information and event management (SIEM)
implementation. McGraw-Hill Higher Education, 2011.
[36] T. Kim, B. Kang, M. Rho, S. Sezer, and E. G. Im, “A multimodal
deep learning method for android malware detection using
various features,” IEEE Transactions on Information Forensics and
Security, vol. 14, no. 3, pp. 773–788, 2018.
[37] J. Yan, Y. Qi, and Q. Rao, “Detecting malware with an ensemble
method based on deep neural network,” Security and Communication
Networks, vol. 2018, 2018.
[38] S. Karapoola, N. Singh, C. Rebeiro, and K. Veezhinathan, “RaDaR:
A real-world dataset for AI powered run-time detection of cyberattacks,”
in The 31st ACM International Conference on Information
and Knowledge Management (CIKM), ACM, 2022.
[39] S. Karapoola, N. Singh, C. Rebeiro, and K. Veezhinathan, “JUGAAD:
Comprehensive malware behavior-as-a-service,” in Cyber
Security Experimentation and Test Workshop, pp. 39–48, 2022.
[40] A. K¨ uchler, A. Mantovani, Y. Han, L. Bilge, and D. Balzarotti,
“Does Every Second Count? Time-based Evolution of Malware
Behavior in Sandboxes,” in 28th Annual Network and Distributed
System Security Symposium, NDSS, The Internet Society, 2021.
[41] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kr¨ ugel, and E. Kirda,
“Scalable, behavior-based malware clustering,” in Proceedings of
the Network and Distributed System Security Symposium, NDSS, 2009.
[42] S. Das, Y. Liu, W. Zhang, and M. Chandramohan, “Semanticsbased
online malware detection: Towards efficient real-time protection
against malware,” IEEE Trans. Inf. Forensics Secur., 2016.
[43] F. Pendlebury, F. Pierazzi, R. Jordaney, J. Kinder, and L. Cavallaro,
“TESSERACT: Eliminating Experimental Bias in Malware
Classification across Space and Time,” in 28th USENIX Security
Symposium, pp. 729–746, 2019.
[44] “CNET: Windows application repository.” Accessed: 2021-09-26.
[45] “Introducing Geekbench-5.” Accessed: 2021-09-26.
Sareena Karapoola is a Ph.D. scholar at the Department of Computer
Science and Engineering, Indian Institute of Technology, Madras. Her
research interests include the cyber-security, malware analysis and
detection, machine learning for security, development of novel attack
mitigation strategies, and testbeds for security research.
Nikhilesh Singh is a Ph.D. student at Department of Computer Science
and Engineering, Indian Institute of Technology, Madras. His research
interests include the deployment of Machine Learning for safety
and system security including malware defenses, micro-architectural
security, operating system security, secure hardware designs.
Chester Rebeiro is an Associate Professor at the Department of Computer
Science and Engineering, Indian Institute of Technology, Madras.
His research interests include hardware security, operating systems
security, and applied cryptography. He also leads the effort at designing
secure RISC-V micro-processors for embedded platforms at IIT Madras.
Kamakoti V. is a Professor and the Director of IIT Madras. He specializes
in the areas of computer architecture, secure systems engineering,
and network security and privacy. He is a coordinator of the Information
Security Education and Awareness program of the Department of Information
Technology, Government of India and the Chairman of the Task
Force on Artificial Intelligence for India’s Economic Transformation. He
has also won several awards such as the IBM Faculty Award (2016).

________________________________________________________________________
JUGAAD: Comprehensive Malware Behavior-as-a-Service
Sareena Karapoola
sareena@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Nikhilesh Singh
nik@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Chester Rebeiro
chester@iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
V. Kamakoti
kama@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
ABSTRACT
An in-depth analysis of the impact of malware across multiple layers
of cyber-connected systems is crucial for confronting evolving
cyber-attacks. Gleaning such insights requires executing malware
samples in analysis frameworks and observing their run-time characteristics.
However, the evasive nature of malware, its dependence
on real-world conditions, Internet connectivity, and short-lived
remote servers to reveal its behavior, and the catastrophic consequences
of its execution, pose significant challenges in collecting
its real-world run-time behavior in analysis environments.
In this context, we propose JUGAAD, a malware behavior-as-aservice
to meet the demands for the safe execution of malware. Such
a service enables the users to submit malware hashes or programs
and retrieve their precise and comprehensive real-world run-time
characteristics. Unlike prior services that analyze malware and
present verdicts on maliciousness and analysis reports, JUGAAD
provides raw run-time characteristics to foster unbounded research
while alleviating the unpredictable risks involved in executing them.
JUGAAD facilitates such a service with a back-end that executes a
regular supply of malware samples on a real-world testbed to feed
a growing data-corpus that is used to serve the users. With heterogeneous
compute and Internet connectivity, the testbed ensures
real-world conditions for malware to operate while containing its
ramifications. The simultaneous capture of multiple execution artifacts
across the system stack, including network, operating system,
and hardware, presents a comprehensive view of malware activity
to foster multi-dimensional research. Finally, the automated
mechanisms in JUGAAD ensure that the data-corpus is continually
growing and is up to date with the changing malware landscape.
CCS CONCEPTS
• Security and privacy→Malware and its mitigation.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CSET 2022, August 8, 2022, Virtual, CA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9684-4/22/08. . . $15.00
https://doi.org/10.1145/3546096.3546108
KEYWORDS
Dynamic Analysis, Malware, Run-time Behavior, Real-world,
Testbeds
ACM Reference Format:
Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, and V. Kamakoti. 2022.
JUGAAD: Comprehensive Malware Behavior-as-a-Service. In Cyber Security
Experimentation and Test Workshop (CSET 2022), August 8, 2022, Virtual, CA,
USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3546096.
3546108
1 INTRODUCTION
Malware attacks are increasing at an alarming scale. The ramifications
of these attacks vary widely, ranging from data breaches to
business disruptions, reputation damage, financial loss, and even
sabotage of critical infrastructures. With millions of malware variants
reported every year, malware is continually evolving in potential
and sophistication, posing significant challenges to security
researchers. Confrontation of such an ever-evolving threat landscape
requires an in-depth understanding of malware behavior
in the wild, including their objectives, functionalities, and consequences.
In fact, behavioral analysis of malware has recently gained
traction in the arms race against malware due to its potential to
detect zero-day malware.
Researchers glean insights into malware behavior from the runtime
trails observed by executing malware samples on analysis
frameworks. This demands access to a large corpus of recently reported
live malware samples. The aspect of being live is important in
the malware context, as its execution heavily depends on its communications
to live remote servers called command-and-control (C&C)
servers. These servers are short-lived and are typically pulled down
in a few months after the malware is first reported, warranting a
timely execution of the sample to elicit its real-world behavior.
Currently, malware research adopts two approaches, supported
mainly by private enterprises, to address the demand for live samples.
Given a hash of a sample to be tested, the first approach
provides the outcome of analysis done by the Anti-virus (AV) engines
housed by these enterprises [55]. The outcome includes the
inference of the maliciousness of the sample, signatures, and reports
from the analyses. However, these signatures and reports are
limited by the capabilities of the available AV engines, whereas
fostering unbounded research requires access to the raw behavioral
data of these samples. The second approach supplies live malware
samples to researchers for execution and subsequent analyses [39].
Such a model has multiple limitations. First, the distribution of live
39
CSET 2022, August 8, 2022, Virtual, CA, USA Sareena, et al.
Download
Collect
Real-World
Testbed
User Submitted Files
FRONT
Data END
Corpus
Execute
Supply of
Samples
BACK-END
Get <Hash/File>
JUGAAD Behavior-as-a-Service
Comprehensive
Run-time Data
For e.g. network,
OS, hardware tails
Figure 1: JUGAAD Framework for Malware Behavior-as-a-Service
samples is highly vulnerable to accidental execution and can be
catastrophic if stringent access policies and processes do not govern
the handling of these samples. Any leakage of samples can lead
to potential misuse, warranting policies for ensuring accountability.
Second, due to the potential risk and liabilities involved, such
services are largely restricted and monopolized by a few private enterprises,
incurring a high cost for a regular supply of new samples.
Third, executing malware and eliciting their real-world behavior
in a laboratory setting is challenging. Researchers typically prioritize
safety and rely on non-connected virtualized frameworks for
analyzing malware. However, modern malware can easily identify
artifacts of such test environments and choose not to execute, thus
evading analysis [40]. Consequently, the data collected does not
represent malware behavior in the wild. In fact, a precise collection
of close-to-real-world behavior requires timely execution of
malware in unrestricted real-world conditions connected to the
Internet, when its C&C servers are likely to be still active.
We propose an alternate model of malware behavior-as-a-service
to meet the demands for the safe execution of samples. Such a model
enables the users to submit hashes of malware samples for analysis
and retrieve their precise and comprehensive run-time trails. We
argue that such a comprehensive view of raw run-time data can
replace the distribution of live malware samples, as behavioral
research primarily relies on passively observable run-time trails
rather than the malware executable contents.
To this end, we present JUGAAD, a framework to facilitate a
comprehensive behavior-as-a-service for malware research. The
framework shown in Figure 1 consists of a front-end that responds
to user requests, a growing data corpus of precise and comprehensive
malware behavior, and a back-end that continually feeds the
data-corpus. The front-end provides API for users to submit program
hashes or files and in return, outputs the corresponding data
retrieved from the corpus. In cases where the data corresponding
to the requested hash/file is not present in the corpus, the front-end
submits the request to the back-end for processing. The back-end executes
a supply of live malware samples on a real-world testbed and
updates the collected behavior to the data-corpus. The testbed with
Internet connectivity ensures real-world conditions for malware
to operate while containing their malicious ramifications. Thus,
JUGAAD alleviates the risks of handling and executing malware to
the research community by facilitating the out-sourcing of the precise
collection of malware behavior. The sustenance of the growing
data corpus relies upon a continual supply of malware samples that
is augmented by a regular feed from online repositories and files
uploaded by users who use the JUGAAD service.
Following are the major contributions of this paper:
(1) A first-of-its-kind behavior-as-a-service model to provide
precise and comprehensive real-world malware behavior for
research, instead of the distribution of the risky malware
samples. Unlike prior malware behavioral datasets [17, 18,
43, 52, 56], the growing corpus of malware behavior keeps
JUGAAD up to date with the changing malware landscape.
(2) A real-world testbed ensuring close-to-real-world heterogeneous
compute and Internet connectivity, with sufficient
triggers for malware execution, demonstrated with 515 offthe-
shelf devices.
(3) A framework with mechanisms for a comprehensive view
of run-time malware activity observable across network, OS,
and hardware. Unlike prior malware testbeds that support
the collection of network and OS trails alone [5], JUGAAD
provides simultaneous capture of hardware behavior along
with these run-time trails.
(4) A framework with mechanisms for timely and large-scale
execution of malware samples, tested up to 255 samples per
day per network (58.6% faster than prior malware testbeds).
Following is the organization of rest of the paper. Section 2 provides
the necessary background for the paper. Section 3 presents the
related work. Section 4 discusses how a comprehensive behavioras-
a-service model can replace the need for distribution of malware
samples. Section 5 describes the framework. Section 6 presents
the implementation details. Section 8 discusses the limitations and
future work in JUGAAD. Finally, section 9 concludes the paper.
2 BACKGROUND
Malware detection takes two broad directions based on the data
they employ for analysis. Static analysis examines the contents of
malware executable binaries to extract signatures and imply its maliciousness.
However, such static signatures can be easily thwarted
by techniques such as packing and obfuscation that change the
malware binary without affecting its functionality. An alternate
approach to malware detection is dynamic analysis, wherein maliciousness
is inferred using the run-time behavior of malware. As
the detection relies on observable behavior, dynamic analysis is
immune to techniques that typically evade static analysis.
Behavioral Analysis. Dynamic analysis adopts two approaches to
analyze malware. Active techniques [32, 42] repeatedly instrument
the malware binary before execution to explore all execution paths
in the malware, whereas passive techniques [4, 10, 57] merely execute
malware and observe the behavioral trails. While such passive
behavioral analysis can analyze the executed path alone, they are
immune to evasive malware that can easily detect the instrumentation
done by active techniques and choose not to execute [36].
Artificial intelligence (AI) driven run-time behavioral analysis
has recently gained traction due to its upper edge in defense against
evolving malware. Such techniques model good behavior and attempt
to detect anomalies, thus facilitating zero-day malware detection.
Primary to fostering such research is the availability of
40
JUGAAD: Comprehensive Malware Behavior-as-a-Service CSET 2022, August 8, 2022, Virtual, CA, USA
ground-truth of malware behavior in the wild. However, collecting
a precise representation of real-world malware behavior in a
laboratory setting is challenging.
Conditions for Executing Malware. Malware execution can be
catastrophic. Consequently, researchers typically rely on virtualized
non-connected analysis environments to execute malware. However,
modern malware is also evasive. They look for real-world
conditions before they reveal their offensive behavior. Hence, they
can quickly identify artifacts of analysis environments to detect
them and remain dormant. Further, malware communications to
its C&C servers are vital for its execution, calling for an active
Internet connection while executing malware. Thus, eliciting realworld
behavior requires executing malware in real-world connected
environments while ensuring the containment of its ramifications.
Challenges in Large-Scale Analysis. Malware also poses challenges
to large-scale evaluations. Specifically, malware execution
impacts the system state in the analysis frameworks. Hence, each
sample should be evaluated in a clean initial state of the frameworks.
Resetting the analysis frameworks to their initial state makes largescale
analysis of thousands of samples time-consuming. Further,
malware execution can cause frequent system crashes and halts
beyond recovery using remote commands, requiring hard power
restarts that significantly affect the large-scale automated analysis.
3 RELATEDWORK
The demand for malware behavioral data in research is addressed
using three approaches, namely, download of old malware samples,
supply of live malware samples, or behavioral datasets, as discussed
next.
Downloading older malware samples. Many online repositories
allow researchers to download a corpus of malware samples
that are a few years old [9]. However, with their C&C servers not
available, most malware samples quit execution prematurely without
exhibiting their real-world behavior.
Supply of live malware samples. Private enterprises such as
VirusTotal provide premium services to download live malware
samples [33, 39]. While such services are highly-priced at 82K$ for
12K malware samples a year, the main challenge lies in the safe
handling and execution of these live malware samples.
Datasets of malware behavior. Multiple prior datasets present
malware behavior for research [4, 15–18, 23, 35, 41, 43, 53, 56]. However,
most of these works rely on virtualized analysis environment,
thus lacking precise real-world behavior [16–18, 23, 35, 43, 52, 56].
On the other hand, datasets generated in real-world conditions
are not open to the community [4, 15, 41, 53]. Additionally, these
datasets are static with no mechanisms to keep pace with the evolving
malware landscape.
In contrast, JUGAAD facilitates behavioral data-as-a-service to
researchers. While it offloads the safe execution of live samples,
it facilitates a dynamic and growing data-corpus that is regularly
augmented with lately reported live samples.
Figure 2: Behavioral data employed by prior research in the
past decade
4 COMPREHENSIVE BEHAVIORAL DATA AS
AN ALTERNATIVE TO MALWARE SAMPLES
JUGAAD advocates the distribution of malware behavioral data
instead of live malware samples. The natural question is whether
data can replace the need for distributing live samples in malware
behavioral research. To answer this question, we first investigate
the vast body of literature on dynamic malware detection to identify
run-time characteristics that were used. Further, we show that any
two programs can be distinguished by the run-time behavior.
Usage of samples in prior works. Figure 2 provides a summary
of the run-time characteristics employed by 400 most cited prior research
in dynamic malware detection since 2010. Theseworks either
rely on available datasets or generate data by executing malware
for their research. The run-time trails of malware are observable at
network (for instance, [1, 2, 4, 7, 8, 12, 18, 25–27, 31, 38, 43]), operating
system (OS) (for instance, [6, 7, 10, 15, 17, 24, 26, 31, 34, 44, 56]),
or, hardware (for instance, [3, 11, 19–21, 37, 41, 45, 49, 58, 59]). The
network trails capture malware communications, including that to
its command-and-control (C&C) servers. On the other hand, OS
trails present the system calls (for e.g. file or registry operations)
made by the malware. In contrast, hardware trails include the microarchitectural
events (e.g., number of cache misses) triggered during
malware execution. These hardware events are measurable using
hardware performance counters (HPC) available in modern processors
[41]. More recently, researchers have explored the potential
of memory snapshots to detect malware using a technique called
volatile memory acquisition (for instance, [13, 47, 50, 51]).
Change in the program leads to change in behavioral trails.
We argue that any change in a program leads to a change in run-time
characteristics that can be visible in the artifacts captured during
malware execution. These differences are evident in a comprehensive
view of malware behavior. To verify the same, we consider
a corpus of 10,000 programs containing an even distribution of
benign, ransomware, downloader, cryptominer, deceptor, potentially
unwanted applications, spyware, and backdoor programs.
Figure 3 plots the distribution of pair-wise dissimilarity between
the behavioral features observed in three example artifacts captured
across the system stack, namely, network, OS, and hardware. The
dissimilarity between two programs p1 and p2 is defined as:
dissimilarity(p1, p2) = 1 − cosine_similarity(p1, p2) , (1)
41
CSET 2022, August 8, 2022, Virtual, CA, USA Sareena, et al.
Figure 3: The dissimilarity between samples across the different
run-time data trails. The rectangular boxes highlight the range of
50% of values in the distribution (i.e., the values between the first
(Q1) and third (Q3) quartile). The horizontal line through the box
indicates the median. The whiskers from each box represent the
minimum and maximum in the distribution. The values between
the minimum and Q1 (lower edge of the box) represent 25% of the
values. Similarly, the values between the maximum and Q3 (upper
edge of the box) indicate the remaining 25% of the distribution.
which measures the difference between the behavioral feature vectors
of the two programs1. A dissimilarity of 0 indicates that the two
programs have identical run-time trails, whereas 1 suggests that
the two are distinct. Thus, most programs (>= 75%) differ in their
network and OS behavior by a dissimilarity measure of 0.3−0.7 and
0.15 − 0.7, respectively. On the other hand, the hardware trails are
comparatively more distinguishable with a dissimilarity of 0.3−0.9
for most programs. While different artifacts can individually distinguish
programs in varying capacities, a comprehensive view of
these artifacts can capture most differences between the programs.
In the current implementation, JUGAAD provides simultaneous
capture of three artifacts, including network, OS, and hardware
behavior, that are predominantly used (Figure 2) for their effectiveness
and decreased overheads in dynamic malware detection. For
comprehensiveness, we intend to include memory snapshots and
other trails such as instruction and power traces in JUGAAD, which
we leave for future work.
5 JUGAAD FRAMEWORK
In this section, we discuss the JUGAAD framework, including its
front-end and back-end, as shown in Figure 1. The front-end handles
the user requests, whereas the back-end is responsible for the
precise and comprehensive collection of malware behavior.
5.1 Front-end
The front-end presents the following Application Programming
Interface (API) calls to the users to access the JUGAAD behavioras-
a-service as described in Figure 4.
Get data for hash. The API GetDataForHash (hash h), allows the
users to submit the hash h of a program and request for its behavioral
data. In response, the front-end extracts the corresponding
behavioral data from the data-corpus and returns it to the user as
1Cosine similarity measures the similarity between two vectors, and is measured by
the cosine of the angle between two vectors.
Yes Is data of hash
h present
in corpus?
Check if data of h is present in
corpus
GetDataForHash (hash h)
Return Data
GetDataForProgram (program p, platform f, <time t> )
Invoke back-end
Data = Execute_Collect (p, f, t)
/*Back-end executes p and returns the Data */
Return Data;
No
Return ERROR
(a) (b)
Data = Extract
data of h from
corpus
Figure 4: The APIs presented by JUGAAD front-end to the users.
The API GetDataForFolder is similar to GetDataForProgram, where
the former takes a folder of programs as input.
shown in Figure 4a. In cases where the data corresponding to the
hash is not present in the data-corpus, the front-end returns an
error.
Get data for a program. The API GetDataForProgram (program
p, platform f, ⟨ time t ⟩), allows the users to submit a program
executable and request for the corresponding behavioral data. The
input includes the program executable p, the platform f (e.g., Linux,
Windows, Android) on which the program needs to be executed,
and optionally, the time duration t for which the program execution
should be observed while collecting the behavioral trails. In
response, the front-end raises a request to the back-end, which
executes the program for a time duration t and collects its behavioral
trails. The collected trails are saved to the data-corpus and
returned to the user. By default, the time duration t is configured
as 2 minutes at the back-end, which is considered to be sufficient
to elicit most of the malicious behaviors of malware [22].
Get data for a folder of samples. Alternatively, users may want
to upload multiple files at once for the collection of behavioral trails.
To this end, the API
GetDataForFolder (program_folder F , platform f, ⟨ time
t⟩), allows users to submit a folder of programs, along with specifying
the platform and time for executing each sample in the folder.
The front-end invokes the back-end to execute and collect the behavior
of the samples and return the data to the user.
5.2 The Back-end
The primary functionality of the back-end is to supply precise closeto-
real-world and comprehensive malware behavior to the datacorpus,
which is used to serve the users. JUGAAD ensures precise
behavioral data by facilitating: (1) timely execution of malware
when their short-lived remote command and control (C&C) servers
are highly likely to be active, and, (2) connected, yet contained realworld
environment for the malware to execute. On the other hand,
it facilitates a comprehensive view of malware activity with simultaneous
capture of run-time trails across the system stack. Figure 5
illustrates the back-end of JUGAAD. The update and test engines
together ensure a regular and timely update of the data-corpus to
service the user requests. On the other hand, the real-world testbed
42
JUGAAD: Comprehensive Malware Behavior-as-a-Service CSET 2022, August 8, 2022, Virtual, CA, USA
Update
Engine
New
Samples
Check for
new samples
Test
Engine
Execute
Collect
Dataset
Corpus
Supply of
Samples
Download
Real-World Testbed
Save Data
Remote
Power Control
Figure 5: Back-end of JUGAAD.
provides connected real-world conditions for malware to operate
while containing its malicious repercussions, and mechanisms to
observe its comprehensive behavior.
5.2.1 Timely Analysis of Malware. Algorithm 1 describes the
working of the update and test engines. The update-engine periodically
crawls public malware repositories for newly reported
malware and downloads them to its local supply of samples (Step
3-8). Further, the test-engine executes these samples immediately
after the download on the real-world testbed. The timely execution
ensures that the samples are executed when their C&C servers are
still available. During the execution, the test engine collects multiple
run-time artifacts (e.g., network communications, OS system calls,
and micro-architectural events) across the system stack, which are
later added to the data-corpus (Execute_and_Collect in Step 9).
The default time for analyzing each sample is set as 2 minutes based
on prior research on the minimum time window required to observe
most of the malicious behaviors of malware [22]. Additionally, the
test-engine services the user-submitted requests for data collection
(Steps 10-15). It executes the programs submitted by the users and
collects the behavioral trails, which are stored in the data-corpus
and returned to the users (Step 15). Thus, the Algorithm 1 ensures
the timely execution of a regular feed of new malware samples
reported in the wild to maintain a growing data-corpus.
5.2.2 Real-world Environment. Modern malware are known
to look for real-world conditions such as a network of diverse physical
machines and Internet connectivity before they reveal their
malicious behavior (refer Section 2). To this end, the real-world
testbed provides a heterogeneous network of machines that can
be employed as a profiler to execute malware. The testbed consists
of desktop computers and single-board embedded platforms with
varying operating systems (e.g., Linux, Windows). The diversity
in machines and software environments not only provides sufficient
triggers for malware to execute, but also enables JUGAAD
to execute malware of diverse platforms. The testbed with autoconfiguration
capability is open, wherein new machines with specific
environments can be added to the testbed seamlessly. Though
these machines are connected in a bus topology, the testbed also
can facilitate user-specific topologies for advanced analysis, such
as the study of malware propagation.
Internet Connectivity and Containment. To provide connectivity
while containing the ramifications, the back-end uses a dedicated
Internet connection (ERNET [14]) that is isolated from the university
network. Further, it connects to the Internet via a two-level
firewall, as highlighted in Figure 5, which ensures containment of
Algorithm 1: JUGAAD Back-end
1 begin
2 while true do
/* Update Engine */
3 Crawl online repositories for newly reported samples
4 if updates are available then
5 NewhashList←Hashes of newly reported malware
samples
6 for h ∈ NewHashList do
7 p ← Download hash h
8 Supply-of-Samples←p
/* Test Engine */
9 Data-Corpus←Execute_Collect (p)
10 Check for requests from front-end
11 if requests queued from front-end then
12 ListOfPrograms←List of programs submitted by
user
13 for p ∈ ListOfPrograms do
14 Supply-of-Samples←p
// Test Engine
15 Data-Corpus←Execute_Collect (p)
the malicious impact of executing malware, while allowing the malware
to operate. The firewalls permit incoming communications
to allow the malware to communicate with its C&C servers, while
extensively scrutinizing outgoing communications to prevent malicious
behavior from permeating outside the testbed. Implementing
the two levels with different firewall models has advantages. The
malware would need to compromise two separate firewalls to infect
machines outside the testbed, which are less likely to be susceptible
to the same malware. Likewise, external attackers would need to
compromise two firewalls to attack the testbed.
While the firewalls allow initial handshakes of connections,
it limits the rate or drops packets when the following scenarios/
triggers from the testbed cross their respective thresholds: (i)
DoS attempt: a high rate of outgoing packets from any machine; (ii)
TCP Scan: a significant number of half-open TCP connections over
time; (iii) SPAM: the number of email messages from the testbed;
(iv) UDP Scan: the ratio of UDP packets from the malware to the unsuccessful
responses (e.g., Internet Control Message Protocol port
unreachable) received. While there is a possibility of some attacks
like DoS to persist when network traffic patterns do not match the
firewall rules, the risk is not unacceptably high. This is because the
running time of every sample is restricted to a threshold, typically
2 minutes based on prior research [22]. After the execution, all
machines in the testbed are reset to their clean initial state, which
reduces the risk of spam, DoS, or unpredictable behavior to a smalltime
duration defined by the threshold. Finally, these rules are not
exhaustive and would require continual monitoring and updates
based on the malware classes that are being analyzed.
Stateless Evaluations. Each malware sample should be evaluated
in a clean initial state or baseline of the testbed (refer Section 2).
Unlike virtual machines, which can be easily reset to their baselines,
43
CSET 2022, August 8, 2022, Virtual, CA, USA Sareena, et al.
Image
Server
Load Image Clean
Baseline state
Post-experiment
State
Baseline-Reset
Initialize
Start
Experiment
Run
complete
Success?
Yes
Image-Reset
No
Using remote
commands or hard
power restart
Figure 6: Two-level reset mechanism in JUGAAD.
1. Reset to clean Baseline state
2. Push program p to platform f
3. Start the data collection
4. Run Malware executable
6. Pull data to logs, save to repository
Keep collecting data,
if available, for t minutes
5. Stop the malware program
Data
Corpus
Data
E.g. OS, Hardware Data
Data
E.g. network
communications
Test Engine
ExecuteProfiler
Collect
(p, f, t)
Internal
tools
External
tools
3. Start the data collection
Figure 7: Data Collection in the testbed (Execute_and_Collect)
resetting all physical machines in the testbed for every sample execution
is not trivial. To this end, JUGAAD employs a two-level reset
mechanism as shown in Figure 6. The first level involves a quick
low-overhead baseline-reset using software methods to restore a
physical machine to its saved clean baselines on restart. JUGAAD
initiates a baseline-reset with a restart of all machines using remote
commands. In cases where the malware makes the system
inaccessible remotely, JUGAAD uses the smart power switches to
hard-restart the machines. Further, critical faults may arise, corrupting
the baselines such that the baseline restore fails. Only in
such scenarios JUGAAD performs an image-reset, which involves
reloading of respective OS images from an image server.
5.2.3 Comprehensive collection of malware behavior. Figure
7 illustrates the working of the test-engine (Execute_and_
Collect) to collect the comprehensive behavior of malware. First,
it begins by resetting all the testbed machines to their clean baselines.
Second, it chooses an appropriate machine in the testbed as a
profiler to execute malware, and pushes the malware sample to it.
For instance, it chooses a Windows machine to evaluate a Win32
malware. Third, it initiates the data collection. It starts the corresponding
tools to capture artifacts inside the profiler. For instance,
to capture OS system call trails, it starts the process monitoring
tool [29] at the profiler. However, it is beneficial to observe some artifacts
from outside the profiler. For instance, observing the network
communications at the gateway that connects the testbed to the
Internet, can capture not only the communications from the profiler
but also malware interactions and their impact on other testbed
machines. Accordingly, the test-engine starts such external tools at
corresponding vantage points. Fourth, the test-engine executes the
sample for a configured duration or provided by the user. Fifth, it
stops the execution and the data collection tools. Finally, it saves
Profiler
Configuration
Engine
Test
Engine
. . .
Control-Plane
Profiler Profiler
Real-world testbed
Remote
Power Control
Internet
Dataset
Corpus
Supply of
Samples
Gateway
Update Engine
Figure 8: The implementation of back-end in JUGAAD
all the collected trails to the data-corpus along with the IP address
of the profiler, and the process ID of the malware executable at the
profiler.
6 IMPLEMENTATION
In this section, we discuss the implementation details of the JUGAAD
framework.
We implement the front-end as an HTTPS web server in python
to provide a public web interface to submit the program hash or
files. The server also provides an HTTP-based public API to enable
users to script submissions in any programming language.
Inspired by software-defined networking, we divide the back-end
into planes, namely gateway, control-plane, and testbed, to support
customization and reconfiguration, as shown in Figure 8. The
gateway connects the back-end to the Internet via a dedicated IP
address in ERNET [14]. It also houses the update engine that feeds
the supply of samples. The control-plane contains the configuration
and test engines to initialize, operate and automate the back-end.
We implement these modules using bash scripts and Python. While
the configuration engine initializes the testbed (for e.g. software
environment), the test engine automates the execution and data
collection on the testbed. Table 1 lists the gateway, control-plane
and testbed machines that are connected as in Figure 8 to realize
the JUGAAD back-end. We next explain the implementation of
timely analysis and the comprehensive data collection discussed in
Section 5.2.
6.1 Timely Analysis
To facilitate access to newly reported malware, JUGAAD has subscriptions
to premium services from online malware repositories
such as Virustotal [54] that provide a daily feed of requested
samples. We implement the back-end algorithm (Algorithm 1) in
Python, which accesses the APIs provided by Virustotal to crawl for
the availability of samples and download them. These samples are
immediately executed by the test engine to ensure timely analysis.
Test engine (Execute-Collect). We implement the test engine
in Python. For Windows-based profiler machines, the test engine
uses PowerShell Remoting and psexec tool to remotely trigger
the malware execution [28]. However, when remote execution is automated,
the malware executes in the background without popping
44
JUGAAD: Comprehensive Malware Behavior-as-a-Service CSET 2022, August 8, 2022, Virtual, CA, USA
Table 1: Devices in the open testbed
Device Description OS Count Role
Desktop Intel i7, 8 cores, 64GB RAM,
2 1G Ethernet ports
Ubuntu
v17.10 1 Gateway
Desktop Intel i7, 8 cores, 64GB RAM,
2 1G Ethernet ports
Ubuntu
v18.04 1 Control
Plane
Desktop Intel i5, 8 cores, 16GB RAM
, 1G Ethernet port Win 10 Pro 2 Testbed
Desktop Intel i7, 8 cores, 64GB RAM
, 2 1G Ethernet ports
Ubuntu
v16.10 1 Testbed
Desktop Intel x86 Atom, 1 Core, 2GB
RAM, 1G Ethernet port
Win 7
Win 10 10 Testbed
Raspberry
Pi 3 B+ 2
ARM Quad core, 1GB RAM,
1G Ethernet
Linux
v3.14 2 Testbed
Galileo
Gen 2
Intel Quark SoC 256MB RAM,
1G Ethernet
Linux
v3.14 500 Testbed
Figure 9: The testbed in JUGAAD
its GUI window. To get the malware executed in the foreground, we
configure the psexec tool to run the malware in the system account.
For Linux-based machines, the test engine uses SSH service to push
and execute a malware sample. The test engine also monitors the
health of machines with a dummy SSH connect request. In case of
a failure, it notifies the administrators with an email and resets the
testbed using the smart power switch.
6.2 Real-world testbed
Figure 9 shows the real-world testbed we built in our lab in the
back-end. We connect all machines in the testbed (refer Table 1) in
a hierarchical bus topology using 28-port D-Link DGS-1210-24 and
24-port HPE-1920s switches. The testbed is open and scalable, as any
new hardware with an Ethernet port can be connected to the switch
to include in the network. It is also powered using WiFi-enabled
smart switches to enable remote hard-restart of the testbed in cases
of failures. Each machine in the testbed is assigned its management
IP address at power-on by the Dynamic Host Control Protocol
(DHCP) server at the control-plane. The control-plane configures
and manages the testbed using the management IP addresses of the
devices.
In the current implementation, all machines in the testbed are
under one network. It is important to note that simultaneous execution
of more than one malware sample would affect the precision
of data as each sample can affect the system state in the entire
network. The number of samples analyzed per day can be increased
if we can divide the testbed into isolated virtual local area networks
(VLAN). With the infrastructure of smart network switches (HPE-
1920s switches), we intend to implement VLANs in future to enable
parallel analysis of multiple samples. Further, we intend to enable
custom topologies and forwarding behavior in the testbed in the
future to facilitate long-term analysis such as malware propagation.
Heterogeneity. As evident from Table 1, the testbed has a heterogeneous
mix of hardware (Raspberry Pi, Intel x86 Atom, Quark,
i5, and i7 machines) and operating systems (different versions of
Linux and Windows) to realize close-to-real-world conditions.
Isolation and Containment. JUGAAD uses a combination of
Snort Intrusion Detection System [46] and Zeek network analysis
framework [60] at the control-plane and gateway. While Snort
can detect known exploits, Zeek can detect protocol violations and
malformed headers using the built-in and custom scripts.
Stateless Evaluations. For baseline-reset, JUGAAD uses software
like Reboot Rx on Windows and SystemBack on Linux [48]. The
test-engine initiates a baseline-reset with an SSH command or a
power-restart using the smart power switches in cases of SSH failures.
During the operation of JUGAAD, we observed the need for
such power-restart in several instances. In either case, the baselinereset
of all machines finishes in ≈ 2.64 minutes. For image-reset,
JUGAAD loads the affected device with the OS image (created using
Clonezilla disk imaging software) from a locally maintained
image-server. However, image-reset is very rare, as we did not
encounter any need for it during the analysis of more than 10K
samples.
6.3 Comprehensive collection
In the current implementation, JUGAAD provides simultaneous
capture of three artifacts, including network, OS, and hardware
behavior. The control-plane captures network communications using
tshark [30]. While external communications pass through the
control-plane, inter-device communications in the testbed are mirrored
to the control-plane using port-mirroring feature of the managed
switches. At the profiler machine, we use process monitor
for Windows, and strace for Linux to capture OS events of the
malware process [29].
For HPCs, we use different interfaces based on the environment.
While Linux provides perftool APIs to configure and fetch the
counters from the userspace, the Windows OS requires some modifications.
We design a custom Windows 10 Driver to read HPCs,
configurable as per the underlying hardware and available events.
Based on empirical observations, we configure the frequency of
event logging to 10ms. It is critical to note that while the hardware
may support hundreds of micro-architectural events, the number
of HPC registers available physically is limited to 4-6 on most architectures.
While time-multiplexing these events across the registers
is a work-around, it can induce significant noise in the data. We
45
CSET 2022, August 8, 2022, Virtual, CA, USA Sareena, et al.
Figure 10: Evasion techniques found in 1000 randomly selected
samples. Malware samples check for specific artifacts to identify
virtualized analysis environments.
address this challenge with multiple binary executions, where limited
events are counted during each execution. Finally, to prevent
the corruption of collected OS and hardware data by malware like
ransomware, they are temporarily stored in system folders (such
as C :\Windows\Temp\) before being moved to the data corpus.
7 EVALUATION
In this section, we evaluate the precision of data collection in JUGAAD,
the time taken for analysis, and the storage requirements
of the data-corpus.
Precise collection of behavior. Modern malware are evasive and
adopt diverse techniques to remain dormant in virtualized analysis
environments [40].We determine if JUGAAD is able to execute and
observe the behavior of such malware.We take a random set of 1000
malware samples. We verify their system call traces to determine
if these samples query for any specific information that aids in
identifying analysis environments, such as differentiating virtual
and physical machines. Figure 10 plots the count of samples using a
subset of 12 techniques typically used for evasion [40]. We observe
that at least 17% of samples had the signature system call to check
if the hard-disk drive size and free space are small. More than 50%
of samples verified the network MAC address, adapter name, and
provider before continuing execution. The real-world conditions
and the Internet connectivity enabled JUGAAD to ensure malware
continues execution beyond these checks in their code.
Analysis Time. Table 2 computes the time taken by the back-end
to analyze a given program sample. Analysis of each sample takes
≈ 338 seconds which includes steps 1-7 in Figure 7. The test-engine
takes ≈ 158 seconds to reset the state of all testbed machines to
their clean baselines in step 1. Hence, JUGAAD can analyse ≈ 255
malware samples per day (refer Table 2). The table also compares
the time for state reset in JUGAAD with techniques used in public
testbeds like DETER[12]. The two-level reset feature in JUGAAD
(refer Section 5.2.2, Figure 6) enables 58.6% times faster reloads
compared to DETER. The shorter time taken for state resets enables
more number of sample analysis (255 per day per network) in
JUGAAD as compared to DETER (154 per day per network).
We intend to increase the number of samples analyzed per day in
JUGAAD by dividing the testbed into isolated VLANs. The current
infrastructure of smart network switches (HPE-1920s switches)
can easily enable such configurations to enable parallel analysis of
multiple samples.
Figure 11: Growing data-corpus over years
Table 2: Time taken by JUGAAD back-end per sample (refer Figure
7)
Testbed Baseline-reset
(Step 1)
Experiment
(Steps 2-7)
Time/
sample
Samples/
/day
JUGAAD 2 m 38 s 3 m 5 m 38 s 255
DETER 6 m 23 s 3 m 9 m 23 s 154
Growing data-corpus. To date, the data-corpus has 2.7 TB of data
and 22M behavioral snapshots of 10,432 malware samples, including
7M network packets, 11.3M operating system call traces, and 3.3M
micro-architectural events from hardware for 8 classes of malware.
Table 3 shows the distribution of malware samples collected in the
growing dataset. Figure 11 plots the growing storage requirements
of the data-corpus in JUGAAD.
Table 3: Distribution of malware classes in the data-corpus
Class % Class % Class %
Backdoor 13.5% Spyware 16% Ransomware 7.5%
Banker 14% Benign 7% Downloader 19.4%
PUA 10.8% Deceptor 10.8% Cryptominer 4.9%
8 DISCUSSION
We present a discussion on the comprehensiveness of the data
collection and the sustenance of JUGAAD behavior-as-a-service
model.
Comprehensive malware behavior. Malware behavior manifests
through diverse artifacts that can be captured across the system
stack during its execution. While network, OS, and hardware
trails have been widely employed for their improved detection
capabilities, many other run-time artifacts can be employed for
detecting malware. Recently, the potential of memory snapshots,
register contents, instruction opcode traces, and power traces have
been explored to detect malware. Such collection modules can be
easily plugged into the data collection framework of JUGAAD in
Figure 7. We leave the inclusion of other such modules and novel
artifacts for malware detection into JUGAAD as future work.
Sustenance. Sustaining the service model relies on a continual
supply of newly reported malware samples. JUGAAD bootstraps
such a supply with a premium subscription with private enterprises
for downloading samples. However, with the continual operation
and increasing user base, we envision a constant supply of malware
samples from the users to ensure a growing data-corpus of malware
behavior.
46
JUGAAD: Comprehensive Malware Behavior-as-a-Service CSET 2022, August 8, 2022, Virtual, CA, USA
9 CONCLUSION
This paper presents JUGAAD, a malware behavior-as-a-service
to present precise and comprehensive malware run-time characteristics
for research. The beneficiaries of JUGAAD are malware
researchers in academia and industry. It offloads the time and
efforts of setting up a real-world evaluation infrastructure for
comprehensive data collection, while alleviating the high risks
involved in handling and executing potent malware. Prior efforts
that provide malware analysis services present inferences on
maliciousness of the user-submitted samples, that are limited by
the capabilities of available state-of-the-art detection engines. In
contrast, JUGAAD provides an unbiased comprehensive view of
real-world malware behavior, enabling researchers to quickly
explore and compare detection mechanisms to counter the evolving
malware landscape.
ACKNOWLEDGMENTS
This research was supported by the Information Security Education
and Awareness (ISEA) project from the Ministry of Electronics
and Information Technology, Government of India. The authors
thank the reviewers and the technical committee for reviewing the
manuscript and providing constructive comments.

____________________________________________________________________________


