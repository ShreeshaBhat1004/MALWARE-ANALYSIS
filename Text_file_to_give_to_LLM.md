# RaDaR: 
RaDaR: A Real-World Dataset for AI Powered
Run-time Detection of Cyber-Attacks
Sareena Karapoola
sareena@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Nikhilesh Singh
nik@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Chester Rebeiro
chester@iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Kamakoti V.
kama@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
ABSTRACT
Artificial Intelligence techniques on malware run-time behavior
have emerged as a promising tool in the arms race against sophisticated
and stealthy cyber-attacks. While data of malware run-time
features are critical for research and benchmark comparisons, unfortunately,
there is a dearth of real-world datasets due to multiple
challenges to their collection. The evasive nature of malware, its
dependence on connected real-world conditions to execute, and its
potential repercussions pose significant challenges for executing
malware in laboratory settings. Consequently, prior open datasets
rely on isolated virtual sandboxes to run malware, resulting in data
that is not representative of malware behavior in the wild.
This paper presents RaDaR, an open real-world dataset for runtime
behavioral analysis of Windows malware. RaDaR is collected
by executing malware on a real-world testbed with Internet connectivity
and in a timely manner, thus providing a close-to-real-world
representation of malware behavior. To enable an unbiased comparison
of different solutions and foster multiple verticals in malware
research, RaDaR provides a multi-perspective data collection
and labeling of malware activity. The multi-perspective collection
provides a comprehensive view of malware activity across the network,
operating system (OS), and hardware. On the other hand, the
multi-perspective labeling provides four independent perspectives
to analyze the same malware, including its methodology, objective,
capabilities, and the information it exfiltrates. To date, RaDaR includes
7 million network packets, 11.3 million OS system call traces,
and 3.3 million hardware events of 10, 434 malware samples having
different methodologies (3 classes) and objectives (9 classes), spread
across 30 well-known malware families.
CCS CONCEPTS
• Security and privacy→Malware and its mitigation; • Computing
methodologies→Machine learning.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CIKM ’22, October 17–21, 2022, Atlanta, GA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9236-5/22/10. . . $15.00
https://doi.org/10.1145/3511808.3557121
KEYWORDS
Artificial Intelligence for Cyber-Security, Datasets, Malware Analysis,
Run-time behavior

1 INTRODUCTION
Cyber-attacks worldwide have increased at an alarming scale, affecting
at least 60% of enterprises worldwide in 2021 [7]. The consequences
of these attacks vary from data loss to reputation damage,
business disruption, financial loss, extortion, and even sabotage
of critical infrastructures. The instrumental tool that enables adversaries
to execute such a wide range of offensive maneuvers is
malware. Despite the decades-long research in malware detection,
the increasing number of attacks and their sophistication indicates
that the problem is far from solved.
Over the last two decades, there have been several attempts
to use Artificial Intelligence (AI) for malware detection that rely
on datasets of malware samples [3, 15, 17, 18, 20, 24, 35, 42, 44,
47, 48, 51]. However, most published works typically use private
datasets. Each dataset captures different malware features such as
static strings in the malware binaries or dynamic run-time behavior,
including network communications, system calls invoked at the
operating system (OS), or hardware events. Furthermore, the size
of the datasets across different papers vary from as low as 500
samples [2, 9, 23] to 1.1 million malware samples [1, 41, 44, 47].
Given the diversity in the datasets, it is not easy to make a fair
comparison across the different detection approaches.
A few organizations, such as Endgame [1] and Microsoft [41]
have attempted to address the issue mentioned above by creating
large-scale open malware datasets containing static features from
malware binaries. These datasets facilitate static analysis that infers
maliciousness using signatures extracted from the malware binaries,
for example, strings in the binaries. These datasets have inherently
become standard benchmarks to compare different detection techniques.
However, detection techniques based on static analysis are
easily evaded by packing and obfuscating the binaries as is becoming
popular in polymorphic and metamorphic malware [36, 37].

Dynamic analysis that executes malware and analyzes their runtime
behavior has recently gained traction over static analysis due
to its ability to counter packing and obfuscation [37]. Such dynamic
analyses, powered with AI, are increasingly adopted for their ability
to detect zero-day1 malware. Primarily, AI enables the modeling
of benign behavior and the identification of anomalies to facilitate
the detection of such malware. Unfortunately, to date, there are
no real-world datasets to compare different dynamic analysis techniques.
This is primarily due to multiple orthogonal challenges in
executing malware to create such a dataset.
C-1. Malware execution typically requires real-world environments,
failing which it can choose not to execute, remaining
stealthy. Specifically, modern malware looks for triggers typically
present in virtualized environments to detect and evade
analysis.
C-2. Malware execution heavily depends on its communications
to remote servers, known as command-and-control (C&C)
servers, that guide and instruct the malware on its subsequent
actions. This requires an active Internet connection
when the malware is executed.
C-3. In many cases, the C&C servers associated with a malware
sample are short-lived. They are pulled down within a few
months after the malware is first reported [29], rendering
later executions of the malware futile.
C-4. While it is important to execute malware in connected environments,
it can be catastrophic if the malicious impact is
not contained.
Hence, executing malware in real-world and connected environments
in a timely manner (before its short-lived C&C servers are
unavailable), is essential to capture a precise representation of malware
behavior, while ensuring the containment. Multiple prior
works have attempted to build datasets of run-time behavior of malware
[3, 15, 17, 18, 20, 24, 35, 42, 44, 47, 48, 51]. However, they are
either not precise due to the use of virtualized and non-connected
environments [17, 18, 20, 24, 35, 44, 47, 51], or are not open to the
research community [3, 15, 18, 20, 42, 48, 51].
This paper presents RaDaR, an open and growing real-world
dataset for run-time behavioral analysis of Windows malware.
RaDaR dataset is precise as it is collected by executing malware
samples in a timely manner (C-3), on a real-world testbed (C-1)
with Internet connectivity (C-2), while containing their malicious
impact (C-4). To ensure a real-world environment, the testbed employs
a network of physical machines connected to the Internet to
execute malware. For timely execution, RaDaR uses an automated
framework that periodically downloads the latest samples from
online repositories [49] and executes them on the testbed to collect
the run-time trails. Thus, the framework ensures that RaDaR is
regularly updated with newer malware samples. Finally, the framework
uses a dedicated Internet connection and a two-level firewall,
which allows the malware to operate while containing its spread.
RaDaR can foster different verticals in malware research with
a multi-perspective data collection and labeling of malware behavior.
A multi-perspective collection, with simultaneous capture
of network, OS, and hardware behavior, enables a fair comparison
of different solutions based on these trails. 
<A Figure>
The figure offers several key insights into the analysis of malware using multiple independent perspectives. Here are the main takeaways:

1. **Multi-dimensional Malware Analysis**: 
   The figure demonstrates that malware can be analyzed from various perspectives, each offering a distinct classification scheme (e.g., family, objective, methodology). This layered approach helps researchers and analysts view malware in a more holistic way, revealing different traits of the same sample depending on the context.

2. **Overlap Between Classifications**:
   The figure shows how the same malware sample (represented by points on the grid) can belong to different categories across perspectives. For example, malware samples labeled "A" and "B" belong to the same family but have different attack objectives and methodologies. This indicates that classifying malware by just one perspective may not be sufficient to capture its full behavior or intent.

3. **Complexity of Malware Behavior**:
   The figure highlights the inherent complexity in malware behavior. A single sample can have multiple functionalities and characteristics that cross different perspectives (e.g., a piece of malware could be part of a family, perform a specific attack objective, use a distinct methodology, and have both backdoor and keylogger capabilities). This underlines the need for diverse analytical lenses to fully understand the behavior of malware.

4. **Different Levels of Abstraction**:
   Each layer represents a different level of abstraction. The "Family" perspective groups malware into 20 broad classes, while perspectives like "Objective" and "Methodology" offer more specific or targeted classifications. This multi-level abstraction highlights the varying granularity that different perspectives provide.

5. **Significance of Cross-Perspective Analysis**:
   The figure emphasizes the importance of considering multiple factors when studying malware. Single-perspective analysis, such as focusing solely on the malware's family or attack objective, may miss key details about its behavior or capabilities. For instance, the same family can consist of samples with diverse objectives and methods, implying that deeper insights are gained by cross-analyzing these factors.

6. **Potential for Better Detection and Defense**:
   By using this multi-dimensional analysis, malware detection systems can be improved. Viewing malware from multiple perspectives allows for a more nuanced understanding, leading to better detection of polymorphic or multi-functional malware, which might otherwise evade detection if analyzed from just one perspective.

In summary, the figure underscores the complexity of malware analysis and the necessity of using a multi-perspective approach to fully capture the diverse behaviors, objectives, and methodologies of malware samples.
<A Figure/>
Figure 1: Independent perspectives of analyzing malware in RaDaR
and their class boundaries. Each surface is a grid of 10,000 points,
where each point represents a malware sample, and its color indicates
its class based on the perspective. For instance, the common corpus
of 10,000 samples belongs to 20 malware families. However, they can
be grouped into 9 classes based on their attack objective or 3 classes
based on their methodology. These classes overlap. For instance, two
malware samples, A and B of the same family, have different attack
objectives and methodologies.
infeasible today as the set of malware samples that these solutions
use in their respective datasets are not consistent. Further, such
a collection has the potential to enable multi-featured analyses,
as malware classes differ in their functionality, leaving varying
run-time trails at network, OS, and hardware.
Another critical aspect not addressed in prior works is the multiperspective
labeling of malware. The same malware can be labeled
differently based on attributes such as its attack objective, methodology
used to infect the victim, capabilities, the information they
exfiltrate, or their family (i.e., code lineage). For instance, spyware
and ransomware have different objectives but may share the same
methodology for infection. Further, some malware may have few
capabilities in addition to their main objective, such as stealthily
logging user keystrokes. Hence, it is beneficial to analyze these malware
attributes independently to draw clear class boundaries. Unlike
prior works [15, 17, 20, 24, 35, 44, 48, 51], which propose a single
perspective to label malware (e.g., family), we propose four independent
perspectives, namely objective, methodology, additional
capabilities, and the information exfiltrated by the malware. Figure 1
illustrates the class distribution and boundaries of 10,000 malware
samples based on these perspectives. While the samples fall into
20 classes based on their family, they can be grouped into 9 classes
based on their objective2. As the class boundaries for the same set
of samples vary widely based on the perspective, independent analysis
of these perspectives is beneficial for effective detection. Such
multi-perspective labeling enables designing specialized solutions
2The classes based on objective include benign applications, ransomware, spyware,
backdoor, banker, cryptominer, deceptor, downloaders, and PUAs.
RaDaR: A Real-World Dataset for AI Powered Run-time Detection of Cyber-Attacks
and novel countermeasures, thus facilitating multiple verticals in
malware research.
Following are the major contributions of this paper:
(1) RaDaR presents an open3 dataset of real-world behavior of
malware samples from 2016 till date collected in a timely
manner, with mechanisms for regular updates, providing
multiple perspectives of malware behavior.
(2) RaDaR provides simultaneous capture of run-time malware
behavior observable at network, OS, and hardware enabling
multi-dimensional analysis and fair comparison of different
solutions.
(3) We propose four independent perspectives to label malware,
including its methodology, objectives, capabilities, and the
type of information it exfiltrates.
(4) To date, RaDaR contains 2.7 terabytes of data with 7 million
network packets, 11.3 million OS system call traces, and 3.3
million hardware events of 10, 434 malware samples having
different methodologies (3 classes), objectives (9 classes), and
spread across 30 well-known malware families.
Following is the organization of the rest of the paper. Section 2
provides the necessary background for the paper. Section 3 presents
the data collection framework. Section 4 discusses the multiple
perspectives of malware behavior that RaDaR provides. Section 5
presents the RaDaR dataset and its class distributions. Section 6
presents our evaluations on the dataset. Section 7 presents the
related work. Finally, Section 8 concludes the paper.
2 BACKGROUND
Malware is a program with malicious intent, which can vary widely
in objective from popping up annoying advertisements (adware),
downloading malicious applications (downloader), exfiltrating sensitive
data (spyware), stealing financial credentials (banker), mining
crypto-currencies (cryptominer), opening a stealthy access pathway
for the attacker (backdoor), to sabotaging the entire system
(ransomware). They can adopt different methodologies to enter the
victim, such as being bundled with other legitimate software, spam
emails that trick the users into downloading malicious files from
infected removable drives, or by exploiting vulnerabilities.
For identification and analysis, security researchers use different
taxonomies to label each malware sample. Malware is traditionally
known based on its type as listed in Table 1 [38]. These names are
based on their unique propagation methodologies (for e.g., trojan,
virus) or their attack objective (for e.g., ransomware). Alternatively,
they are known by their family, which is a collection of malware
produced from the same code base and authors, and may have similar
filename references, language, or C&C infrastructures. Typically,
Anti-Virus (AV) companies also include in the label, a string that
indicates the platform (Windows, Linux, etc.), type (Table 1), and
family[16]. For instance, Win64.Trojan.NukeSped.A_ sample is a
64-bit Windows executable trojan belonging to NukeSped family.
Malware Analysis. Malware analysis is typically done in two
ways. Static analysis examines malware binaries statically without
executing them to extract signatures and imply its maliciousness.

Table 1: Malware taxonomy based on its type or common-name [15,
17, 20, 37, 38, 48]
| Class       | Description                                                                                              |
|-------------|----------------------------------------------------------------------------------------------------------|
| Trojan      | A type of malware that downloads onto a computer disguised as a legitimate program.                       |
| Virus       | A program that can copy itself and infect a computer without the knowledge of the user via infected removable drives. |
| Worm        | A type of malware that typically exploits vulnerabilities to spread by making copies of itself from computer to computer. |
| Bot         | A self-propagating malware capable of infecting a large number of hosts and taking complete control over a computer. |
| Spyware     | A type of malware that infiltrates the victim and keeps gleaning sensitive information for an extended period. |
| Adware      | A type of malware that pops-up annoying advertisements and inappropriate content.                         |
| Downloader  | A type of malware that downloads other malware on the victim.                                             |
| Ransomware  | A type of malware that can sabotage user files and extort a ransom from the user for restoration.         |
| Cryptominer | A type of malware that exploits the computing resources of the victim to mine cryptocurrencies.           |
| Backdoor    | A type of malware that bypasses access control and grants an alternate covert pathway to resources at the victim. |


However, such static signatures can be easily thwarted by techniques that change the malware binary without affecting its
functionality or run-time behavior. For instance, packing used in
the popular polymorphic malware encrypts the contents of the
binary, whereas obfuscation modifies the binary to create different
copies of the same malware [36]. In contrast, dynamic analysis
executes the malware and analyzes the run-time trails observable
on the system stack. Consequently, it can counter the packing and
obfuscation techniques that typically foil static analysis. Further, its
potential to facilitate non-signature-based approaches that compare
the run-time behavior of malware and benign applications makes
it capable of detecting even zero-day malware.
Malware run-time behavioral trails are typically collected at network
[3], OS [18], or hardware [42]. The network data capture all
malware communications, including that to its C&C, whereas OS
data captures its attempt to remain stealthy, achieve persistence,
and execute its objective. More recently, researchers have explored
the potential of micro-architectural events (e.g., number of cache
misses) to detect malware [42]. These hardware events are measurable
using hardware performance counters (HPC) [19] available in
most modern microprocessors. Researchers rely on these behavioral
trails to analyze and detect malware.
Evasion. While dynamic analysis is more powerful than static
analysis against packing and obfuscation, modern malware have
evolved to identify and evade even dynamic analysis environments.
Specifically, they look for artifacts (such as the presence of a virtual
machine) to identify analysis environments and choose to remain
dormant. To this end, the precise collection of malware behavior
requires executing malware in real-world environments.
CIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, and Kamakoti V.
Figure 2: Automated real-world framework [22] for data collection
in RaDaR.
This image represents a workflow for analyzing malware using a system with multiple stages, potentially part of the RaDAR (or a similar) framework for malware analysis. Here’s a detailed breakdown of each component and insights into how they fit together:

### Components and Workflow:

1. **Update Engine** (Top Left):
   - The Update Engine checks the Internet for **new malware samples**.
   - Once new samples are found, they are added to the **Malware Corpus**, which is a repository of malware that will undergo further testing and analysis.

2. **Malware Corpus** (Center):
   - The **Malware Corpus** stores all collected malware samples. It acts as a central database for new and existing malware used in the test and evaluation process.
   - This step highlights the importance of constantly updating the malware dataset to ensure analysis is based on the latest real-world threats.

3. **Test Engine** (Center Top):
   - The **Test Engine** takes raw data from the malware corpus and initiates tests. The tests are likely designed to observe the behavior of the malware in different conditions and collect information such as its objective, methods, capabilities, etc.
   - The **Multi-Perspective Data** (indicated in blue) is generated during this process. It implies that the test engine gathers information across various perspectives (e.g., objectives, methodologies, capabilities), providing a broader understanding of each malware sample.

4. **Label Engine** (Center Bottom):
   - The **Label Engine** processes the raw data from the Test Engine and assigns **Multi-Perspective Labels** (indicated in green). These labels classify the malware samples into different categories based on attributes such as attack objectives, methodologies, family, or capabilities (as discussed in previous figures).
   - The labeled data is then added to the **RaDaR Dataset**, creating a comprehensive dataset enriched with multi-perspective classifications.

5. **RaDaR Dataset** (Bottom Left):
   - The **RaDaR Dataset** is the output of the Label Engine, containing malware samples with detailed, multi-perspective labels.
   - This dataset is likely used for further analysis, research, and training of malware detection systems. It emphasizes the importance of comprehensive labeling, ensuring that the dataset captures malware behavior from multiple viewpoints.

6. **Real-World Testbed** (Top Right):
   - The **Test Engine** can also interface with a **Real-World Testbed** (connected to the Internet). This means the system can test malware in live, real-world environments (e.g., on actual or simulated networks and systems), providing data on how the malware behaves in real, operational settings.
   - This real-world testing is crucial to understanding how malware interacts with different systems and how effective it is under real conditions.
   - It offers insights into how malware could spread, infect, or exploit vulnerabilities in a real-world scenario.

### Insights:

1. **Multi-Perspective Malware Analysis**:
   - The image emphasizes a multi-perspective approach to malware analysis. By collecting both multi-perspective data and labels, the system can classify malware in various ways, such as based on family, attack objectives, capabilities, or techniques. This comprehensive classification allows for a deeper understanding of the malware’s full behavior and impact.
  
2. **Dynamic and Real-World Context**:
   - The workflow integrates real-world testing, making it highly relevant to actual malware threats. By linking to real-world testbeds, the system doesn't just analyze malware in isolation but sees how it behaves in live environments, offering practical insights into its effectiveness and spread.
  
3. **Continuous Updating**:
   - The continuous feedback loop where the **Update Engine** retrieves new malware samples from the Internet ensures that the **Malware Corpus** is always updated. This dynamic aspect of the workflow ensures that the system keeps pace with evolving threats.
  
4. **Integration of Raw Data and Labels**:
   - The system ensures that raw data from testing is converted into meaningful, multi-dimensional labels, enriching the dataset and providing a resource that can be used for future detection models, research, and defense systems.

5. **Importance of Comprehensive Datasets**:
   - The image highlights the creation of a robust, labeled malware dataset (RaDaR Dataset). Having a well-labeled dataset is critical for training machine learning models to detect and classify malware accurately.
  
6. **Modular Design**:
   - The system is modular, with distinct engines for different tasks (e.g., testing, labeling, updating), which could suggest flexibility. Each component can potentially be updated or enhanced independently, allowing for scalability and adaptability as malware threats evolve.

### Conclusion:

This diagram illustrates a sophisticated malware analysis and detection framework, focusing on collecting, testing, and classifying malware across multiple perspectives. By leveraging both real-world test environments and dynamically updated datasets, the system provides comprehensive insights into malware behavior and characteristics, helping improve detection and mitigation strategies. The use of multi-perspective data and labels ensures that malware analysis is both thorough and multi-dimensional, capturing the complexities of modern malware.

3 DATA COLLECTION FRAMEWORK
The trails captured by executing a malware sample are precise if
they closely represent its real-world behavior. This section presents
the framework used for collecting the RaDaR dataset and describes
how it addresses the challenges C-1 to C-4 to ensure the precision
of the collected trails.
Figure 2 describes the data collection framework [22], which has
three engines, namely the update, test, and label engines. The update
engine downloads the malware samples for analysis, whereas
the test engine executes them on a real-world testbed to collect
their run-time trails. Finally, the label-engine labels the collected
trails based on different perspectives. Next, we discuss how the
framework addresses the challenges in precisely collecting malware
behavior.
Real-World Testbed (C-1). To prevent malware from evading
analysis, the environment should be close to that of the real-world,
which includes non-virtualized physical systems, preferably a heterogeneous
network of devices, and Internet connectivity. The test
engine employs a real-world testbed designed in our laboratory
with a network of 512 different physical machines to execute malware.
These machines include Windows and Linux desktops and
single-board computers (Raspberry Pi and Intel Galileo boards).
This heterogeneity of devices ensures that the testbed is likely to
have sufficient real-world conditions that malware looks for similar
to that in the wild.
Further, the execution of each sample can affect the system state,
such as files, registry, and the micro-architecture. Since these modifications
made by prior samples can affect subsequent analysis,
we start every execution in a clean initial state of the system. To
achieve this, the testbed has a quick state-reset mechanism that
resets the machines to initial states before executing every sample.
Internet Connectivity (C-2) and Containment (C-4). To provide
connectivity while containing the malware impact, we use
a dedicated Internet connection (ERNET [10]) for the framework
that is isolated from our university network. Further, the testbed
connects to the Internet via a two-level firewall that is lenient on incoming
communications while extensively scrutinizing all outgoing
communications for malicious behaviors such as Denial-of-Service
attempts, network scans, and spam emails. On detecting such malicious
outgoing communications, the firewall blocks them. Thus, the
testbed allows malware communications with their remote C&C
servers while preventing the malicious impact from permeating
outside the testbed.

Figure 3: T-SNE visualization [52] that indicates the distinguishability
of backdoor, spyware, and ransomware run-time trails from that
of benign applications at network, OS, and hardware. We capture
these trails by executing 1000 samples of each class on the real-world
testbed. The trails are pre-processed to extract 40 features from the
network (e.g., number of flows), 9 features from OS (e.g., write to file),
and 56 features from the hardware (e.g., L1 cache misses) trails. The
axes values are 2D- projections of these multi-dimensional features.
### Diagram Insights:

This figure appears to visualize the distribution of malware samples (likely backdoor, spyware, and ransomware) across different environments or dimensions: **Network**, **OS**, and **HPC** (which could stand for High-Performance Computing). Each subplot shows how the malware samples cluster or spread within each dimension. Here are detailed insights for each section:

1. **Rows Represent Different Environments**:
   - **Network**: The top row shows how different malware types (backdoor, spyware, ransomware) are distributed in a network environment.
   - **OS**: The middle row represents the distribution across an operating system environment.
   - **HPC**: The bottom row illustrates how malware is distributed in a high-performance computing (HPC) environment.

2. **Columns Represent Different Malware Types**:
   - **Backdoor**: The first column visualizes the distribution of backdoor malware samples.
   - **Spyware**: The second column visualizes spyware distribution.
   - **Ransomware**: The third column visualizes ransomware distribution.

3. **Color Coding**:
   - The color coding (red and blue) likely represents different subcategories or classification characteristics (such as distinct classes or families within a particular type of malware).
   - The colors can also indicate the results of different clustering algorithms, showing how samples with certain traits tend to group together.

4. **Key Observations by Malware and Environment**:
   - **Network (Top Row)**:
     - The distribution of samples (for backdoor, spyware, and ransomware) appears fairly scattered in the network environment, with no obvious structure or clustering for any malware type.
     - This could suggest that in network analysis, the behavioral patterns of these malware types are varied and do not fit into strict categories.
   
   - **OS (Middle Row)**:
     - The distribution for backdoor malware in the OS environment shows a sparse scattering of samples, suggesting either a lack of distinct characteristics or that the OS layer is not as relevant for backdoor analysis.
     - Spyware and ransomware show more structured, circular clustering. This could suggest that in the OS environment, these types of malware follow clearer, identifiable patterns, likely due to specific techniques they use (e.g., targeting system processes or files).
   
   - **HPC (Bottom Row)**:
     - For the HPC environment, backdoor malware shows a tight, circular clustering, suggesting that HPC characteristics are strongly correlated with certain backdoor behaviors.
     - Spyware has a more complex structure, indicating that its behavior in HPC environments might be more varied or dependent on additional factors.
     - Ransomware exhibits a dense, tightly clustered distribution, possibly due to the specific ways ransomware exploits HPC systems for high impact (e.g., encrypting large data sets or locking out critical infrastructure).

### Text Replication:

Here’s how the diagram might be described in text, column by column, for easy interpretation:

---

**Backdoor Malware**:
- **Network**: The samples are scattered widely with no apparent clustering, indicating varied behavior in network environments.
- **OS**: The backdoor samples show minimal distribution and are sparsely placed, suggesting a lack of distinctive behavior or a weaker correlation with the OS environment.
- **HPC**: In HPC environments, the backdoor samples cluster tightly in a circular pattern, suggesting stronger behavioral similarities or predictable attack patterns.

---

**Spyware Malware**:
- **Network**: The samples are spread out and show some clustering, but overall behavior is still varied in the network environment.
- **OS**: There is a clear circular clustering of spyware samples, indicating that specific spyware behaviors emerge when interacting with the OS layer.
- **HPC**: The spyware samples form a more complex, partially circular structure in the HPC environment, suggesting some consistency but also diversity in how spyware interacts with HPC systems.

---

**Ransomware Malware**:
- **Network**: The ransomware samples are widely distributed with no obvious clustering, showing varied behavior in the network environment.
- **OS**: Similar to spyware, the ransomware samples form a clear circular pattern, implying consistent and predictable behaviors when attacking the OS environment.
- **HPC**: Ransomware samples exhibit a very dense clustering, indicating that ransomware behavior in HPC systems is uniform and strongly correlated, likely due to its specific attack methods on high-value computing resources.

---

### Conclusion:

This visualization demonstrates how different types of malware (backdoor, spyware, ransomware) interact with different environments (network, OS, HPC). The distribution patterns suggest that malware exhibits varied behavior depending on the environment, with some environments showing clearer, more consistent attack patterns (e.g., ransomware in HPC) while others display greater diversity in behavior (e.g., backdoor malware in network environments). Understanding these patterns can aid in designing better detection and defense mechanisms tailored to each environment.


Automated Timely Execution of Malware (C-3). The framework
in Figure 2 is automated to collect behavioral data of malware
in a timely manner when their short-lived C&C servers are likely
Benign Malware to be active. The update engine periodically crawls public malware
repositories for newly reported samples and downloads them to the
malware corpus. The addition of new samples to the corpus triggers
the test engine, which executes the latest samples from the corpus
on the real-world testbed. The testbed collects the behavioral trails,
which are later labeled and added to the RaDaR dataset. Thus the
framework ensures a regular feed of new malware samples reported
in the wild, which are analyzed immediately and updated to the
RaDaR dataset.
4 MULTI-PERSPECTIVE ANALYSIS
RaDaR presents multiple perspectives of malware execution, including
data collection observed at network, OS, and hardware, and
different ways of labeling them. While multi-perspective collection
provides a comprehensive view of malware activity in the system,
multi-perspective labeling provides different perspectives to analyze
the same malware. This section discusses the need for different
perspectives to collect and label malware behavior.
4.1 Multi-Perspective Collection
Malware classes differ in objectives and functionalities and can leave
varying trails in the network, OS, and hardware. Figure 3 presents
a t-SNE visualization [52] of behavioral features that is indicative
of the distinguishability of backdoor, spyware, and ransomware
trails from that of benign applications. As seen in the figure, some
malware classes are more easily identifiable using one trail than
the others.
RaDaR: A Real-World Dataset for AI Powered Run-time Detection of Cyber-Attacks CIKM ’22, October 17–21, 2022, Atlanta, GA, USA
Figure 4: Overlapping malware attributes: (a) Methodology and
Objective: The horizontal blue shaded bars (trojan, worm, and virus)
differ in the methodology, while the vertical bars (e.g., spyware,
backdoor, and downloader) differ in the attack objective; (b) Family
and Objective: The green circles indicate families of different sizes.
The rectangular boxes indicate families having the same objective.

### **Deep Insights into the Image**

This image contains two distinct sub-diagrams: (a) Methodology and Objective, and (b) Family and Objective. Both provide deep insights into how malware is categorized and how different characteristics like **methodology**, **family**, and **objective** interplay in malware analysis. Let’s dive into each sub-diagram:

---

### **(a) Methodology and Objective**

In this diagram, different malware types are represented by overlapping rectangular blocks, with each block corresponding to a malware class. The x-axis likely represents **methodology** (the techniques or processes the malware uses), and the y-axis shows **objective** (what the malware aims to achieve). Key observations:

1. **Overlap Between Methodologies and Objectives**:
   - Certain malware types, such as **Spyware**, **Backdoor**, and **Downloader**, have significant overlap in both methodology and objectives, indicated by their shared space. This suggests that these types of malware may share similar ways of operating and achieving their goals (e.g., gaining unauthorized access, stealing information).
   - **Backdoor** and **Downloader** malware, in particular, overlap strongly, implying that they may rely on similar methodologies (like gaining access covertly and downloading additional payloads).
   - On the other hand, classes like **Virus** and **Botnet** are more isolated, indicating distinct methodologies or objectives, which means they may be more specialized in their behaviors.

2. **Boundaries of Malware Categories**:
   - The boundaries separating the malware types hint that some classes are relatively distinct. For example, **Trojan** and **Ransomware** sit on the lower end of the methodology/objective axis, meaning they could have more well-defined techniques that differentiate them from others.

3. **Adware and Ransomware Relationships**:
   - **Adware** and **Ransomware** occupy separate but connected spaces. This relationship suggests that while their objectives differ (e.g., Adware seeks profit through advertisements, Ransomware demands ransom for decryption), they could share similarities in how they are delivered or installed (methodology).

4. **Visual Clustering**:
   - The way some classes, like **Backdoor** and **Downloader**, are closer together could indicate that defense mechanisms or detection systems for one might be effective for the other, as they share common traits. The same could apply to **Botnet** and **Virus**.

---

### **(b) Family and Objective**

This sub-diagram uses circle clusters to represent malware families and their objectives. Each quadrant corresponds to a different type of malware (Spyware, Backdoor, Ransomware, Downloader), and the size and color of the circles carry key insights:

1. **Circle Size Represents Family Proportion**:
   - The size of the circles likely indicates the proportion of samples belonging to each family. Larger circles represent more prevalent families, meaning those families contribute the most to that particular malware type.
   - For example, **Spyware** and **Ransomware** show large circles, indicating certain families dominate within these malware types. This suggests that these types have "leading families" that are responsible for the bulk of activity in this space.

2. **Color Represents Objectives**:
   - The color coding differentiates between **families with similar objectives** (yellow, pink) and **families with different objectives** (green). 
   - In some quadrants, like **Ransomware**, green circles dominate, suggesting that multiple families of ransomware share similar objectives (e.g., extortion), while in **Downloader**, the objectives might vary more (represented by a mix of colors).

3. **Concentration of Objectives**:
   - In the **Spyware** and **Backdoor** quadrants, you notice a more evenly distributed mix of small and large circles, representing that these malware types are more diverse in their objectives and family composition. This diversity could make it harder to classify or detect these types, as different families may employ different strategies and methods.
   - In contrast, **Ransomware** shows tight clustering with large green circles, suggesting a more unified objective (likely financial gain) and more concentration around a few prominent families.

4. **Different vs. Similar Objectives**:
   - The color differentiation highlights that while many families share similar objectives, there are some notable differences. For example, in the **Backdoor** quadrant, some families (in pink and yellow) pursue distinct objectives, which could imply that while the delivery mechanisms are similar, the end goals differ (e.g., some might aim to steal data, others to maintain persistent access).

5. **Implications for Malware Detection**:
   - The clustering patterns in each quadrant suggest that malware detection methods could be tailored based on the specific objectives and families of each type. For example, detecting **Ransomware** might be easier given the more concentrated objectives, while detecting **Downloader** malware could require more nuanced approaches due to the variety of objectives and methods represented.

---

### **General Insights from Both Diagrams**:

1. **Interplay of Methodology, Objective, and Family**:
   - The two diagrams together emphasize that malware classification is not linear or one-dimensional. Malware families can share objectives but employ different methodologies, or conversely, the same methodology could be used to achieve different objectives across families.
   - This complexity implies that effective malware detection systems need to account for both objective-based (e.g., what the malware is trying to achieve) and method-based (e.g., how it operates) analysis to be comprehensive.

2. **Family-Objective Relationship**:
   - The second diagram shows that while some families are objective-agnostic (represented by green circles with different objectives), others are strongly linked to a particular goal. This distinction can help in focusing security efforts. For example, focusing on large green circles in the **Ransomware** quadrant might yield more success in detecting financially motivated attacks.

3. **Overlap in Functionality**:
   - The overlap in the first diagram hints that malware can often serve multiple purposes, or evolve over time from one type to another (e.g., a **Downloader** could evolve into a **Ransomware** threat by first downloading the ransomware payload). Understanding these overlaps can help security researchers anticipate and detect blended threats.

4. **Modularity in Malware Analysis**:
   - Both diagrams showcase the modularity in how malware can be studied: by **methodology**, **objective**, and **family**. This kind of breakdown allows for multi-dimensional analysis, where researchers can see not only how malware behaves (methodology) but also what it aims to achieve (objective) and how it relates to others in its class (family).

---

### Conclusion:

These diagrams provide a rich visualization of how different types of malware relate to their methodology, objectives, and families. They highlight the diversity in behavior and goals among different malware types and suggest that a holistic approach—looking at both the methods and objectives—is necessary for effective malware detection and classification. Additionally, the clear clustering in some areas (e.g., **Ransomware** families) suggests there are dominant trends within certain malware classes, while others (e.g., **Spyware**) are more diverse and harder to classify definitively. These insights can be invaluable for building more robust malware detection systems and security defenses.


 For instance, backdoor functionality involves consistent communications to its remote adversary to create a covert access
pathway, leaving strong indicators in the network. Hence, backdoor
behavior is most distinguishable from benign in the network
compared to the OS and hardware. On the other hand, spyware
functionality, which involves reading files and gleaning sensitive information,
is most distinguishable in the OS and network compared
to the hardware. Likewise, ransomware is most distinguishable in
the hardware as it triggers distinct micro-architectural events when
it encrypts a large number of victim’s files.
Prior works have extensively explored run-time trails for malware
detection, typically using one of the network, OS, or hardware
trails [3, 5, 15, 17, 18, 20, 24, 35, 44, 48, 51]. Each of these works
presents highly acceptable results using the respective trails. However,
a comparison of this large body of research to evaluate the
capabilities of different trails is infeasible today, as every work
uses execution trails of different samples collected in different timeframes
and environments. A fair comparison of these solutions
requires a comprehensive view of malware run-time activity in the
system. While few datasets present a combination of network and
OS trails [17, 20, 35, 44], we argue that the hardware perspective is
also needed to build a comprehensive view of malware behavior,
especially for classes like ransomware. Further, the differences in
capabilities of run-time trails provide opportunities to explore more
sophisticated ensemble-based approaches.
To facilitate multiple run-time perspectives, the testbed in Figure
2 simultaneously captures the network, OS, and hardware trails
during malware execution. We use tshark [34] and Windows Process
Monitor [33] to capture network and OS trails, respectively. On
the other hand, we develop a customized Windows driver based on
existing works to measure Hardware Performance Counters [13].
The network logs are collected at the gateway connecting the
testbed to the Internet since all the network traffic is routed through
it. The traffic can be attributed to different machines in the testbed
based on the IP address. On the other hand, OS and hardware behavior
are collected locally at the machine executing the malware,
and are attributed to the malware based on its process identifier.
4.2 Multi-Perspective Labeling
Contemporary malware research typically labels malware based
on type, family, or AV-labels (Refer Section 2) [15, 18, 20, 24, 35, 44,
45, 47, 48]. However, malware samples are diverse with multiple
attributes, making it favorable to label malware with different perspectives.
Further, some of these attributes may overlap, warranting
an independent evaluation of each perspective.
Malware diversity and attributes. A malware sample can be
characterized by a tuple of its attributes as ⟨methodology, objective,
capabilities, family, information it exfiltrates⟩, each of which can
vary widely as discussed next.
(1) Methodology. Malware can adopt different methodologies
to infect the victim and propagate to other systems. Accordingly,
they can be a virus, trojan, or worm. A virus is
a malicious piece of code that attaches to a host program
to get executed. It is transmitted from one computer to another
through the host program. On the other hand, trojans
and worms are standalone programs. While trojans require
user interactions for activation and propagation, worms can
self-activate and self-replicate via the network.
(2) Objective. Malware can have different attack objectives based
on which it can be adware, downloader, spyware, banker,
cryptominer, backdoor, botnet, or ransomware (Refer Section
2). Accordingly, they pose varying levels of risk to users.
For instance, ransomware that sabotages the system is a
high-risk malware, whereas adware that merely pops up
user-annoying advertisements is a low-risk malware.
(3) Capabilities. Apart from the main objective, we observe that
some malware may also have other capabilities. Some malware
have key-logging capability to log user inputs, while
others may have a hidden backdoor that opens an alternate
access pathway for the attacker, in addition to their primary
objective.
(4) Information Exfiltrated. We observe that every malware either
steals or destroys some information of the target. The
exfiltrated information typically includes one or more of
the following | (i) System details (e.g., version of OS and
system settings to identify analysis environments for evasion);
(ii) User credentials; (iii) Keystrokes; (iv) Application
passwords; (v) Details of email accounts; (vi) Clipboard and
screenshots; (vii) Digital certificates; (viii) File-system contents;
(ix) Process and hardware details; (x) Network-related
details, including active ports and other systems in the network;
(xi) Online activities of the user; and (xii) location
and language.
Overlapping Attributes. While malware samples have different
tuples characterizing them, they share some commonalities due
to overlapping attributes. For instance, spyware that exfiltrates
data could be implemented using any methodology (trojan, worm,
or virus). Figure 4a illustrates this overlap with a distribution of
malware samples based on their methodology (blue shaded horizontal
bars) and objective (vertical bars). The horizontal and vertical
bars are individually disjoint, but together, they overlap and can
significantly affect the accuracy of classification.
Similarly, Figure 4b illustrates the overlap between attributes of
family (green circles) and objective (rectangular boxes). As evident,
many families can share the same attack objective. For instance,
Corebot [39], Delf [12], and Formbook [31] are all backdoor families.
Further, a family may have malware samples of different objectives
(i.e., circles overlapping two or more boxes). For instance,
CIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, and Kamakoti V.
Table 2: Features observed at network, operating system and hardware.
| Category | Features |
|----------|----------|
| **Network** | Connection-based (Destination IP and Port, Protocol; Total number of flows, packets per flow; Number of inbound and outbound packets; Average and standard deviation duration; Ratio of sizes of packets from originator and responder; Ratio of established states.) |
|          | TLS-based (Ratio of TLS and non-TLS connection records; Ratio of TLS and SSL version in connection record; SNI and destination IP comparison; Ratio of self-signed certificates; Ratio of SSL records having SNI; Ratio of self-signed certificates; Average of certificate paths.) |
|          | Certificate-based (Average length of certificate keys, average and standard deviation of certificate validity; Validity of certificate period; Number of certificates; Number of domains in certificate; Ratio of certificate records and SSL records; the presence of ServerName Indication in subject alternative name.) |
|          | HTTP-based (URL path length; Number of URL query parameters; Filename length; Inter-arrival time; Number of URL flows; Number of downloaded and uploaded bytes; Number of files; Ratio of digits, Alphabets, Special characters; Upper/lower case, and Vowels in filename, URL, and Hostname respectively.) |
| **OS (e.g., L_ OS)** | Registry (Read, Query or Write to Windows Registry), File (Read a file, Create/Write to a file, Lock/Unlock a file); Security (Retrieve/change security descriptors of files, Add new CLSID2, Modify existing CLSID); Process-related (Process Start, Process Exit, Load an image, Thread create, Thread exit); Path-indicator (Encoding of the path of the resource accessed, Prior knowledge of association of the filename, or directory path with malware); Network (TCP Send/Receive, UDP Send/Receive, Length of data); Parameters and return values (Length of data read/write; Desired Access rights for the requested resource, Shared Read, Write or Delete, Encoding of the options including Open Reparse Point, Synchronous I/O Alert etc, return values) |
| **Hardware** | Hardware Performance Counters (HPCs) (54 HPCs such as: Core clock cycles; Instructions retired; Instruction length decoder stalls; micro-operations from loop stream detector, decoders, and micro-operation cache; resource stalls; Branches taken; Mispredicted branches; Register moves eliminated; Register moves elimination unsuccessful; L1-data cache misses; L1-data cache replacements Instruction-TLB misses, L2 requests, DTLB store/load misses leading to a walk; Hardware interrupts received; Instruction cache misses); Memory load LLC hits/misses.) |

Bladabindi is a family of samples that can be a backdoor or spyware
[32]. Figure 1 shows the significant variations between classes
based on these attributes of a corpus of 10, 000 malware samples.
Thus, it is beneficial to analyze different malware attributes independently
to draw clear class boundaries for effective classification.
Labeling in RaDaR. To facilitate multi-perspective analysis, RaDaR
labels malware with four different independent attributes in addition
to its family, namely, methodology, objective, two capabilities
including keylogger and backdoor, and the information it exfiltrates.
Each of these attributes presents different perspectives of malware
and can aid in designing specialized solutions such as mechanisms
to prevent malware infection (based on methodology) or attack
mitigation (based on objective). Specifically, an objective-based perspective
can enable multi-dimensional models to improve detection
accuracy (refer Section 4.1) and customized responses based on user
tolerance to false positives. For example, users would prefer the
termination of high-risk malware (such as ransomware) as soon
as possible to minimize the attack impact. On the other hand, they
would not want the termination of low-risk classes like adware
unless the detection is highly precise in order to minimize the false
positives. Likewise, a capability-based perspective can help design
specialized keylogger or backdoor detectors, whereas an information
leak-based perspective can help implement appropriate data
protection mechanisms.
In contrast, the single perspective of type, family or AV-labels in
prior works [15, 18, 20, 24, 35, 44, 45, 47, 48], limits the scope of analyses
possible on such datasets. Further, both type and family-based
perspectives can result in fuzzy class boundaries, thus affecting
the classification accuracy. Specifically, type-based perspective (Refer
Table 1) mixes the attributes of methodology and objective
(Figure 4a). On the other hand, different families can have similar
functionalities and behavioral trails, while others may have distinctly
behaving malware samples in the same family (Figure 4b).
Such a characteristic of family-based perspective can be attributed
to its definition, which is more indicative of code lineage and static
features than run-time behavior. In contrast, AV-based labels often
include specific meta-data in addition to type and family names,
making them too specific for any generalization [43].
5 RADAR DATASET
The RaDaR dataset to date contains the behavior of 10,434 malware
samples from 2016 obtained from Anti-Virus companies [21] and
public malware repositories [50] using the automated framework
discussed in Section 3. In this section, we first describe the snapshots
and features in the RaDaR dataset. We next present the class
distributions of different perspectives in RaDaR .
Raw Behavioral Snapshots. As described in Section 2, the logs
capture the time-series behavior of malware execution observable
at network, OS, and hardware. Network logs contain the network
packets from the machine executing the malware. In contrast, the
OS logs capture all the system call traces of the malware, including
its file, registry, process, and other operations. On the other hand,
the hardware logs contain the values of hardware performance
counters at a periodic interval of 100 ms. To date, RaDaR has 2.7
tera-bytes of data, including 7 million network packets, 11.3 million
OS system call traces, and 3.3 million hardware events.
RaDaR: A Real-World Dataset for AI Powered Run-time Detection of Cyber-Attacks CIKM ’22, October 17–21, 2022, Atlanta, GA, USA
Features. RaDaR extracts a comprehensive set of features about
malware execution as listed in Table 2. For the network data, we use
the Zeek [54] tool to pre-process the packet-level logs into network
flow summaries. A network flow comprises of all communications
that share the same source and destination IP addresses and ports.
In total RaDaR has 60K network flow summaries and 58 features.
We use custom scripts to parse the OS and hardware logs. In this
way, we extract 11 features for OS and 54 features for hardware.
The data is converted to a matrix, where the rows are the behavioral
snapshots and columns are the feature values. Thus, RaDaR has
3 matrices of order 60K × 58 for network, 11.3M × 11 for OS, and
3.3M× 54 for hardware, and each row in these matrices are labeled
as per the perspectives.
Class Distributions. Table 3 shows the distribution of malware
based on different perspectives in the RaDaR dataset. Most malware
samples in the dataset belong to the trojan class (71%), which is similar
to the distribution of malware in the real-world [46]. Similarly,
RaDaR contains representative classes having different objectives,
including banker, downloader, Potentially Unwanted Applications
(PUA), deceptor, spyware, backdoor, ransomware, and cryptominers.
While 20.3% of malware samples in RaDaR can log keystrokes
of the users in addition to their primary objective, 19.1% of them
have backdoor capabilities. A graphical representation of this distribution
and the significant overlaps between malware classes across
perspectives is shown in Figure 1.
Table 3 also provides the number of behavioral snapshots present
for each class per perspective across network, OS, and hardware in
RaDaR. As evident, the distribution of network, OS, and hardware
snapshots may not match that of malware in the dataset, as each
malware differs in its activity across the three system components.
Table 4 lists the distributions of malware families in the dataset.
RaDaR contains 30 families that can be grouped into 9 classes based
on the objective of the malware. Table 5 shows the distribution
of samples based on the information the malware collects and
steals from the target. Most malware (> 51%) collect the system
information to help it identify virtualized analysis environments.
While 19% of malware log keystrokes, 6% and 3% of samples capture
the screenshots and clipboard.
6 RESULTS
In this section, we present the results of our evaluation of RaDaR.
These results on well-known models provide a baseline for future
experiments. For our experiments, we apply Principal Component
Analysis (PCA) to reduce the feature space to 10 network features,
10 OS features, and 20 features from the hardware. For each perspective,
we split the dataset in 70:15:15 ratio corresponding to
train, validate and test sets, with an even distribution of classes.
Methodology. To evaluate the detection of methodology, we train
standard multi-class machine learning (ML) models including Decision
Tree [25], k-Nearest Neighbours [27], Logistic regression [26],
Random-Forest [28], XGBoost [53], and LightGBM [30]. For XGBoost
and LightGBM, we consider both the one-versus-one (OvO)
and one-versus-rest (OvR) configurations for multi-class classification
[30, 53]. Table 6 presents the best F1-Score observed for detecting
methodology using network, OS, or hardware trails. Methodology
of a malware is best detected using OS and hardware features
as compared to network. Intuitively, methodology deals with how
malware infects a system and activates itself, and hence, OS and
hardware trails have stronger indicators than the network trails.
While Random-Forest offers the best F1-Score for detection at network
and OS, LightGBM gives the best F1-Score at hardware.
Objective.We also observe that the detection F1-Score of objective
using multi-class classifiers [25–28, 30, 53] was very low, which
presents a wide scope for model improvements. In this regard, we
next evaluate how different each objective class is from benign applications.
To this end, we train specialized XGBoost binary models

| **Perspective** | **Class**     | **%age** | **Number of Snapshots**               |
|-----------------|---------------|----------|---------------------------------------|
|                 |               |          | **Network** | **OS** | **Hardware** |
| **Methodology** | Trojan         | 71.55%   | 36K         | 37K   | 4.8M        |
|                 | Worm           | 11.8%    | 14K         | 2.6M  | 413K        |
|                 | Benign         | 9.95%    | 8594        | 1.3M  | 578K        |
|                 | Cryptominer    | 4.95%    | 393         | 155K  |             |
|                 | Banker         | 13.5%    | 1755        | 777K  | 517K        |
|                 | Spyware        | 13.5%    | 1878        | 1.9M  | 512K        |
|                 | Backdoor       | 7.35%    | 894         | 1.1M  | 578K        |
| **Objective**   | PUA            | 10.75%   | 1594        | 4.8M  |             |
|                 | Downloader     | 15.45%   | 1991        | 7.1M  | 6.8M        |
|                 | Deceptor       | 9.45%    | 893         | 2.3M  | 578K        |
|                 | Benign         | 9.95%    | 8694        | 1.3M  | 578K        |
| **Capabilities**| Keylogger      | 30.5%    | 8618        | 1.8M  | 108K        |
|                 | Non-Keylogger  | 69.75%   | 2395        | 8.4M  | 2.4M        |
|                 | Benign         | 9.95%    | 8694        | 1.3M  | 578K        |
|                 | Backdoor       | 71%      | 334         | 4.1M  | 3.2M        |
|                 | Non-Backdoor   | 9.75%    | 33.4K       | 1.1M  | 3.3M        |
|                 | Benign         | 9.95%    | 8694        | 1.3M  | 578K        |

_Table 4: Families in RaDaR_

| **Objective Class** | **Count** | **Family**                                                                 |
|---------------------|-----------|-----------------------------------------------------------------------------|
| Downloader          | 1991      | Agent(1989), Chindo(69), Small(16), XeyaRAT(6), erosaa(9)                   |
| Banker              | 1442      | Emotet(144)                                                                 |
| PUA                 | 1106      | enox(1106)                                                                  |
| Ransomware          | 770       | Gandcrab(550), cueloo(305), Ryuk(40), Rapid(29), Ouroboros(25), Sigma(40)   |
| Spyware             | 1639      | Bladabindi(946), Agent(350), Volso(255), Butbray(77), Expolit(14)           |
| Backdoor            | 1388      | Corebot(330), Formbook(250), Agent(441), Delf(377)                          |
| Deceptor            | 585       | Deceptor(585)                                                               |
| Cryptominer         | 500       | Canniminer(500)                                                             |
| Dropper             | 260       | Agent(220), NukeSpeed(33), Delf(10)                                         |

_Table 5: Distribution of information exfiltrated in RaDaR_

| **Information** | **%**   | **Information** | **%**   | **Information** | **%**    |
|-----------------|---------|-----------------|---------|-----------------|----------|
| System          | 51.31   | Accounts        | 1.35    | Location        | 3.71     |
| User            | 20.00   | Keystrokes      | 19.89   | Language        | 3.79     |
| Network         | 8.70    | Screenshots     | 8.19    | Data            | 3.43     |
| Hardware        | 4.91    | Passwords       | 1.34    | Documents       | 3.49     |
| Process         | 2.40    | Clipboard       | 3.19    | Unknown         | 41.45    |

CIKM ’22, October 17–21, 2022, Atlanta, GA, USA Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, and Kamakoti V.

Table 4: Results of the evaluation of RaDaR
| Perspective    | Class         | Best F1-Score                                | Best Model                                |
|----------------|---------------|----------------------------------------------|-------------------------------------------|
|                |               | **Network** | **OS**   | **Hardware** | **Network** | **OS**   | **Hardware** |
| **Methodology**| Benign         | 0.996       | 1.0     | 0.999         | RF          | LGBM     | LGBM         |
|                | Trojan         | 0.856       | 0.999   | 0.968         | RF          | RF       | LGBM         |
|                | Worm           | 0.866       | 0.999   | 0.988         | RF          | RF       | LGBM         |
| **Objective**  | Cryptominer    | 0.83        | 0.87    | 0.94          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Banker         | 0.9         | 0.82    | 0.75          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Spyware        | 0.89        | 0.92    | 0.7           | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Backdoor       | 0.92        | 0.8     | 0.75          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Ransomware     | 0.86        | 0.8     | 0.68          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | PUA            | 0.88        | 0.65    | 0.56          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Downloader     | 0.99        | 0.84    | 0.6           | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Deceptor       | 0.89        | 0.87    | 0.78          | XGB-Binary  | LGBM-Binary | XGB-Binary |
|                | Benign         | 0.99        | 0.998   | 0.998         | XGB-Binary  | LGBM-Binary | XGB-Binary |
| **Capabilities**| Non-Keylogger | 0.892       | 0.994   | 0.999         | XGB-OVR     | XGB-OVR   | XGB-OVR     |
|                | Keylogger      | 0.875       | 0.89    | 0.999         | XGB-OVR     | XGB-OVR   | XGB-OVR     |
|                | Benign         | 0.998       | 0.999   | 0.998         | XGB-OVR     | XGB-OVR   | XGB-OVR     |
|                | Non-Backdoor   | 0.930       | 0.998   | 0.998         | LGBM-OVR    | RF        | XGB-OVR     |
|                | Backdoor       | 0.935       | 0.8     | 0.9           | LGBM-OVR    | RF        | XGB-OVR     |
| **Family (e.g., Backdoor)** | Corebot [39] | 0.430   | 0.050   | 0.870         | XGB-OVR     | RF        | XGB-OVR     |
|                | Delf [12]      | 0.390       | 0.153   | 0.982         | XGB-OVR     | RF        | XGB-OVR     |
|                | Agent [11]     | 0.682       | 0.151   | 0.458         | XGB-OVR     | RF        | XGB-OVR     |
|                | Formbook [31]  | 0.496       | 0.316   | 0.401         | XGB-OVR     | RF        | XGB-OVR     |

RF: Random Forest XGB: XGBoost LGBM: LightGBM XGB-Binary/LGBM-Binary: Binary Models comparing benign and an objective class
OvR: One-versus-Rest OvO: One-versus-One
on each objective class and benign applications. Table 6 presents
the results. Each malware class differs in the trails that best differentiate
it from benign applications, as highlighted in the table.
While the network trails are effective for most classes, OS trails are
the best to detect spyware. Similarly, hardware trails are the most
effective for detecting cryptominers and ransomware. We leave the
exploration of complex models and ensemble-based approaches
that can exploit these differences in run-time trails for future work.
Capabilities. We next evaluate detection of keylogging capability
using standard multi-class models [25–28, 30, 53] on network,
OS and hardware trails. The perspective has three classes namely,
benign, non-keylogger, and keylogger. Interestingly, we find the
keylogging capability is best detected using hardware and OS features,
as shown in Table 6. Its detection F1-Score is the lowest in the
network, as keylogging primarily involves intercepting the system
calls to log the user keystrokes and does not involve any network
activity.
Similar evaluation of backdoor capability using multi-class models
[25–28, 30, 53] indicate that it is best detected with OS features,
as compared to network and hardware (Table 6). This capability
perspective has three classes, namely, benign, non-backdoor, and
backdoor. The results are in contrast to the objective-based evaluation,
wherein OS trails had lower detection F1-Score than network.
We believe the models trained with the capability perspective
are able to learn the traits of backdoor functionality better than
objective-based classes, which can have overlapping capabilities.
Family. Finally, we evaluate the relevance of family taxonomy for
malware detection based on run-time behavior. As there are a large
number of malware families (Table 4), we consider the example of
backdoor families. Table 6 presents the results of detection F1-Score
using standard multi-class classifiers on 4 backdoor families using
the network, OS, and hardware trails. The results are sub-optimal.
In essence, family is an indicator of code lineage and attribution and
hence mainly useful for static analysis. In contrast, run-time behavior
depends on malware functionalities, which is the same for all
families of a particular class of malware and can affect classification.
7 RELATEDWORK
Multiple prior works have proposed datasets of run-time behavior
of Windows malware [3, 5, 15, 17, 18, 20, 24, 35, 42, 44, 47, 48,
51]. Table 7 compares them based on the environment they use to
execute the malware and the perspectives of data collection and
labeling that they present.
Analysis Environments. Most works rely on virtual machines
that are easily evaded by modern malware, and hence, are not
representative of real-world behavior [17, 18, 20, 24, 35, 44, 47, 51].
On the other hand, the datasets generated in a timely manner under
real-world conditions are not open [3, 15, 42, 48], or are least two
decades old (2001) [5]. Such outdated datasets may not be relevant
in the current malware landscape, as modern malware have evolved
considerably. In contrast, the real-world testbed framework (refer
Section 3) ensures a precise representation of malware behavior
in the wild while providing mechanisms to continually augment
RaDaR with the latest malware samples (refer Section 5).
RaDaR: A Real-World Dataset for AI Powered Run-time Detection of Cyber-Attacks CIKM ’22, October 17–21, 2022, Atlanta, GA, USA
Table 7: Comparison of prior works based on analysis environments and perspectives of data collection and labeling.
| Dataset     | Real World | Year of Samples   | Year of Capture | Multi-Perspective Collection                                      | Multi-Perspective Labeling                                                                                                                                                        |
|-------------|------------|-------------------|-----------------|------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|             |            |                   |                 | Network | OS  | Hardware | Independent Analysis | Binary | AV Label | Method | Objective | Family | Keylogger? | Backdoor? | Target of Information | Open-World Testbed |
| **CAIDA [5]** | ✓          | 2001              | 2001            | ✓       | ✗   | ✗       | ✓                  | ✓      | ✗      | ✗      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **ISOT [35]** | ✗          | * (2004-05, 2010, 2017) | * (2004-05, 2010, 2017) | ✗       | ✓   | ✗       | ✓                  | ✓      | ✗      | ✓      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **CTU [24]**  | ✓          | 2011              | 2011            | ✓       | ✗   | ✗       | ✗                  | ✓      | ✗      | ✗      | ✓        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **[48]**      | ✗          | 2011              | 2011            | ✓       | ✗   | ✗       | ✗                  | ✓      | ✓      | ✓      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **ADFA [17]** | ✓          | 2012-2013         | 2013            | ✓       | ✓   | ✗       | ✗                  | ✓      | ✗      | ✗      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **UCI [47]**  | ✓          | 2010-2014         | 2010-2014       | ✗       | ✗   | ✗       | ✗                  | ✓      | ✗      | ✗      | ✗        | ✗      | ✗         | ✗         | ✗                    | ✗                |
| **[18]**      | ✗          | 2015              | 2015            | ✓       | ✗   | ✗       | ✓                  | ✓      | ✗      | ✓      | ✓        | ✓      | ✗         | ✗         | ✗                    | ✗                |
| **MalRec [44]**| ✓          | 2014-2016         | 2014-2016       | ✓       | ✓   | ✓       | ✓                  | ✓      | ✓      | ✓      | ✓        | ✓      | ✓         | ✗         | ✓                    | ✗                |
| **[42]**      | ✓          | 2018              | 2018            | ✓       | ✓   | ✓       | ✗                  | ✓      | ✓      | ✓      | ✓        | ✓      | ✗         | ✗         | ✗                    | ✗                |
| **RaDaR [51]**| ✓          | 2016-2022         | 2019-2022*      | ✓       | ✓   | ✓       | ✓                  | ✓      | ✓      | ✓      | ✓        | ✓      | ✓         | ✓         | ✓                    | ✓                |


# ✗ in Independent Attributes indicates the type taxonomy that mixes different attributes. Ψ Presents only one or two classes.
- Not Present * No information available or Not Open
$ Growing dataset till date.
Data Collection. Most datasets lack a simultaneous capture of
different trails of malware behavior, including hardware. They either
present a single trail [3, 5, 15, 17, 18, 20, 24, 35, 42, 44, 48, 51],
or a combination of network and OS trails [17, 20, 35, 44]. In contrast,
the comprehensive view of malware activity across the system
stack facilitates a fair comparison of different solutions and
a multi-dimensional analysis of malware behavior as discussed in
Sections 4.1 and 6.
Perspectives in Labeling. Prior datasets use the perspective of
binary, type, family or AV-based strings to label malware [3, 5, 15,
17, 18, 20, 24, 35, 42, 44, 45, 47, 48, 51]. Binary datasets that classify
samples into benign and malware are too restricted for any
multi-dimensional analysis such as assessing risk, capabilities or
forensics [3, 5, 15, 18, 20, 24, 35, 42, 44, 45, 48, 51]. On the other hand,
the datasets based on type [15, 20, 47, 48] and family [18, 24, 35, 44]
present only a single multi-class perspective, and can have fuzzy
class boundaries due to overlapping attributes. While few open
datasets present independent perspectives of objective or methodology
of malware, they are limited to one or two classes (Ransomware
and Botnet [35], RedWorm [5]), thus limiting the scope of analyses
using such datasets. Finally, the AV-based perspective [43, 45] is too
specific for any generalization, thus affecting the classification. In
contrast, we present a dataset with four independent perspectives
in addition to family: methodology, objective, additional capabilities
including keylogging and backdoor, and the information it exfiltrates
(Section 4.2). To the best of our knowledge, RaDaR is the first
open dataset to capture precise malware behavior using real-world
systems with diverse perspectives of its run-time activities.
8 CONCLUSION
This paper presents RaDaR, an open real-world dataset for malware
behavioral analysis, with mechanisms to keep pace with the evolving
malware landscape. RaDaR has multiple use cases for AI-based
security research, including an unbiased comparison of detection
approaches and the development of novel countermeasures incorporating
multiple perspectives of malware execution. While the
challenges in executing malware have resulted in datasets being
largely private or restricted to the security researchers, we firmly
believe that the open RaDaR dataset enables other communities,
especially the data science researchers, to explore and analyze it.
ACKNOWLEDGMENTS
The authors acknowledge K7 Security Pvt. Ltd. and Education and
Research Network for India (ERNET) for the access to malware
samples and the support in the experimental setup. This research
was funded by the Information Security Education and Awareness
(ISEA) project from the Ministry of Electronics and Information
Technology, Government of India, and the FIST program from the
Department of Science and Technology, Government of India. The
authors also thank the reviewers and the technical committee for
reviewing the manuscript and providing constructive comments.

_____________________________________________________________________________________________________________________________________
JUGAAD: Comprehensive Malware Behavior-as-a-Service
Sareena Karapoola
sareena@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Nikhilesh Singh
nik@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
Chester Rebeiro
chester@iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
V. Kamakoti
kama@cse.iitm.ac.in
Indian Institute of Technology Madras
Chennai, India
ABSTRACT
An in-depth analysis of the impact of malware across multiple layers
of cyber-connected systems is crucial for confronting evolving
cyber-attacks. Gleaning such insights requires executing malware
samples in analysis frameworks and observing their run-time characteristics.
However, the evasive nature of malware, its dependence
on real-world conditions, Internet connectivity, and short-lived
remote servers to reveal its behavior, and the catastrophic consequences
of its execution, pose significant challenges in collecting
its real-world run-time behavior in analysis environments.
In this context, we propose JUGAAD, a malware behavior-as-aservice
to meet the demands for the safe execution of malware. Such
a service enables the users to submit malware hashes or programs
and retrieve their precise and comprehensive real-world run-time
characteristics. Unlike prior services that analyze malware and
present verdicts on maliciousness and analysis reports, JUGAAD
provides raw run-time characteristics to foster unbounded research
while alleviating the unpredictable risks involved in executing them.
JUGAAD facilitates such a service with a back-end that executes a
regular supply of malware samples on a real-world testbed to feed
a growing data-corpus that is used to serve the users. With heterogeneous
compute and Internet connectivity, the testbed ensures
real-world conditions for malware to operate while containing its
ramifications. The simultaneous capture of multiple execution artifacts
across the system stack, including network, operating system,
and hardware, presents a comprehensive view of malware activity
to foster multi-dimensional research. Finally, the automated
mechanisms in JUGAAD ensure that the data-corpus is continually
growing and is up to date with the changing malware landscape.
CCS CONCEPTS
• Security and privacy→Malware and its mitigation.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CSET 2022, August 8, 2022, Virtual, CA, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9684-4/22/08. . . $15.00
https://doi.org/10.1145/3546096.3546108
KEYWORDS
Dynamic Analysis, Malware, Run-time Behavior, Real-world,
Testbeds
ACM Reference Format:
Sareena Karapoola, Nikhilesh Singh, Chester Rebeiro, and V. Kamakoti. 2022.
JUGAAD: Comprehensive Malware Behavior-as-a-Service. In Cyber Security
Experimentation and Test Workshop (CSET 2022), August 8, 2022, Virtual, CA,
USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3546096.
3546108
1 INTRODUCTION
Malware attacks are increasing at an alarming scale. The ramifications
of these attacks vary widely, ranging from data breaches to
business disruptions, reputation damage, financial loss, and even
sabotage of critical infrastructures. With millions of malware variants
reported every year, malware is continually evolving in potential
and sophistication, posing significant challenges to security
researchers. Confrontation of such an ever-evolving threat landscape
requires an in-depth understanding of malware behavior
in the wild, including their objectives, functionalities, and consequences.
In fact, behavioral analysis of malware has recently gained
traction in the arms race against malware due to its potential to
detect zero-day malware.
Researchers glean insights into malware behavior from the runtime
trails observed by executing malware samples on analysis
frameworks. This demands access to a large corpus of recently reported
live malware samples. The aspect of being live is important in
the malware context, as its execution heavily depends on its communications
to live remote servers called command-and-control (C&C)
servers. These servers are short-lived and are typically pulled down
in a few months after the malware is first reported, warranting a
timely execution of the sample to elicit its real-world behavior.
Currently, malware research adopts two approaches, supported
mainly by private enterprises, to address the demand for live samples.
Given a hash of a sample to be tested, the first approach
provides the outcome of analysis done by the Anti-virus (AV) engines
housed by these enterprises [55]. The outcome includes the
inference of the maliciousness of the sample, signatures, and reports
from the analyses. However, these signatures and reports are
limited by the capabilities of the available AV engines, whereas
fostering unbounded research requires access to the raw behavioral
data of these samples. The second approach supplies live malware
samples to researchers for execution and subsequent analyses [39].
Such a model has multiple limitations. First, the distribution of live
39
CSET 2022, August 8, 2022, Virtual, CA, USA Sareena, et al.
Download
Collect
Real-World
Testbed
User Submitted Files
FRONT
Data END
Corpus
Execute
Supply of
Samples
BACK-END
Get <Hash/File>
JUGAAD Behavior-as-a-Service
Comprehensive
Run-time Data
For e.g. network,
OS, hardware tails
Figure 1: JUGAAD Framework for Malware Behavior-as-a-Service
samples is highly vulnerable to accidental execution and can be
catastrophic if stringent access policies and processes do not govern
the handling of these samples. Any leakage of samples can lead
to potential misuse, warranting policies for ensuring accountability.
Second, due to the potential risk and liabilities involved, such
services are largely restricted and monopolized by a few private enterprises,
incurring a high cost for a regular supply of new samples.
Third, executing malware and eliciting their real-world behavior
in a laboratory setting is challenging. Researchers typically prioritize
safety and rely on non-connected virtualized frameworks for
analyzing malware. However, modern malware can easily identify
artifacts of such test environments and choose not to execute, thus
evading analysis [40]. Consequently, the data collected does not
represent malware behavior in the wild. In fact, a precise collection
of close-to-real-world behavior requires timely execution of
malware in unrestricted real-world conditions connected to the
Internet, when its C&C servers are likely to be still active.
We propose an alternate model of malware behavior-as-a-service
to meet the demands for the safe execution of samples. Such a model
enables the users to submit hashes of malware samples for analysis
and retrieve their precise and comprehensive run-time trails. We
argue that such a comprehensive view of raw run-time data can
replace the distribution of live malware samples, as behavioral
research primarily relies on passively observable run-time trails
rather than the malware executable contents.
To this end, we present JUGAAD, a framework to facilitate a
comprehensive behavior-as-a-service for malware research. The
framework shown in Figure 1 consists of a front-end that responds
to user requests, a growing data corpus of precise and comprehensive
malware behavior, and a back-end that continually feeds the
data-corpus. The front-end provides API for users to submit program
hashes or files and in return, outputs the corresponding data
retrieved from the corpus. In cases where the data corresponding
to the requested hash/file is not present in the corpus, the front-end
submits the request to the back-end for processing. The back-end executes
a supply of live malware samples on a real-world testbed and
updates the collected behavior to the data-corpus. The testbed with
Internet connectivity ensures real-world conditions for malware
to operate while containing their malicious ramifications. Thus,
JUGAAD alleviates the risks of handling and executing malware to
the research community by facilitating the out-sourcing of the precise
collection of malware behavior. The sustenance of the growing
data corpus relies upon a continual supply of malware samples that
is augmented by a regular feed from online repositories and files
uploaded by users who use the JUGAAD service.
Following are the major contributions of this paper:
(1) A first-of-its-kind behavior-as-a-service model to provide
precise and comprehensive real-world malware behavior for
research, instead of the distribution of the risky malware
samples. Unlike prior malware behavioral datasets [17, 18,
43, 52, 56], the growing corpus of malware behavior keeps
JUGAAD up to date with the changing malware landscape.
(2) A real-world testbed ensuring close-to-real-world heterogeneous
compute and Internet connectivity, with sufficient
triggers for malware execution, demonstrated with 515 offthe-
shelf devices.
(3) A framework with mechanisms for a comprehensive view
of run-time malware activity observable across network, OS,
and hardware. Unlike prior malware testbeds that support
the collection of network and OS trails alone [5], JUGAAD
provides simultaneous capture of hardware behavior along
with these run-time trails.
(4) A framework with mechanisms for timely and large-scale
execution of malware samples, tested up to 255 samples per
day per network (58.6% faster than prior malware testbeds).
Following is the organization of rest of the paper. Section 2 provides
the necessary background for the paper. Section 3 presents the
related work. Section 4 discusses how a comprehensive behavioras-
a-service model can replace the need for distribution of malware
samples. Section 5 describes the framework. Section 6 presents
the implementation details. Section 8 discusses the limitations and
future work in JUGAAD. Finally, section 9 concludes the paper.
2 BACKGROUND
Malware detection takes two broad directions based on the data
they employ for analysis. Static analysis examines the contents of
malware executable binaries to extract signatures and imply its maliciousness.
However, such static signatures can be easily thwarted
by techniques such as packing and obfuscation that change the
malware binary without affecting its functionality. An alternate
approach to malware detection is dynamic analysis, wherein maliciousness
is inferred using the run-time behavior of malware. As
the detection relies on observable behavior, dynamic analysis is
immune to techniques that typically evade static analysis.
Behavioral Analysis. Dynamic analysis adopts two approaches to
analyze malware. Active techniques [32, 42] repeatedly instrument
the malware binary before execution to explore all execution paths
in the malware, whereas passive techniques [4, 10, 57] merely execute
malware and observe the behavioral trails. While such passive
behavioral analysis can analyze the executed path alone, they are
immune to evasive malware that can easily detect the instrumentation
done by active techniques and choose not to execute [36].
Artificial intelligence (AI) driven run-time behavioral analysis
has recently gained traction due to its upper edge in defense against
evolving malware. Such techniques model good behavior and attempt
to detect anomalies, thus facilitating zero-day malware detection.
Primary to fostering such research is the availability of
40
JUGAAD: Comprehensive Malware Behavior-as-a-Service CSET 2022, August 8, 2022, Virtual, CA, USA
ground-truth of malware behavior in the wild. However, collecting
a precise representation of real-world malware behavior in a
laboratory setting is challenging.
Conditions for Executing Malware. Malware execution can be
catastrophic. Consequently, researchers typically rely on virtualized
non-connected analysis environments to execute malware. However,
modern malware is also evasive. They look for real-world
conditions before they reveal their offensive behavior. Hence, they
can quickly identify artifacts of analysis environments to detect
them and remain dormant. Further, malware communications to
its C&C servers are vital for its execution, calling for an active
Internet connection while executing malware. Thus, eliciting realworld
behavior requires executing malware in real-world connected
environments while ensuring the containment of its ramifications.
Challenges in Large-Scale Analysis. Malware also poses challenges
to large-scale evaluations. Specifically, malware execution
impacts the system state in the analysis frameworks. Hence, each
sample should be evaluated in a clean initial state of the frameworks.
Resetting the analysis frameworks to their initial state makes largescale
analysis of thousands of samples time-consuming. Further,
malware execution can cause frequent system crashes and halts
beyond recovery using remote commands, requiring hard power
restarts that significantly affect the large-scale automated analysis.
3 RELATEDWORK
The demand for malware behavioral data in research is addressed
using three approaches, namely, download of old malware samples,
supply of live malware samples, or behavioral datasets, as discussed
next.
Downloading older malware samples. Many online repositories
allow researchers to download a corpus of malware samples
that are a few years old [9]. However, with their C&C servers not
available, most malware samples quit execution prematurely without
exhibiting their real-world behavior.
Supply of live malware samples. Private enterprises such as
VirusTotal provide premium services to download live malware
samples [33, 39]. While such services are highly-priced at 82K$ for
12K malware samples a year, the main challenge lies in the safe
handling and execution of these live malware samples.
Datasets of malware behavior. Multiple prior datasets present
malware behavior for research [4, 15–18, 23, 35, 41, 43, 53, 56]. However,
most of these works rely on virtualized analysis environment,
thus lacking precise real-world behavior [16–18, 23, 35, 43, 52, 56].
On the other hand, datasets generated in real-world conditions
are not open to the community [4, 15, 41, 53]. Additionally, these
datasets are static with no mechanisms to keep pace with the evolving
malware landscape.
In contrast, JUGAAD facilitates behavioral data-as-a-service to
researchers. While it offloads the safe execution of live samples,
it facilitates a dynamic and growing data-corpus that is regularly
augmented with lately reported live samples.
Figure 2: Behavioral data employed by prior research in the
past decade
4 COMPREHENSIVE BEHAVIORAL DATA AS
AN ALTERNATIVE TO MALWARE SAMPLES
JUGAAD advocates the distribution of malware behavioral data
instead of live malware samples. The natural question is whether
data can replace the need for distributing live samples in malware
behavioral research. To answer this question, we first investigate
the vast body of literature on dynamic malware detection to identify
run-time characteristics that were used. Further, we show that any
two programs can be distinguished by the run-time behavior.
Usage of samples in prior works. Figure 2 provides a summary
of the run-time characteristics employed by 400 most cited prior research
in dynamic malware detection since 2010. Theseworks either
rely on available datasets or generate data by executing malware
for their research. The run-time trails of malware are observable at
network (for instance, [1, 2, 4, 7, 8, 12, 18, 25–27, 31, 38, 43]), operating
system (OS) (for instance, [6, 7, 10, 15, 17, 24, 26, 31, 34, 44, 56]),
or, hardware (for instance, [3, 11, 19–21, 37, 41, 45, 49, 58, 59]). The
network trails capture malware communications, including that to
its command-and-control (C&C) servers. On the other hand, OS
trails present the system calls (for e.g. file or registry operations)
made by the malware. In contrast, hardware trails include the microarchitectural
events (e.g., number of cache misses) triggered during
malware execution. These hardware events are measurable using
hardware performance counters (HPC) available in modern processors
[41]. More recently, researchers have explored the potential
of memory snapshots to detect malware using a technique called
volatile memory acquisition (for instance, [13, 47, 50, 51]).
Change in the program leads to change in behavioral trails.
We argue that any change in a program leads to a change in run-time
characteristics that can be visible in the artifacts captured during
malware execution. These differences are evident in a comprehensive
view of malware behavior. To verify the same, we consider
a corpus of 10,000 programs containing an even distribution of
benign, ransomware, downloader, cryptominer, deceptor, potentially
unwanted applications, spyware, and backdoor programs.
Figure 3 plots the distribution of pair-wise dissimilarity between
the behavioral features observed in three example artifacts captured
across the system stack, namely, network, OS, and hardware. The
dissimilarity between two programs p1 and p2 is defined as:
dissimilarity(p1, p2) = 1 − cosine_similarity(p1, p2) , (1)
41
CSET 2022, August 8, 2022, Virtual, CA, USA Sareena, et al.
Figure 3: The dissimilarity between samples across the different
run-time data trails. The rectangular boxes highlight the range of
50% of values in the distribution (i.e., the values between the first
(Q1) and third (Q3) quartile). The horizontal line through the box
indicates the median. The whiskers from each box represent the
minimum and maximum in the distribution. The values between
the minimum and Q1 (lower edge of the box) represent 25% of the
values. Similarly, the values between the maximum and Q3 (upper
edge of the box) indicate the remaining 25% of the distribution.
which measures the difference between the behavioral feature vectors
of the two programs1. A dissimilarity of 0 indicates that the two
programs have identical run-time trails, whereas 1 suggests that
the two are distinct. Thus, most programs (>= 75%) differ in their
network and OS behavior by a dissimilarity measure of 0.3−0.7 and
0.15 − 0.7, respectively. On the other hand, the hardware trails are
comparatively more distinguishable with a dissimilarity of 0.3−0.9
for most programs. While different artifacts can individually distinguish
programs in varying capacities, a comprehensive view of
these artifacts can capture most differences between the programs.
In the current implementation, JUGAAD provides simultaneous
capture of three artifacts, including network, OS, and hardware
behavior, that are predominantly used (Figure 2) for their effectiveness
and decreased overheads in dynamic malware detection. For
comprehensiveness, we intend to include memory snapshots and
other trails such as instruction and power traces in JUGAAD, which
we leave for future work.
5 JUGAAD FRAMEWORK
In this section, we discuss the JUGAAD framework, including its
front-end and back-end, as shown in Figure 1. The front-end handles
the user requests, whereas the back-end is responsible for the
precise and comprehensive collection of malware behavior.
5.1 Front-end
The front-end presents the following Application Programming
Interface (API) calls to the users to access the JUGAAD behavioras-
a-service as described in Figure 4.
Get data for hash. The API GetDataForHash (hash h), allows the
users to submit the hash h of a program and request for its behavioral
data. In response, the front-end extracts the corresponding
behavioral data from the data-corpus and returns it to the user as
1Cosine similarity measures the similarity between two vectors, and is measured by
the cosine of the angle between two vectors.
Yes Is data of hash
h present
in corpus?
Check if data of h is present in
corpus
GetDataForHash (hash h)
Return Data
GetDataForProgram (program p, platform f, <time t> )
Invoke back-end
Data = Execute_Collect (p, f, t)
/*Back-end executes p and returns the Data */
Return Data;
No
Return ERROR
(a) (b)
Data = Extract
data of h from
corpus
Figure 4: The APIs presented by JUGAAD front-end to the users.
The API GetDataForFolder is similar to GetDataForProgram, where
the former takes a folder of programs as input.
shown in Figure 4a. In cases where the data corresponding to the
hash is not present in the data-corpus, the front-end returns an
error.
Get data for a program. The API GetDataForProgram (program
p, platform f, ⟨ time t ⟩), allows the users to submit a program
executable and request for the corresponding behavioral data. The
input includes the program executable p, the platform f (e.g., Linux,
Windows, Android) on which the program needs to be executed,
and optionally, the time duration t for which the program execution
should be observed while collecting the behavioral trails. In
response, the front-end raises a request to the back-end, which
executes the program for a time duration t and collects its behavioral
trails. The collected trails are saved to the data-corpus and
returned to the user. By default, the time duration t is configured
as 2 minutes at the back-end, which is considered to be sufficient
to elicit most of the malicious behaviors of malware [22].
Get data for a folder of samples. Alternatively, users may want
to upload multiple files at once for the collection of behavioral trails.
To this end, the API
GetDataForFolder (program_folder F , platform f, ⟨ time
t⟩), allows users to submit a folder of programs, along with specifying
the platform and time for executing each sample in the folder.
The front-end invokes the back-end to execute and collect the behavior
of the samples and return the data to the user.
5.2 The Back-end
The primary functionality of the back-end is to supply precise closeto-
real-world and comprehensive malware behavior to the datacorpus,
which is used to serve the users. JUGAAD ensures precise
behavioral data by facilitating: (1) timely execution of malware
when their short-lived remote command and control (C&C) servers
are highly likely to be active, and, (2) connected, yet contained realworld
environment for the malware to execute. On the other hand,
it facilitates a comprehensive view of malware activity with simultaneous
capture of run-time trails across the system stack. Figure 5
illustrates the back-end of JUGAAD. The update and test engines
together ensure a regular and timely update of the data-corpus to
service the user requests. On the other hand, the real-world testbed
42
JUGAAD: Comprehensive Malware Behavior-as-a-Service CSET 2022, August 8, 2022, Virtual, CA, USA
Update
Engine
New
Samples
Check for
new samples
Test
Engine
Execute
Collect
Dataset
Corpus
Supply of
Samples
Download
Real-World Testbed
Save Data
Remote
Power Control
Figure 5: Back-end of JUGAAD.
provides connected real-world conditions for malware to operate
while containing its malicious repercussions, and mechanisms to
observe its comprehensive behavior.
5.2.1 Timely Analysis of Malware. Algorithm 1 describes the
working of the update and test engines. The update-engine periodically
crawls public malware repositories for newly reported
malware and downloads them to its local supply of samples (Step
3-8). Further, the test-engine executes these samples immediately
after the download on the real-world testbed. The timely execution
ensures that the samples are executed when their C&C servers are
still available. During the execution, the test engine collects multiple
run-time artifacts (e.g., network communications, OS system calls,
and micro-architectural events) across the system stack, which are
later added to the data-corpus (Execute_and_Collect in Step 9).
The default time for analyzing each sample is set as 2 minutes based
on prior research on the minimum time window required to observe
most of the malicious behaviors of malware [22]. Additionally, the
test-engine services the user-submitted requests for data collection
(Steps 10-15). It executes the programs submitted by the users and
collects the behavioral trails, which are stored in the data-corpus
and returned to the users (Step 15). Thus, the Algorithm 1 ensures
the timely execution of a regular feed of new malware samples
reported in the wild to maintain a growing data-corpus.
5.2.2 Real-world Environment. Modern malware are known
to look for real-world conditions such as a network of diverse physical
machines and Internet connectivity before they reveal their
malicious behavior (refer Section 2). To this end, the real-world
testbed provides a heterogeneous network of machines that can
be employed as a profiler to execute malware. The testbed consists
of desktop computers and single-board embedded platforms with
varying operating systems (e.g., Linux, Windows). The diversity
in machines and software environments not only provides sufficient
triggers for malware to execute, but also enables JUGAAD
to execute malware of diverse platforms. The testbed with autoconfiguration
capability is open, wherein new machines with specific
environments can be added to the testbed seamlessly. Though
these machines are connected in a bus topology, the testbed also
can facilitate user-specific topologies for advanced analysis, such
as the study of malware propagation.
Internet Connectivity and Containment. To provide connectivity
while containing the ramifications, the back-end uses a dedicated
Internet connection (ERNET [14]) that is isolated from the university
network. Further, it connects to the Internet via a two-level
firewall, as highlighted in Figure 5, which ensures containment of
Algorithm 1: JUGAAD Back-end
1 begin
2 while true do
/* Update Engine */
3 Crawl online repositories for newly reported samples
4 if updates are available then
5 NewhashList←Hashes of newly reported malware
samples
6 for h ∈ NewHashList do
7 p ← Download hash h
8 Supply-of-Samples←p
/* Test Engine */
9 Data-Corpus←Execute_Collect (p)
10 Check for requests from front-end
11 if requests queued from front-end then
12 ListOfPrograms←List of programs submitted by
user
13 for p ∈ ListOfPrograms do
14 Supply-of-Samples←p
// Test Engine
15 Data-Corpus←Execute_Collect (p)
the malicious impact of executing malware, while allowing the malware
to operate. The firewalls permit incoming communications
to allow the malware to communicate with its C&C servers, while
extensively scrutinizing outgoing communications to prevent malicious
behavior from permeating outside the testbed. Implementing
the two levels with different firewall models has advantages. The
malware would need to compromise two separate firewalls to infect
machines outside the testbed, which are less likely to be susceptible
to the same malware. Likewise, external attackers would need to
compromise two firewalls to attack the testbed.
While the firewalls allow initial handshakes of connections,
it limits the rate or drops packets when the following scenarios/
triggers from the testbed cross their respective thresholds: (i)
DoS attempt: a high rate of outgoing packets from any machine; (ii)
TCP Scan: a significant number of half-open TCP connections over
time; (iii) SPAM: the number of email messages from the testbed;
(iv) UDP Scan: the ratio of UDP packets from the malware to the unsuccessful
responses (e.g., Internet Control Message Protocol port
unreachable) received. While there is a possibility of some attacks
like DoS to persist when network traffic patterns do not match the
firewall rules, the risk is not unacceptably high. This is because the
running time of every sample is restricted to a threshold, typically
2 minutes based on prior research [22]. After the execution, all
machines in the testbed are reset to their clean initial state, which
reduces the risk of spam, DoS, or unpredictable behavior to a smalltime
duration defined by the threshold. Finally, these rules are not
exhaustive and would require continual monitoring and updates
based on the malware classes that are being analyzed.
Stateless Evaluations. Each malware sample should be evaluated
in a clean initial state or baseline of the testbed (refer Section 2).
Unlike virtual machines, which can be easily reset to their baselines,
43
CSET 2022, August 8, 2022, Virtual, CA, USA Sareena, et al.
Image
Server
Load Image Clean
Baseline state
Post-experiment
State
Baseline-Reset
Initialize
Start
Experiment
Run
complete
Success?
Yes
Image-Reset
No
Using remote
commands or hard
power restart
Figure 6: Two-level reset mechanism in JUGAAD.
1. Reset to clean Baseline state
2. Push program p to platform f
3. Start the data collection
4. Run Malware executable
6. Pull data to logs, save to repository
Keep collecting data,
if available, for t minutes
5. Stop the malware program
Data
Corpus
Data
E.g. OS, Hardware Data
Data
E.g. network
communications
Test Engine
ExecuteProfiler
Collect
(p, f, t)
Internal
tools
External
tools
3. Start the data collection
Figure 7: Data Collection in the testbed (Execute_and_Collect)
resetting all physical machines in the testbed for every sample execution
is not trivial. To this end, JUGAAD employs a two-level reset
mechanism as shown in Figure 6. The first level involves a quick
low-overhead baseline-reset using software methods to restore a
physical machine to its saved clean baselines on restart. JUGAAD
initiates a baseline-reset with a restart of all machines using remote
commands. In cases where the malware makes the system
inaccessible remotely, JUGAAD uses the smart power switches to
hard-restart the machines. Further, critical faults may arise, corrupting
the baselines such that the baseline restore fails. Only in
such scenarios JUGAAD performs an image-reset, which involves
reloading of respective OS images from an image server.
5.2.3 Comprehensive collection of malware behavior. Figure
7 illustrates the working of the test-engine (Execute_and_
Collect) to collect the comprehensive behavior of malware. First,
it begins by resetting all the testbed machines to their clean baselines.
Second, it chooses an appropriate machine in the testbed as a
profiler to execute malware, and pushes the malware sample to it.
For instance, it chooses a Windows machine to evaluate a Win32
malware. Third, it initiates the data collection. It starts the corresponding
tools to capture artifacts inside the profiler. For instance,
to capture OS system call trails, it starts the process monitoring
tool [29] at the profiler. However, it is beneficial to observe some artifacts
from outside the profiler. For instance, observing the network
communications at the gateway that connects the testbed to the
Internet, can capture not only the communications from the profiler
but also malware interactions and their impact on other testbed
machines. Accordingly, the test-engine starts such external tools at
corresponding vantage points. Fourth, the test-engine executes the
sample for a configured duration or provided by the user. Fifth, it
stops the execution and the data collection tools. Finally, it saves
Profiler
Configuration
Engine
Test
Engine
. . .
Control-Plane
Profiler Profiler
Real-world testbed
Remote
Power Control
Internet
Dataset
Corpus
Supply of
Samples
Gateway
Update Engine
Figure 8: The implementation of back-end in JUGAAD
all the collected trails to the data-corpus along with the IP address
of the profiler, and the process ID of the malware executable at the
profiler.
6 IMPLEMENTATION
In this section, we discuss the implementation details of the JUGAAD
framework.
We implement the front-end as an HTTPS web server in python
to provide a public web interface to submit the program hash or
files. The server also provides an HTTP-based public API to enable
users to script submissions in any programming language.
Inspired by software-defined networking, we divide the back-end
into planes, namely gateway, control-plane, and testbed, to support
customization and reconfiguration, as shown in Figure 8. The
gateway connects the back-end to the Internet via a dedicated IP
address in ERNET [14]. It also houses the update engine that feeds
the supply of samples. The control-plane contains the configuration
and test engines to initialize, operate and automate the back-end.
We implement these modules using bash scripts and Python. While
the configuration engine initializes the testbed (for e.g. software
environment), the test engine automates the execution and data
collection on the testbed. Table 1 lists the gateway, control-plane
and testbed machines that are connected as in Figure 8 to realize
the JUGAAD back-end. We next explain the implementation of
timely analysis and the comprehensive data collection discussed in
Section 5.2.
6.1 Timely Analysis
To facilitate access to newly reported malware, JUGAAD has subscriptions
to premium services from online malware repositories
such as Virustotal [54] that provide a daily feed of requested
samples. We implement the back-end algorithm (Algorithm 1) in
Python, which accesses the APIs provided by Virustotal to crawl for
the availability of samples and download them. These samples are
immediately executed by the test engine to ensure timely analysis.
Test engine (Execute-Collect). We implement the test engine
in Python. For Windows-based profiler machines, the test engine
uses PowerShell Remoting and psexec tool to remotely trigger
the malware execution [28]. However, when remote execution is automated,
the malware executes in the background without popping
44
JUGAAD: Comprehensive Malware Behavior-as-a-Service CSET 2022, August 8, 2022, Virtual, CA, USA
Table 1: Devices in the open testbed
Device Description OS Count Role
Desktop Intel i7, 8 cores, 64GB RAM,
2 1G Ethernet ports
Ubuntu
v17.10 1 Gateway
Desktop Intel i7, 8 cores, 64GB RAM,
2 1G Ethernet ports
Ubuntu
v18.04 1 Control
Plane
Desktop Intel i5, 8 cores, 16GB RAM
, 1G Ethernet port Win 10 Pro 2 Testbed
Desktop Intel i7, 8 cores, 64GB RAM
, 2 1G Ethernet ports
Ubuntu
v16.10 1 Testbed
Desktop Intel x86 Atom, 1 Core, 2GB
RAM, 1G Ethernet port
Win 7
Win 10 10 Testbed
Raspberry
Pi 3 B+ 2
ARM Quad core, 1GB RAM,
1G Ethernet
Linux
v3.14 2 Testbed
Galileo
Gen 2
Intel Quark SoC 256MB RAM,
1G Ethernet
Linux
v3.14 500 Testbed
Figure 9: The testbed in JUGAAD
its GUI window. To get the malware executed in the foreground, we
configure the psexec tool to run the malware in the system account.
For Linux-based machines, the test engine uses SSH service to push
and execute a malware sample. The test engine also monitors the
health of machines with a dummy SSH connect request. In case of
a failure, it notifies the administrators with an email and resets the
testbed using the smart power switch.
6.2 Real-world testbed
Figure 9 shows the real-world testbed we built in our lab in the
back-end. We connect all machines in the testbed (refer Table 1) in
a hierarchical bus topology using 28-port D-Link DGS-1210-24 and
24-port HPE-1920s switches. The testbed is open and scalable, as any
new hardware with an Ethernet port can be connected to the switch
to include in the network. It is also powered using WiFi-enabled
smart switches to enable remote hard-restart of the testbed in cases
of failures. Each machine in the testbed is assigned its management
IP address at power-on by the Dynamic Host Control Protocol
(DHCP) server at the control-plane. The control-plane configures
and manages the testbed using the management IP addresses of the
devices.
In the current implementation, all machines in the testbed are
under one network. It is important to note that simultaneous execution
of more than one malware sample would affect the precision
of data as each sample can affect the system state in the entire
network. The number of samples analyzed per day can be increased
if we can divide the testbed into isolated virtual local area networks
(VLAN). With the infrastructure of smart network switches (HPE-
1920s switches), we intend to implement VLANs in future to enable
parallel analysis of multiple samples. Further, we intend to enable
custom topologies and forwarding behavior in the testbed in the
future to facilitate long-term analysis such as malware propagation.
Heterogeneity. As evident from Table 1, the testbed has a heterogeneous
mix of hardware (Raspberry Pi, Intel x86 Atom, Quark,
i5, and i7 machines) and operating systems (different versions of
Linux and Windows) to realize close-to-real-world conditions.
Isolation and Containment. JUGAAD uses a combination of
Snort Intrusion Detection System [46] and Zeek network analysis
framework [60] at the control-plane and gateway. While Snort
can detect known exploits, Zeek can detect protocol violations and
malformed headers using the built-in and custom scripts.
Stateless Evaluations. For baseline-reset, JUGAAD uses software
like Reboot Rx on Windows and SystemBack on Linux [48]. The
test-engine initiates a baseline-reset with an SSH command or a
power-restart using the smart power switches in cases of SSH failures.
During the operation of JUGAAD, we observed the need for
such power-restart in several instances. In either case, the baselinereset
of all machines finishes in ≈ 2.64 minutes. For image-reset,
JUGAAD loads the affected device with the OS image (created using
Clonezilla disk imaging software) from a locally maintained
image-server. However, image-reset is very rare, as we did not
encounter any need for it during the analysis of more than 10K
samples.
6.3 Comprehensive collection
In the current implementation, JUGAAD provides simultaneous
capture of three artifacts, including network, OS, and hardware
behavior. The control-plane captures network communications using
tshark [30]. While external communications pass through the
control-plane, inter-device communications in the testbed are mirrored
to the control-plane using port-mirroring feature of the managed
switches. At the profiler machine, we use process monitor
for Windows, and strace for Linux to capture OS events of the
malware process [29].
For HPCs, we use different interfaces based on the environment.
While Linux provides perftool APIs to configure and fetch the
counters from the userspace, the Windows OS requires some modifications.
We design a custom Windows 10 Driver to read HPCs,
configurable as per the underlying hardware and available events.
Based on empirical observations, we configure the frequency of
event logging to 10ms. It is critical to note that while the hardware
may support hundreds of micro-architectural events, the number
of HPC registers available physically is limited to 4-6 on most architectures.
While time-multiplexing these events across the registers
is a work-around, it can induce significant noise in the data. We
45
CSET 2022, August 8, 2022, Virtual, CA, USA Sareena, et al.
Figure 10: Evasion techniques found in 1000 randomly selected
samples. Malware samples check for specific artifacts to identify
virtualized analysis environments.
address this challenge with multiple binary executions, where limited
events are counted during each execution. Finally, to prevent
the corruption of collected OS and hardware data by malware like
ransomware, they are temporarily stored in system folders (such
as C :\Windows\Temp\) before being moved to the data corpus.
7 EVALUATION
In this section, we evaluate the precision of data collection in JUGAAD,
the time taken for analysis, and the storage requirements
of the data-corpus.
Precise collection of behavior. Modern malware are evasive and
adopt diverse techniques to remain dormant in virtualized analysis
environments [40].We determine if JUGAAD is able to execute and
observe the behavior of such malware.We take a random set of 1000
malware samples. We verify their system call traces to determine
if these samples query for any specific information that aids in
identifying analysis environments, such as differentiating virtual
and physical machines. Figure 10 plots the count of samples using a
subset of 12 techniques typically used for evasion [40]. We observe
that at least 17% of samples had the signature system call to check
if the hard-disk drive size and free space are small. More than 50%
of samples verified the network MAC address, adapter name, and
provider before continuing execution. The real-world conditions
and the Internet connectivity enabled JUGAAD to ensure malware
continues execution beyond these checks in their code.
Analysis Time. Table 2 computes the time taken by the back-end
to analyze a given program sample. Analysis of each sample takes
≈ 338 seconds which includes steps 1-7 in Figure 7. The test-engine
takes ≈ 158 seconds to reset the state of all testbed machines to
their clean baselines in step 1. Hence, JUGAAD can analyse ≈ 255
malware samples per day (refer Table 2). The table also compares
the time for state reset in JUGAAD with techniques used in public
testbeds like DETER[12]. The two-level reset feature in JUGAAD
(refer Section 5.2.2, Figure 6) enables 58.6% times faster reloads
compared to DETER. The shorter time taken for state resets enables
more number of sample analysis (255 per day per network) in
JUGAAD as compared to DETER (154 per day per network).
We intend to increase the number of samples analyzed per day in
JUGAAD by dividing the testbed into isolated VLANs. The current
infrastructure of smart network switches (HPE-1920s switches)
can easily enable such configurations to enable parallel analysis of
multiple samples.
Figure 11: Growing data-corpus over years
Table 2: Time taken by JUGAAD back-end per sample (refer Figure
7)
Testbed Baseline-reset
(Step 1)
Experiment
(Steps 2-7)
Time/
sample
Samples/
/day
JUGAAD 2 m 38 s 3 m 5 m 38 s 255
DETER 6 m 23 s 3 m 9 m 23 s 154
Growing data-corpus. To date, the data-corpus has 2.7 TB of data
and 22M behavioral snapshots of 10,432 malware samples, including
7M network packets, 11.3M operating system call traces, and 3.3M
micro-architectural events from hardware for 8 classes of malware.
Table 3 shows the distribution of malware samples collected in the
growing dataset. Figure 11 plots the growing storage requirements
of the data-corpus in JUGAAD.
Table 3: Distribution of malware classes in the data-corpus
Class % Class % Class %
Backdoor 13.5% Spyware 16% Ransomware 7.5%
Banker 14% Benign 7% Downloader 19.4%
PUA 10.8% Deceptor 10.8% Cryptominer 4.9%
8 DISCUSSION
We present a discussion on the comprehensiveness of the data
collection and the sustenance of JUGAAD behavior-as-a-service
model.
Comprehensive malware behavior. Malware behavior manifests
through diverse artifacts that can be captured across the system
stack during its execution. While network, OS, and hardware
trails have been widely employed for their improved detection
capabilities, many other run-time artifacts can be employed for
detecting malware. Recently, the potential of memory snapshots,
register contents, instruction opcode traces, and power traces have
been explored to detect malware. Such collection modules can be
easily plugged into the data collection framework of JUGAAD in
Figure 7. We leave the inclusion of other such modules and novel
artifacts for malware detection into JUGAAD as future work.
Sustenance. Sustaining the service model relies on a continual
supply of newly reported malware samples. JUGAAD bootstraps
such a supply with a premium subscription with private enterprises
for downloading samples. However, with the continual operation
and increasing user base, we envision a constant supply of malware
samples from the users to ensure a growing data-corpus of malware
behavior.
46
JUGAAD: Comprehensive Malware Behavior-as-a-Service CSET 2022, August 8, 2022, Virtual, CA, USA
9 CONCLUSION
This paper presents JUGAAD, a malware behavior-as-a-service
to present precise and comprehensive malware run-time characteristics
for research. The beneficiaries of JUGAAD are malware
researchers in academia and industry. It offloads the time and
efforts of setting up a real-world evaluation infrastructure for
comprehensive data collection, while alleviating the high risks
involved in handling and executing potent malware. Prior efforts
that provide malware analysis services present inferences on
maliciousness of the user-submitted samples, that are limited by
the capabilities of available state-of-the-art detection engines. In
contrast, JUGAAD provides an unbiased comprehensive view of
real-world malware behavior, enabling researchers to quickly
explore and compare detection mechanisms to counter the evolving
malware landscape.
ACKNOWLEDGMENTS
This research was supported by the Information Security Education
and Awareness (ISEA) project from the Ministry of Electronics
and Information Technology, Government of India. The authors
thank the reviewers and the technical committee for reviewing the
manuscript and providing constructive comments.

____________________________________________________________________________


